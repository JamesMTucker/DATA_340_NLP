{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/assignment_notebooks/Webscraping.ipynb)\n",
    "\n",
    "# Webscraping Assignment\n",
    "\n",
    "Reminder: you are permitted to work with another classmate on this assignment. If you do, please submit a single notebook with both of your names at the top.\n",
    "\n",
    "## Due date\n",
    "\n",
    "Friday, February 24 (12:00 pm), 2023\n",
    "\n",
    "## Assignment description\n",
    "\n",
    "In this project you will write a Jupyter Notebook or R Markdown file to scrape a selected website. You will need to:\n",
    "\n",
    "1. Write a function that takes a URL as input and returns the HTML of the page as a string.\n",
    "2. Inspect the HTML of the page and use regular expressions to extract the documents within the page.\n",
    "3. Model the documents in a corpus\n",
    "4. Analyze the corpus using the bag of words model\n",
    "5. Implement a TF-IDF model to extract the most n-important words for each document in the corpus.\n",
    "\n",
    "### Objective\n",
    "\n",
    "This assignment reinforces previous lecture topics on the linguistic background, properties of language, information theory, and Regular Expressions.\n",
    "\n",
    "\n",
    "## Submission medium\n",
    "\n",
    "Jupyter Notebook or R Markdown file. See additional instructions at the final section of this document.\n",
    "\n",
    "## Code Dependencies\n",
    "\n",
    "You will need to install the following packages:\n",
    "\n",
    "- `requests`\n",
    "- `re`\n",
    "- `beautifulsoup4`\n",
    "- `nltk`\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `matplotlib`\n",
    "\n",
    "\n",
    "## Grading\n",
    "\n",
    "This assignment is worth 10 points. (extra credit 1 point to final grade if you create a heatmap of the TF-IDF matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function that takes a URL as input and returns the HTML of the page as a string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Write a function that takes a URL as input and returns the HTML of the page as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_html(url) -> str:\n",
    "    \"\"\"Get the HTML of a webpage and return the HTML as a string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL of the webpage to scrape.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The HTML of the webpage as a string.\n",
    "    \"\"\"\n",
    "    ## YOUR CODE HERE\n",
    "    html_source: str = requests.get(url).text\n",
    "    assert isinstance(html_source, str), \"The HTML should be a string.\"\n",
    "    return html_source"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Inspect the HTML of the page. Can you identify any patterns in the HTML that might be useful for extracting the documents within the page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the the HTML source code from the URL (this is the same URL we used in class)\n",
    "url = \"https://www.gutenberg.org/files/1/1-0.txt\"\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use the BeautifulSoup library to create a BeautifulSoup object from the HTML string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs4\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Extract the HTML body text and examine the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please explain what the following line of code does in the cell below.\n",
    "body = soup.find(\"body\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Use regular expressions to extract the documents within the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Your regex here to capture the documents\n",
    "doc_extractor = r\"\"\n",
    "\n",
    "# Explain this line of code in the cell below.\n",
    "# __Note:__ You will need to use the `re.MULTILINE` flag to ensure that the\n",
    "# regular expression matches across multiple lines.\n",
    "found_documents: list = re.findall(doc_extractor, body.text, re.MULTILINE)\n",
    "\n",
    "assert len(found_documents) == 9, \"Please check your regex. You should have found a total 9 documents.\"\n",
    "\n",
    "## if you are having trouble with the regex remeber that you can use regex101.com to test and debug.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain: `documents = re.findall(doc_extractor, body.text, re.MULTILINE)`\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Explore the contents of the Documents\n",
    "\n",
    "In the matched documents, you will find a heading appended to the text by project Gutenberg. For the purposes of this assignment, I provided a cleaner function to extract the Gutenberg headings from the text for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gutenberg(text: str) -> str:\n",
    "    \"\"\"Clean the text of a Gutenberg document.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text of a Gutenberg document.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The cleaned text of the document.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"\\[Etext #\\d+\\]\", \"\", text)\n",
    "    text = re.sub(r\"(\\r\\n)+\", \" \", text)\n",
    "    text = re.sub(r\"^The Project Gutenberg.*?Independence\\*\\*\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"^ \\*\\*\\*\\*The Project Gutenberg Etext of The U. S. Bill of Rights\\*\\*\\*\\*\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"^ November.*?EST\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"^ \\*\\*The Project.*?, USA\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"^ \\*\\*\\*\\*\\*The Project.*?corrections\\. \\*\\*\\*\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"^ The Project.*?1775\\.\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"^ Officially.*?calendar\\]\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"^ \\*\\*The Project.*?, 1865\", \"\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r\"^ The Project.*?, 1861\", \"\", text, flags=re.MULTILINE)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i, doc in enumerate(found_documents):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the corpus here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the above corpus of documents using TF-IDF\n",
    "\n",
    "In the follow steps, I would like for you to accomplish the follow preprocessing steps. \n",
    "\n",
    "1. Tokenize the documents\n",
    "2. Lemmatize the tokens\n",
    "3. Remove stop words\n",
    "4. Remove punctuation\n",
    "5. Apply TF-IDF to the corpus\n",
    "    * You can write a TF-IDF model from sratch or use the `sklearn` library\n",
    "\n",
    "_tip: see lecture notebooks 4, 5, and 6 for examples of how to work with pandas_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TIP ###\n",
    "## if you want to work with pandas create a dataframe with documents as rows and columns for the document number and the text\n",
    "import pandas as pd\n",
    "corpus = pd.DataFrame({\"docID\": range(len(corpus)), \"text\": corpus})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words\n",
    "\n",
    "You can use the `nltk` library to remove stop words. You can also use the `nltk` library to remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the documents and corpus using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Please submit your assignment as a Jupyter Notebook or R Markdown file. You can submit your assignment as a link to a Google Colab notebook or a link to a GitHub repository. If you are submitting a link to a GitHub repository, please make sure that your repository is public. If you email the notebook to me, please zip the file before sending it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d99a3f87c484a74ba405ca572f7f1b4059e93a8c4d7f8027bf5ae12e7919d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
