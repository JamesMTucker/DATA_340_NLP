{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/Fall_2023/notebooks/06_Vector_Semantics.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 15: 2023-28-03 Vector Semantics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of lecture\n",
    "\n",
    "- Introduction to lexical semantics\n",
    "- Introduction to Vector Semantics\n",
    "  - Vector semantics: Osgood et al. (1957)\n",
    "  - Vector semantics: Joos (1950), Harris (1954), Firth (1957)\n",
    "- Embeddings\n",
    "    - Word2Vec\n",
    "    - GloVe\n",
    "    - FastText\n",
    "    - ELMo\n",
    "    - BERT\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Neural Networks\n",
    "\n",
    "<center><img src=\"images/Neuron.drawio.png\" width=\"800\" height=\"400\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Caculating Loss (measuring error - training a model adjusts weights and biases to minimize loss)\n",
    "* Optimizing Loss (adjust weights and biases to minimize loss)\n",
    "* Backpropagation (calculate the gradient of the loss function with respect to the weights and biases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Lexical Semantics\n",
    "\n",
    "Taken from Jurafsky and Martin (2023) chapter 23:\n",
    "\n",
    "```\n",
    "\n",
    "Lady Bracknell: Are your parents living?\n",
    "Jack: I have lost both my parents.\n",
    "Lady Bracknell: To lose one parent, Mr. Worthing, may be regarded as a misfortune; to lose both looks like carelessness.\n",
    "\n",
    "```\n",
    "\n",
    "* words are relational units that are prone to messiness and ambiguity\n",
    "* Ambiguity is a fact of life in language (`mouse` as in a rodent or a computer device)\n",
    "* Polysemy: a word or lemma with multiple meanings (`bank` as in a river bank or a financial institution)\n",
    "* `Antonymy`: words (or lemmas) with opposite meanings (`hot` and `cold`)\n",
    "* `Synonym`: words (or lemmas) that are similar in meaning (`couch` and `sofa`)\n",
    "* Taxonimic relations\n",
    "    * `hyponymy` (subordinate): words (or lemmas) that are more specific (`poodle` is a hyponym of `dog`) - subclasses or members\n",
    "    * `hypernym` (superordinate): words (or lemmas) that are more general (`dog` is a hypernym of `poodle`) - classes\n",
    "        * entailment: being A entails being B (`dog` entails `poodle`)\n",
    "        * is-a hierarchy: a hierarchy of classes that is organized by the is-a relation or A IS-A B\n",
    "    * `meronymy`: words (or lemmas) that are part of a larger entity (`leg` is a meronym of `human`) - part-whole relationships\n",
    "    * `metonymy`: words (or lemmas) that are associated with a larger entity (`the crown` is a metonym of `the queen`) - association (prototype categories)\n",
    "    * `holonymy`: words (or lemmas) that are a whole of a smaller entity (`face` is a holonym of `eye`) - whole-part relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_taxonomy(noun):\n",
    "    synsets = wn.synsets(noun)\n",
    "    if synsets:\n",
    "        synset = synsets[0]  # take the first synset\n",
    "        hypernyms = synset.hypernyms()\n",
    "        hyponyms = synset.hyponyms()\n",
    "        meronyms = synset.part_meronyms() + synset.substance_meronyms() + synset.member_holonyms()\n",
    "        holonyms = synset.part_holonyms() + synset.substance_holonyms() + synset.member_meronyms()\n",
    "        return {\n",
    "            \"word\": synset.name(),\n",
    "            \"definition\": synset.definition(),\n",
    "            \"hypernyms\": [h.name() for h in hypernyms],\n",
    "            \"hyponyms\": [h.name() for h in hyponyms],\n",
    "            \"meronyms\": [m.name() for m in meronyms],\n",
    "            \"holonyms\": [h.name() for h in holonyms]\n",
    "        }\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_taxonomy(\"dog\")\n",
    "for k,v in result.items():\n",
    "    print(k, v, sep=\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_verb_relations(verb):\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    if synsets:\n",
    "        relations = {\n",
    "            \"antonyms\": set(),\n",
    "            \"entailments\": set(),\n",
    "            \"causes\": set(),\n",
    "            \"also_sees\": set(),\n",
    "            \"verb_groups\": set(),\n",
    "            \"similar_tos\": set()\n",
    "        }\n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                antonyms = lemma.antonyms()\n",
    "                if antonyms:\n",
    "                    relations[\"antonyms\"].add(antonyms[0].name())\n",
    "            for entailment in synset.entailments():\n",
    "                relations[\"entailments\"].add(entailment.name())\n",
    "            for cause in synset.causes():\n",
    "                relations[\"causes\"].add(cause.name())\n",
    "            for also_see in synset.also_sees():\n",
    "                relations[\"also_sees\"].add(also_see.name())\n",
    "            for verb_group in synset.verb_groups():\n",
    "                relations[\"verb_groups\"].add(verb_group.name())\n",
    "            for similar in synset.similar_tos():\n",
    "                relations[\"similar_tos\"].add(similar.name())\n",
    "        return relations\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_verb_relations(\"catch\")\n",
    "for k,v in result.items():\n",
    "    print(k, v, sep=\":\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Semantics\n",
    "\n",
    "* Firth (1957) proposed a model of word meaning based on the idea that words are associated with other words in a network of semantic relations.\n",
    "* Firth (1957), Joos (1950), and Harris (1954) all proposed models of word meaning based on the idea that words are associated with other words in a network of semantic relations. Thus the idea of distributional semantics takes its name from the fact that the meaning of a word is discerned by the words that tend to occur in its company.\n",
    "\n",
    "> You shall know a word by the company it keeps. (Firth, 1957)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity\n",
    "\n",
    "* Word similarity is a measure of the degree of semantic similarity between two words. This measure takes into account the distributional properties of words in a corpus. Whereas words like `coffee` would rarely occur in a dictionary entry for the word `cup`, users of language expect that the words `coffee` and `cup` are similar in meaning. They are similar, in this case, because semantic frames are shared between the two words. The semantic frame of `coffee` is a hot beverage, and the semantic frame of `cup` is a container for a hot beverage. The semantic frames of `coffee` and `cup` overlap, and this overlap is the basis for the similarity between the two words. We can capture these similarities by computing the distributional properties of words in a corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we represent words and their meanings in numerical format?\n",
    "\n",
    "We vectorize it!\n",
    "\n",
    "We can represent words in a vector space or embedding space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec, Mikolov et al., 2013\n",
    "\n",
    "Goal: to create “techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity.” (Mikolov, et al., 2013a, 2013b)\n",
    "\n",
    "Mikolov et al. propose two log-linear solutions\n",
    "\n",
    "* Continuous Bag-of-Words Model\n",
    "* Continuous Skip-gram Model \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/mikolov.png\" width=\"900\" height=\"500\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec embeddings are static embeddings, and therefore they do not capture the cooccurrence of words in a sentence. This is a problem for downstream tasks that require contextualized embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove, Pennington et al., 2014"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“...the shallow window-based methods [e.g., log bi-linear models, CBOW, or Skipgram] suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the entire corpus, which fails to take advantage of the vast amount of repetition in the data.” Pennington, et al., 2014."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/glove.png\" width=\"800\" height=\"400\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText, Bojanowski et al., 2017"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/fasttext.png\" width=\"900\" height=\"500\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elmo, Peters et al., 2018"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/elmo.png\" width=\"900\" height=\"400\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"They [embeddings] should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).\" ([Peters et al., 2018, p. 1](https://arxiv.org/pdf/1802.05365.pdf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create static word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code out the word2vec CBOW and Skipgram models and compare them. To do this, let's define our configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Number of dimensions\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "# Window size\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "ITERATIONS = 10000\n",
    "\n",
    "# OUTPUT\n",
    "OUTPUT_PATH = \"outputs\"\n",
    "\n",
    "## Let's plot the loss for the skipgram model\n",
    "SKIPGRAM_LOSS = os.path.join(OUTPUT_PATH, 'loss_skipgram')\n",
    "SKIPGRAM_TSNE = os.path.join(OUTPUT_PATH, 'tsne_skipgram')\n",
    "\n",
    "## let's plot the loss for the cbow model\n",
    "CBOW_LOSS = os.path.join(OUTPUT_PATH, 'loss_cbow')\n",
    "CBOW_TSNE = os.path.join(OUTPUT_PATH, 'tsne_cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to preprocess the textual data\n",
    "\n",
    "# We can use tensorflow to preprocess the data\n",
    "import tensorflow as tf\n",
    "\n",
    "def tokenize_data(data):\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/text_to_word_sequence\n",
    "    tokenized_text = tf.keras.preprocessing.text.text_to_word_sequence(input_text=data)\n",
    "\n",
    "    vocab = sorted(set(tokenized_text))\n",
    "    tokenized_text_size = len(tokenized_text)\n",
    "\n",
    "    return (vocab, tokenized_text_size, tokenized_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the CBOW algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our imports \n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "from sklearn.manifold import TSNE\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load our data - we use the Lord of the Rings trilogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use google to load the data from drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "#datasets_dir = \"/content/drive/My Drive/DATA_340_3_NLP/Datasets/LOTR/\"\n",
    "\n",
    "datasets_dir = \"../datasets/LOTR/\"\n",
    "\n",
    "# get the txt files\n",
    "filenames = [os.path.join(datasets_dir, f) for f in os.listdir(datasets_dir) if f.endswith(\".txt\") and 'LOTR' in f]\n",
    "\n",
    "# read the files\n",
    "corpus = []\n",
    "\n",
    "# read \n",
    "for f in filenames:\n",
    "    with open(f, 'r', encoding='UTF-8') as file:\n",
    "        corpus.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's shorten the corpus\n",
    "corpus = corpus[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's flatten the corpus to one string and remove unnecessary spaces\n",
    "\n",
    "corpus = \" \".join(corpus)\n",
    "corpus = \" \".join(corpus.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.lower()\n",
    "\n",
    "# let's take the first 1000 words\n",
    "corpus = \" \".join(corpus.split()[:1000])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import unidecode\n",
    "except ModuleNotFoundError:\n",
    "  !pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Clean up the accents in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "corpus = unidecode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "(vocab, tokenized_text_size, tokenized_text) = tokenize_data(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets look at our data\n",
    "print(\"Vocab size: {}\".format(len(vocab)))\n",
    "print(\"Text size: {}\".format(tokenized_text_size))\n",
    "print(\"Text: {}\".format(tokenized_text[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create our context and center vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map our words to indices\n",
    "vocab_to_index = {\n",
    "    uniqueWord:index for (index, uniqueWord) in enumerate(vocab)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of our vocab\n",
    "index_to_vocab = np.array(vocab)\n",
    "index_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text to integers\n",
    "text_as_int = np.array([vocab_to_index[word] for word in tokenized_text])\n",
    "text_as_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Intialize our context and center vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of random data for our context vectors\n",
    "context_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenized_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "context_vector_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of random data for our center vectors\n",
    "center_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenized_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "center_vector_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define our optimizer\n",
    "\n",
    "<center><img src=\"images/Neuron.drawio.png\" width=\"800\" height=\"400\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T17:44:04.811310209Z",
     "start_time": "2023-10-03T17:44:04.697754898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define our optimizer\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "optimizer = tf.optimizers.Adam()\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train our CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the vectors for the context and center words\n",
    "for iter in tqdm(range(ITERATIONS)):\n",
    "    loss_per_epoch = 0 # initialize the loss per epoch to 0\n",
    "\n",
    "    # create our context slider\n",
    "    for start in range(tokenized_text_size - WINDOW_SIZE):\n",
    "        indices = text_as_int[start:start + WINDOW_SIZE]\n",
    "\n",
    "    # intialize the gradient for automatic differentiation\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        combined_context = 0 # initialize the combined context to 0\n",
    "\n",
    "        # loop through the indices to create the combined context\n",
    "        for count, index in enumerate(indices):\n",
    "            if count != WINDOW_SIZE // 2: # skip the center word\n",
    "                combined_context += context_vector_matrix[index, :] # add the context vector to the combined context\n",
    "        \n",
    "        combined_context /= (WINDOW_SIZE - 1) # divide by the window size minus the center word to create an average\n",
    "\n",
    "        # perform the matrix multiplication between the center vector and the combined context\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/linalg/matmul\n",
    "        output = tf.matmul(center_vector_matrix, tf.expand_dims(combined_context, 1))\n",
    "\n",
    "        # apply softmax to the output\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/softmax\n",
    "        softout = tf.nn.softmax(output, axis=0)\n",
    "        loss = softout[indices[WINDOW_SIZE // 2]] # get the loss for the center word\n",
    "\n",
    "        # compute the log loss (negative log likelihood)\n",
    "        logloss = -tf.math.log(loss)\n",
    "\n",
    "        # accumulate the loss per epoch : we want this number to decrease\n",
    "        loss_per_epoch += logloss.numpy()\n",
    "        \n",
    "        # compute the gradient of the loss with respect to the context and center vectors\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "        grad = tape.gradient(\n",
    "            logloss, [context_vector_matrix, center_vector_matrix]\n",
    "        )\n",
    "\n",
    "        # apply the gradient to the context and center vectors\n",
    "        optimizer.apply_gradients(\n",
    "            zip(grad, [context_vector_matrix, center_vector_matrix])\n",
    "        )\n",
    "\n",
    "        # append the loss per epoch to the loss list\n",
    "        loss_list.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "print(\"[INFO] Plotting loss ...\")\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(CBOW_LOSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Reduce the dimensionality of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the embeddings to 2D\n",
    "# tsne_embed = (\n",
    "#     TSNE(n_components=2)\n",
    "#     .fit_transform(center_vector_matrix.numpy())\n",
    "# )\n",
    "# tsne_decode = (\n",
    "#     TSNE(n_components=2)\n",
    "#     .fit_transform(context_vector_matrix.numpy())\n",
    "# )\n",
    "\n",
    "\n",
    "# Assuming center_vector_matrix and context_vector_matrix are available\n",
    "# center_vector_matrix = np.random.rand(100, 300)  # Example data\n",
    "# context_vector_matrix = np.random.rand(100, 300)  # Example data\n",
    "\n",
    "def compute_tsne(data):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    return tsne.fit_transform(data)\n",
    "\n",
    "# Using joblib to parallelize\n",
    "results = Parallel(n_jobs=-1)(delayed(compute_tsne)(data) for data in [center_vector_matrix, context_vector_matrix])\n",
    "\n",
    "tsne_embed, tsne_decode = results[0], results[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tsne embeddings\n",
    "if not os.path.exists(CBOW_TSNE):\n",
    "    os.makedirs(CBOW_TSNE)\n",
    "\n",
    "# save both the center and context vectors\n",
    "np.save(os.path.join(CBOW_TSNE, \"center_vectors\"), tsne_embed)\n",
    "np.save(os.path.join(CBOW_TSNE, \"context_vectors\"), tsne_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsne embeddings\n",
    "tsne_embed = np.load(os.path.join(CBOW_TSNE, \"center_vectors.npy\"))\n",
    "tsne_decode = np.load(os.path.join(CBOW_TSNE, \"context_vectors.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the embeddings for 100 words\n",
    "index_count = 0\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "print(\"[INFO] Plotting TSNE embeddings ...\")\n",
    "\n",
    "for (word, embedding) in tsne_decode[:100]:\n",
    "    # plot the point in 2d space\n",
    "    plt.scatter(word, embedding)\n",
    "    # annotate the point with the word\n",
    "    plt.annotate(index_to_vocab[index_count], (word, embedding))\n",
    "    index_count += 1\n",
    "plt.savefig(CBOW_TSNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the SKIPGRAM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## same as above but for skipgram\n",
    "(vocab, tokenize_text_size, tokenized_text) = tokenize_data(corpus)\n",
    "\n",
    "# Map our words to indices\n",
    "vocab_to_index = {\n",
    "    unique_word:index for (index, unique_word) in enumerate(vocab)\n",
    "}\n",
    "\n",
    "# Create an array of our vocab\n",
    "index_to_vocab = np.array(vocab)\n",
    "\n",
    "# convert the text to integers\n",
    "text_as_int = np.array([vocab_to_index[word] for word in tokenized_text])\n",
    "\n",
    "# Create a matrix of random data for our context vectors\n",
    "context_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenize_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "\n",
    "# Create a matrix of random data for our center vectors\n",
    "center_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenize_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "\n",
    "# Define our optimizer\n",
    "optimizer = tf.optimizers.Adam()\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train our SKIPGRAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in tqdm(range(ITERATIONS)):\n",
    "    loss_per_epoch = 0\n",
    "\n",
    "    for start in range(tokenize_text_size - WINDOW_SIZE):\n",
    "        indices = text_as_int[start:start + WINDOW_SIZE]\n",
    "        \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        # loop through the indices to create the combined context\n",
    "        center_vector = center_vector_matrix[indices[WINDOW_SIZE // 2], :]\n",
    "        \n",
    "        # multiply the center vector by the context vector matrix\n",
    "        output = tf.matmul(\n",
    "            context_vector_matrix, tf.expand_dims(center_vector, 1)\n",
    "        )\n",
    "\n",
    "        # apply softmax to the output\n",
    "        softmax_output = tf.nn.softmax(output, axis=0)\n",
    "\n",
    "        # compute the loss\n",
    "        for (count, index) in enumerate(indices):\n",
    "            if count != WINDOW_SIZE // 2: # skip the center word\n",
    "                loss += softmax_output[index]\n",
    "\n",
    "            # compute the log loss (negative log likelihood)\n",
    "            logloss = -tf.math.log(loss)\n",
    "\n",
    "        # accumulate the loss per epoch : we want this number to decrease\n",
    "        loss_per_epoch += logloss.numpy()\n",
    "        \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "        grad = tape.gradient(\n",
    "            logloss, [context_vector_matrix, center_vector_matrix]\n",
    "        )\n",
    "        \n",
    "        # apply the gradient to the context and center vectors\n",
    "        optimizer.apply_gradients(\n",
    "            zip(grad, [context_vector_matrix, center_vector_matrix])\n",
    "        )\n",
    "    # append our loss per epoch to the loss list\n",
    "    loss_list.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plot the loss for SKIPGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] plotting loss ...\")\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(SKIPGRAM_LOSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Reduce the dimensionality of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the embeddings to 2D\n",
    "tsneEmbed = (\n",
    "    TSNE(n_components=2)\n",
    "    .fit_transform(center_vector_matrix.numpy())\n",
    ")\n",
    "tsneDecode = (\n",
    "    TSNE(n_components=2)\n",
    "    .fit_transform(context_vector_matrix.numpy())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tsne embeddings\n",
    "if not os.path.exists(SKIPGRAM_TSNE):\n",
    "    os.makedirs(SKIPGRAM_TSNE)\n",
    "\n",
    "# save both the center and context vectors\n",
    "np.save(os.path.join(SKIPGRAM_TSNE, \"center_vectors\"), tsneEmbed)\n",
    "np.save(os.path.join(SKIPGRAM_TSNE, \"context_vectors\"), tsneDecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsne embeddings\n",
    "tsneEmbed = np.load(os.path.join(SKIPGRAM_TSNE, \"center_vectors.npy\"))\n",
    "tsneDecode = np.load(os.path.join(SKIPGRAM_TSNE, \"context_vectors.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexCount = 0 \n",
    "\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "print(\"[INFO] Plotting TSNE Embeddings...\")\n",
    "for (word, embedding) in tsneEmbed[100:200]:\n",
    "    plt.scatter(word, embedding)\n",
    "    plt.annotate(index_to_vocab[indexCount], (word, embedding))\n",
    "    indexCount += 1\n",
    "plt.savefig(SKIPGRAM_TSNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federalist Papers - Word2Vec with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the papers\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gensim\n",
    "\n",
    "# load the papers\n",
    "corpus_dir = '../datasets/Federalist_Papers/FedPapersCorpus/FedPapersCorpus'\n",
    "corpus_file_names = [f for f in os.listdir(corpus_dir) if f.endswith('.txt')]\n",
    "len(corpus_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our text corpus of a list of lists\n",
    "corpus = []\n",
    "for file_name in corpus_file_names:\n",
    "    with open(os.path.join(corpus_dir, file_name), 'r', encoding='utf-8') as file:\n",
    "        corpus.append(file.read())\n",
    "        \n",
    "assert len(corpus) == len(corpus_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # strip the nbsp\n",
    "    text = text.replace('&nbsp;||', ' ')\n",
    "    # strip tabs\n",
    "    text = text.replace('\\t', ' ')\n",
    "    # strip new lines\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "corpus = [clean_text(text) for text in corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the metadata\n",
    "import pandas as pd\n",
    "\n",
    "fed_df = pd.read_csv(Path(\"..\", \"datasets\", \"Federalist_Papers\", \"fedPapers85.csv\"))\n",
    "fed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the authors\n",
    "fed_df['author'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Word2Vec model with Gensim\n",
    "\n",
    "In order to train a Word2Vec model with Gensim, we need to install the Gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gensim\n",
    "except ModuleNotFoundError:\n",
    "    !pip install gensim\n",
    "    \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert our data to a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the corpus to lemmas\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocessor(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.lemma_ for sent in doc.sents]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = [preprocessor(text) for text in corpus]\n",
    "# flatten the corpus\n",
    "cleaned_corpus = [item for sublist in cleaned_corpus for item in sublist]\n",
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned corpus to disk as one file per text in the corpus\n",
    "corpus_dir = '../datasets/Federalist_Papers/FedPapersCorpus/FedPapersCorpus/processed'\n",
    "if not os.path.exists(corpus_dir):\n",
    "    os.makedirs(corpus_dir)\n",
    "\n",
    "with open(os.path.join(corpus_dir, 'fed_papers_cleaned.txt'), 'w', encoding='utf-8') as file:\n",
    "    file.write(\"\\n\".join(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generator to read the file\n",
    "corpus_file = os.path.join(corpus_dir, 'fed_papers_cleaned.txt')\n",
    "\n",
    "# yield the lines of the file\n",
    "def read_corpus(corpus_file):\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            yield line.split()\n",
    "            \n",
    "corpus = read_corpus(corpus_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "vocab = list(set([word for sentence in corpus for word in sentence]))\n",
    "\n",
    "model.build_vocab(list(corpus))\n",
    "\n",
    "# train a word2vec model\n",
    "model = Word2Vec(sentences=list(corpus),\n",
    "                 vector_size=300,\n",
    "                 window=5,\n",
    "                 min_count=5,\n",
    "                 workers=-1,\n",
    "                 epochs=10,\n",
    "                 max_vocab_size=len(vocab))\n",
    "\n",
    "# save the model\n",
    "# model.save(\"fed_papers.model\")\n",
    "\n",
    "# # load the model\n",
    "# model = Word2Vec.load(\"fed_papers.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the vocabulary words in a dataframe\n",
    "vocab = list(model.wv.index_to_key)\n",
    "\n",
    "vocab_df = pd.DataFrame(vocab, columns=[\"word\"])\n",
    "vocab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most similar words\n",
    "model.wv.most_similar(\"government\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Embeddings Projector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d99a3f87c484a74ba405ca572f7f1b4059e93a8c4d7f8027bf5ae12e7919d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
