{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/Fall_2023/notebooks/06_Vector_Semantics.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 15: 2023-28-03 Vector Semantics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of lecture\n",
    "\n",
    "- Introduction to lexical semantics\n",
    "- Introduction to Vector Semantics\n",
    "  - Vector semantics: Osgood et al. (1957)\n",
    "  - Vector semantics: Joos (1950), Harris (1954), Firth (1957)\n",
    "- Embeddings\n",
    "    - Word2Vec\n",
    "    - GloVe\n",
    "    - FastText\n",
    "    - ELMo\n",
    "    - BERT\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Neural Networks\n",
    "\n",
    "<center><img src=\"images/Neuron.drawio.png\" width=\"800\" height=\"400\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Caculating Loss (measuring error - training a model adjusts weights and biases to minimize loss)\n",
    "* Optimizing Loss (adjust weights and biases to minimize loss)\n",
    "* Backpropagation (calculate the gradient of the loss function with respect to the weights and biases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Lexical Semantics\n",
    "\n",
    "Taken from Jurafsky and Martin (2023) chapter 23:\n",
    "\n",
    "```\n",
    "\n",
    "Lady Bracknell: Are your parents living?\n",
    "Jack: I have lost both my parents.\n",
    "Lady Bracknell: To lose one parent, Mr. Worthing, may be regarded as a misfortune; to lose both looks like carelessness.\n",
    "\n",
    "```\n",
    "\n",
    "* words are relational units that are prone to messiness and ambiguity\n",
    "* Ambiguity is a fact of life in language (`mouse` as in a rodent or a computer device)\n",
    "* Polysemy: a word or lemma with multiple meanings (`bank` as in a river bank or a financial institution)\n",
    "* `Antonymy`: words (or lemmas) with opposite meanings (`hot` and `cold`)\n",
    "* `Synonym`: words (or lemmas) that are similar in meaning (`couch` and `sofa`)\n",
    "* Taxonimic relations\n",
    "    * `hyponymy` (subordinate): words (or lemmas) that are more specific (`poodle` is a hyponym of `dog`) - subclasses or members\n",
    "    * `hypernym` (superordinate): words (or lemmas) that are more general (`dog` is a hypernym of `poodle`) - classes\n",
    "        * entailment: being A entails being B (`dog` entails `poodle`)\n",
    "        * is-a hierarchy: a hierarchy of classes that is organized by the is-a relation or A IS-A B\n",
    "    * `meronymy`: words (or lemmas) that are part of a larger entity (`leg` is a meronym of `human`) - part-whole relationships\n",
    "    * `metonymy`: words (or lemmas) that are associated with a larger entity (`the crown` is a metonym of `the queen`) - association (prototype categories)\n",
    "    * `holonymy`: words (or lemmas) that are a whole of a smaller entity (`face` is a holonym of `eye`) - whole-part relationships\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our datasets dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set environment variables if in google colab\n",
    "import os\n",
    "\n",
    "IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # mount our google drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    data_dir = \"/content/drive/MyDrive/DATA_340_NLP/Datasets\"\n",
    "else:\n",
    "    data_dir = \"../datasets\"\n",
    "    \n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_taxonomy(noun):\n",
    "    synsets = wn.synsets(noun)\n",
    "    if synsets:\n",
    "        synset = synsets[0]  # take the first synset\n",
    "        hypernyms = synset.hypernyms()\n",
    "        hyponyms = synset.hyponyms()\n",
    "        meronyms = synset.part_meronyms() + synset.substance_meronyms() + synset.member_holonyms()\n",
    "        holonyms = synset.part_holonyms() + synset.substance_holonyms() + synset.member_meronyms()\n",
    "        return {\n",
    "            \"word\": synset.name(),\n",
    "            \"definition\": synset.definition(),\n",
    "            \"hypernyms\": [h.name() for h in hypernyms],\n",
    "            \"hyponyms\": [h.name() for h in hyponyms],\n",
    "            \"meronyms\": [m.name() for m in meronyms],\n",
    "            \"holonyms\": [h.name() for h in holonyms]\n",
    "        }\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_taxonomy(\"dog\")\n",
    "for k,v in result.items():\n",
    "    print(k, v, sep=\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_verb_relations(verb):\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    if synsets:\n",
    "        relations = {\n",
    "            \"antonyms\": set(),\n",
    "            \"entailments\": set(),\n",
    "            \"causes\": set(),\n",
    "            \"also_sees\": set(),\n",
    "            \"verb_groups\": set(),\n",
    "            \"similar_tos\": set()\n",
    "        }\n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                antonyms = lemma.antonyms()\n",
    "                if antonyms:\n",
    "                    relations[\"antonyms\"].add(antonyms[0].name())\n",
    "            for entailment in synset.entailments():\n",
    "                relations[\"entailments\"].add(entailment.name())\n",
    "            for cause in synset.causes():\n",
    "                relations[\"causes\"].add(cause.name())\n",
    "            for also_see in synset.also_sees():\n",
    "                relations[\"also_sees\"].add(also_see.name())\n",
    "            for verb_group in synset.verb_groups():\n",
    "                relations[\"verb_groups\"].add(verb_group.name())\n",
    "            for similar in synset.similar_tos():\n",
    "                relations[\"similar_tos\"].add(similar.name())\n",
    "        return relations\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_verb_relations(\"catch\")\n",
    "for k,v in result.items():\n",
    "    print(k, v, sep=\":\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Semantics\n",
    "\n",
    "* Firth (1957) proposed a model of word meaning based on the idea that words are associated with other words in a network of semantic relations.\n",
    "* Firth (1957), Joos (1950), and Harris (1954) all proposed models of word meaning based on the idea that words are associated with other words in a network of semantic relations. Thus the idea of distributional semantics takes its name from the fact that the meaning of a word is discerned by the words that tend to occur in its company.\n",
    "\n",
    "> You shall know a word by the company it keeps. (Firth, 1957)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity\n",
    "\n",
    "* Word similarity is a measure of the degree of semantic similarity between two words. This measure takes into account the distributional properties of words in a corpus. Whereas words like `coffee` would rarely occur in a dictionary entry for the word `cup`, users of language expect that the words `coffee` and `cup` are similar in meaning. They are similar, in this case, because semantic frames are shared between the two words. The semantic frame of `coffee` is a hot beverage, and the semantic frame of `cup` is a container for a hot beverage. The semantic frames of `coffee` and `cup` overlap, and this overlap is the basis for the similarity between the two words. We can capture these similarities by computing the distributional properties of words in a corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we represent words and their meanings in numerical format?\n",
    "\n",
    "We vectorize it!\n",
    "\n",
    "We can represent words in a vector space or embedding space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec, Mikolov et al., 2013\n",
    "\n",
    "Goal: to create “techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity.” (Mikolov, et al., 2013a, 2013b)\n",
    "\n",
    "Mikolov et al. propose two log-linear solutions\n",
    "\n",
    "* Continuous Bag-of-Words Model\n",
    "* Continuous Skip-gram Model \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/mikolov.png\" width=\"900\" height=\"500\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec embeddings are static embeddings, and therefore they do not capture the cooccurrence of words in a sentence. This is a problem for downstream tasks that require contextualized embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove, Pennington et al., 2014"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“...the shallow window-based methods [e.g., log bi-linear models, CBOW, or Skipgram] suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the entire corpus, which fails to take advantage of the vast amount of repetition in the data.” Pennington, et al., 2014."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/glove.png\" width=\"800\" height=\"400\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText, Bojanowski et al., 2017"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/fasttext.png\" width=\"900\" height=\"500\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elmo, Peters et al., 2018"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/elmo.png\" width=\"900\" height=\"400\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"They [embeddings] should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).\" ([Peters et al., 2018, p. 1](https://arxiv.org/pdf/1802.05365.pdf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create static word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code out the word2vec CBOW and Skipgram models and compare them. To do this, let's define our configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Number of dimensions\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "# Window size\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "ITERATIONS = 10000\n",
    "\n",
    "# OUTPUT\n",
    "OUTPUT_PATH = \"outputs\"\n",
    "\n",
    "## Let's plot the loss for the skipgram model\n",
    "SKIPGRAM_LOSS = os.path.join(OUTPUT_PATH, 'loss_skipgram')\n",
    "SKIPGRAM_TSNE = os.path.join(OUTPUT_PATH, 'tsne_skipgram')\n",
    "\n",
    "## let's plot the loss for the cbow model\n",
    "CBOW_LOSS = os.path.join(OUTPUT_PATH, 'loss_cbow')\n",
    "CBOW_TSNE = os.path.join(OUTPUT_PATH, 'tsne_cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to preprocess the textual data\n",
    "\n",
    "# We can use tensorflow to preprocess the data\n",
    "import tensorflow as tf\n",
    "\n",
    "def tokenize_data(data):\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/text_to_word_sequence\n",
    "    tokenized_text = tf.keras.preprocessing.text.text_to_word_sequence(input_text=data)\n",
    "\n",
    "    vocab = sorted(set(tokenized_text))\n",
    "    tokenized_text_size = len(tokenized_text)\n",
    "\n",
    "    return (vocab, tokenized_text_size, tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the CBOW algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our imports \n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "from sklearn.manifold import TSNE\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load our data - we use the Lord of the Rings trilogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use google to load the data from drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)\n",
    "#datasets_dir = \"/content/drive/My Drive/DATA_340_3_NLP/Datasets/LOTR/\"\n",
    "\n",
    "datasets_dir = \"../datasets/LOTR/\"\n",
    "\n",
    "# get the txt files\n",
    "filenames = [os.path.join(datasets_dir, f) for f in os.listdir(datasets_dir) if f.endswith(\".txt\") and 'LOTR' in f]\n",
    "\n",
    "# read the files\n",
    "corpus = []\n",
    "\n",
    "# read \n",
    "for f in filenames:\n",
    "    with open(f, 'r', encoding='UTF-8') as file:\n",
    "        corpus.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's shorten the corpus\n",
    "corpus = corpus[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's flatten the corpus to one string and remove unnecessary spaces\n",
    "corpus = \" \".join(corpus)\n",
    "corpus = \" \".join(corpus.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize the case of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.lower()\n",
    "\n",
    "# let's take the first 1000 words\n",
    "corpus = \" \".join(corpus.split()[:1000])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import unidecode\n",
    "except ModuleNotFoundError:\n",
    "  !pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "corpus = unidecode(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "(vocab, tokenized_text_size, tokenized_text) = tokenize_data(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets look at our data\n",
    "print(\"Vocab size: {}\".format(len(vocab)))\n",
    "print(\"Text size: {}\".format(tokenized_text_size))\n",
    "print(\"Text: {}\".format(tokenized_text[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Create our context and center vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map our words to indices\n",
    "vocab_to_index = {\n",
    "    uniqueWord:index for (index, uniqueWord) in enumerate(vocab)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of our vocab\n",
    "index_to_vocab = np.array(vocab)\n",
    "index_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text to integers\n",
    "text_as_int = np.array([vocab_to_index[word] for word in tokenized_text])\n",
    "text_as_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to slide over our text and create our context and center vectors. Let's illustrate with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_window_over_tokens(tokens, window_size):\n",
    "    \"\"\"\n",
    "    Slides a window over the given list of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    - tokens: List of words/tokens in a sentence.\n",
    "    - window_size: The total size of the window, including the target word and context words.\n",
    "    \n",
    "    Yields:\n",
    "    - The position of the target word and the words within the window around it.\n",
    "    \"\"\"\n",
    "    for index, word in enumerate(tokens):\n",
    "        start = max(0, index - window_size // 2)\n",
    "        end = min(len(tokens), index + window_size // 2 + 1)\n",
    "        window = tokens[start:end]\n",
    "        print(f\"Target: {word}, Window: {window}\")\n",
    "\n",
    "# Example usage\n",
    "tokens = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "window_size = 5  # This means 1 word before and 1 word after the target\n",
    "slide_window_over_tokens(tokens, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Intialize our context and center vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of random data for our context vectors\n",
    "context_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenized_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "context_vector_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of random data for our center vectors\n",
    "center_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenized_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "center_vector_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define our optimizer\n",
    "\n",
    "Word2Vec employs two architectures for producing a distributed representation of words: Continuous Bag of Words (CBOW) and Skip-Gram. Both architectures use a shallow neural network model for learning word embeddings, but they differ in the way they predict words.\n",
    "\n",
    "CBOW predicts a target word based on context words surrounding it. The objective is to estimate the probability of a word given a context.\n",
    "Skip-Gram, on the other hand, uses a target word to predict context words. This model aims to maximize the probability of context words given a target word.\n",
    "\n",
    "The optimization process in Word2Vec involves adjusting the weights of the neural network to minimize a loss function. This loss function measures the difference between the predicted probability distribution of context words and the actual distribution from the corpus. For CBOW, the loss function could be the negative log likelihood of the target word given the context. For Skip-Gram, it involves the sum of the negative log likelihoods for each context word given the target word.\n",
    "\n",
    "<center><img src=\"images/Neuron.drawio.png\" width=\"800\" height=\"400\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code for updating weights in Word2Vec optimization\n",
    "def update_weights(weights, learning_rate, gradient):\n",
    "    # Update the weights by moving a small step in the direction of the gradient\n",
    "    new_weights = weights - learning_rate * gradient\n",
    "    return new_weights\n",
    "\n",
    "# Example values (in a real scenario, these would be computed based on your model and data)\n",
    "weights = 0.5  # Initial weights\n",
    "learning_rate = 0.01  # Learning rate\n",
    "gradient = 0.2  # Example gradient\n",
    "\n",
    "# Update weights based on gradient\n",
    "new_weights = update_weights(weights, learning_rate, gradient)\n",
    "print(f\"Updated weights: {new_weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T17:44:04.811310209Z",
     "start_time": "2023-10-03T17:44:04.697754898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define our optimizer\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "optimizer = tf.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_update(weights, gradients, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    # Update biased first moment estimate\n",
    "    m = [beta1 * m_i + (1 - beta1) * g for m_i, g in zip(m, gradients)]\n",
    "    # Update biased second raw moment estimate\n",
    "    v = [beta2 * v_i + (1 - beta2) * (g ** 2) for v_i, g in zip(v, gradients)]\n",
    "    # Compute bias-corrected first moment estimate\n",
    "    m_hat = [m_i / (1 - beta1 ** t) for m_i in m]\n",
    "    # Compute bias-corrected second raw moment estimate\n",
    "    v_hat = [v_i / (1 - beta2 ** t) for v_i in v]\n",
    "    # Update weights\n",
    "    weights = [w - learning_rate * m_i / (v_i ** 0.5 + epsilon) for w, m_i, v_i in zip(weights, m_hat, v_hat)]\n",
    "    return weights, m, v\n",
    "\n",
    "# Example usage\n",
    "weights = [0.1, 0.2]  # Example weights\n",
    "gradients = [0.01, -0.02]  # Example gradients\n",
    "m = [0, 0]  # Initial first moment vector\n",
    "v = [0, 0]  # Initial second moment vector\n",
    "t = 1  # Time step\n",
    "\n",
    "# Update weights using Adam\n",
    "weights, m, v = adam_update(weights, gradients, m, v, t)\n",
    "print(\"Updated weights:\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam (Adaptive moment estimation) optimization is based on adaptive estimates of lower-order moments. The algorithm maintains two moving averages for each weight in the network: one for gradients ($m_t$) and one for the square of gradients ($v_t$). These moving averages are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively.\n",
    "\n",
    "The Adam update rule is given by:\n",
    "\n",
    "- $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$\n",
    "- $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$\n",
    "\n",
    "General values:\n",
    "\n",
    "- $\\beta_1$ = 0.9 (dw)\n",
    "- $\\beta_2$ = 0.999 (dw^2)\n",
    "- $\\epsilon$ = $10^{-8}$\n",
    "\n",
    "where:\n",
    "- $m_t$ is the biased first moment estimate,\n",
    "- $v_t$ is the biased second moment estimate,\n",
    "- $g_t$ is the gradient at time step $t$,\n",
    "- $\\beta_1$ and $\\beta_2$ are exponential decay rates for the moment estimates, typically close to 1.\n",
    "\n",
    "To correct for their initialization bias towards zero, Adam computes bias-corrected versions of these moving averages:\n",
    "\n",
    "- $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$\n",
    "- $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n",
    "\n",
    "Finally, the weights are updated with:\n",
    "\n",
    "- $w_{t+1} = w_t - \\frac{\\eta \\cdot \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n",
    "\n",
    "where:\n",
    "- $w_{t+1}$ is the updated weight,\n",
    "- $\\eta$ is the learning rate,\n",
    "- $\\epsilon$ is a small number to prevent division by zero, often $10^{-8}$.\n",
    "\n",
    "### Role in Optimization\n",
    "\n",
    "The role of the Adam optimizer in neural network training is to adaptively adjust the learning rate for each weight. This means it scales the step size by an estimate of the first and second moments of the gradients. This adaptability helps in dealing with sparse gradients and different curvature across parameters, making Adam well-suited for a wide range of problems and data types.\n",
    "\n",
    "For additional information: https://youtu.be/JXQT_vxqwIs?si=XhDqIou_jHLWlnHw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train our CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the vectors for the context and center words\n",
    "for iter in tqdm(range(ITERATIONS)):\n",
    "    loss_per_epoch = 0 # initialize the loss per epoch to 0\n",
    "\n",
    "    # create our context slider\n",
    "    for start in range(tokenized_text_size - WINDOW_SIZE):\n",
    "        indices = text_as_int[start:start + WINDOW_SIZE]\n",
    "\n",
    "    # intialize the gradient for automatic differentiation\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        combined_context = 0 # initialize the combined context to 0\n",
    "\n",
    "        # loop through the indices to create the combined context\n",
    "        for count, index in enumerate(indices):\n",
    "            if count != WINDOW_SIZE // 2: # skip the center word\n",
    "                combined_context += context_vector_matrix[index, :] # add the context vector to the combined context\n",
    "        \n",
    "        combined_context /= (WINDOW_SIZE - 1) # divide by the window size minus the center word to create an average\n",
    "\n",
    "        # perform the matrix multiplication between the center vector and the combined context\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/linalg/matmul\n",
    "        output = tf.matmul(center_vector_matrix, tf.expand_dims(combined_context, 1))\n",
    "\n",
    "        # apply softmax to the output\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/softmax\n",
    "        softout = tf.nn.softmax(output, axis=0)\n",
    "        loss = softout[indices[WINDOW_SIZE // 2]] # get the loss for the center word\n",
    "\n",
    "        # compute the log loss (negative log likelihood)\n",
    "        logloss = -tf.math.log(loss)\n",
    "\n",
    "        # accumulate the loss per epoch : we want this number to decrease\n",
    "        loss_per_epoch += logloss.numpy()\n",
    "        \n",
    "        # compute the gradient of the loss with respect to the context and center vectors\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "        grad = tape.gradient(\n",
    "            logloss, [context_vector_matrix, center_vector_matrix]\n",
    "        )\n",
    "\n",
    "        # apply the gradient to the context and center vectors\n",
    "        optimizer.apply_gradients(\n",
    "            zip(grad, [context_vector_matrix, center_vector_matrix])\n",
    "        )\n",
    "\n",
    "        # append the loss per epoch to the loss list\n",
    "        loss_list.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "print(\"[INFO] Plotting loss ...\")\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(CBOW_LOSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Reduce the dimensionality of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the embeddings to 2D\n",
    "# tsne_embed = (\n",
    "#     TSNE(n_components=2)\n",
    "#     .fit_transform(center_vector_matrix.numpy())\n",
    "# )\n",
    "# tsne_decode = (\n",
    "#     TSNE(n_components=2)\n",
    "#     .fit_transform(context_vector_matrix.numpy())\n",
    "# )\n",
    "\n",
    "\n",
    "# Assuming center_vector_matrix and context_vector_matrix are available\n",
    "# center_vector_matrix = np.random.rand(100, 300)  # Example data\n",
    "# context_vector_matrix = np.random.rand(100, 300)  # Example data\n",
    "\n",
    "def compute_tsne(data):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    return tsne.fit_transform(data)\n",
    "\n",
    "# Using joblib to parallelize\n",
    "results = Parallel(n_jobs=-1)(delayed(compute_tsne)(data) for data in [center_vector_matrix, context_vector_matrix])\n",
    "\n",
    "tsne_embed, tsne_decode = results[0], results[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tsne embeddings\n",
    "if not os.path.exists(CBOW_TSNE):\n",
    "    os.makedirs(CBOW_TSNE)\n",
    "\n",
    "# save both the center and context vectors\n",
    "np.save(os.path.join(CBOW_TSNE, \"center_vectors\"), tsne_embed)\n",
    "np.save(os.path.join(CBOW_TSNE, \"context_vectors\"), tsne_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsne embeddings\n",
    "tsne_embed = np.load(os.path.join(CBOW_TSNE, \"center_vectors.npy\"))\n",
    "tsne_decode = np.load(os.path.join(CBOW_TSNE, \"context_vectors.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the embeddings for 100 words\n",
    "index_count = 0\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "print(\"[INFO] Plotting TSNE embeddings ...\")\n",
    "\n",
    "for (word, embedding) in tsne_decode[:100]:\n",
    "    # plot the point in 2d space\n",
    "    plt.scatter(word, embedding)\n",
    "    # annotate the point with the word\n",
    "    plt.annotate(index_to_vocab[index_count], (word, embedding))\n",
    "    index_count += 1\n",
    "plt.savefig(CBOW_TSNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the SKIPGRAM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## same as above but for skipgram\n",
    "(vocab, tokenize_text_size, tokenized_text) = tokenize_data(corpus)\n",
    "\n",
    "# Map our words to indices\n",
    "vocab_to_index = {\n",
    "    unique_word:index for (index, unique_word) in enumerate(vocab)\n",
    "}\n",
    "\n",
    "# Create an array of our vocab\n",
    "index_to_vocab = np.array(vocab)\n",
    "\n",
    "# convert the text to integers\n",
    "text_as_int = np.array([vocab_to_index[word] for word in tokenized_text])\n",
    "\n",
    "# Create a matrix of random data for our context vectors\n",
    "context_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenize_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "\n",
    "# Create a matrix of random data for our center vectors\n",
    "center_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenize_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "\n",
    "# Define our optimizer\n",
    "optimizer = tf.optimizers.Adam()\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Train our SKIPGRAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in tqdm(range(ITERATIONS)):\n",
    "    loss_per_epoch = 0\n",
    "\n",
    "    for start in range(tokenize_text_size - WINDOW_SIZE):\n",
    "        indices = text_as_int[start:start + WINDOW_SIZE]\n",
    "        \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        # loop through the indices to create the combined context\n",
    "        center_vector = center_vector_matrix[indices[WINDOW_SIZE // 2], :]\n",
    "        \n",
    "        # multiply the center vector by the context vector matrix\n",
    "        output = tf.matmul(\n",
    "            context_vector_matrix, tf.expand_dims(center_vector, 1)\n",
    "        )\n",
    "\n",
    "        # apply softmax to the output\n",
    "        softmax_output = tf.nn.softmax(output, axis=0)\n",
    "\n",
    "        # compute the loss\n",
    "        for (count, index) in enumerate(indices):\n",
    "            if count != WINDOW_SIZE // 2: # skip the center word\n",
    "                loss += softmax_output[index]\n",
    "\n",
    "            # compute the log loss (negative log likelihood)\n",
    "            logloss = -tf.math.log(loss)\n",
    "\n",
    "        # accumulate the loss per epoch : we want this number to decrease\n",
    "        loss_per_epoch += logloss.numpy()\n",
    "        \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "        grad = tape.gradient(\n",
    "            logloss, [context_vector_matrix, center_vector_matrix]\n",
    "        )\n",
    "        \n",
    "        # apply the gradient to the context and center vectors\n",
    "        optimizer.apply_gradients(\n",
    "            zip(grad, [context_vector_matrix, center_vector_matrix])\n",
    "        )\n",
    "    # append our loss per epoch to the loss list\n",
    "    loss_list.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plot the loss for SKIPGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] plotting loss ...\")\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(SKIPGRAM_LOSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Reduce the dimensionality of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the embeddings to 2D\n",
    "tsneEmbed = (\n",
    "    TSNE(n_components=2)\n",
    "    .fit_transform(center_vector_matrix.numpy())\n",
    ")\n",
    "tsneDecode = (\n",
    "    TSNE(n_components=2)\n",
    "    .fit_transform(context_vector_matrix.numpy())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tsne embeddings\n",
    "if not os.path.exists(SKIPGRAM_TSNE):\n",
    "    os.makedirs(SKIPGRAM_TSNE)\n",
    "\n",
    "# save both the center and context vectors\n",
    "np.save(os.path.join(SKIPGRAM_TSNE, \"center_vectors\"), tsneEmbed)\n",
    "np.save(os.path.join(SKIPGRAM_TSNE, \"context_vectors\"), tsneDecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tsne embeddings\n",
    "tsneEmbed = np.load(os.path.join(SKIPGRAM_TSNE, \"center_vectors.npy\"))\n",
    "tsneDecode = np.load(os.path.join(SKIPGRAM_TSNE, \"context_vectors.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexCount = 0 \n",
    "\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "print(\"[INFO] Plotting TSNE Embeddings...\")\n",
    "for (word, embedding) in tsneEmbed[100:200]:\n",
    "    plt.scatter(word, embedding)\n",
    "    plt.annotate(index_to_vocab[indexCount], (word, embedding))\n",
    "    indexCount += 1\n",
    "plt.savefig(SKIPGRAM_TSNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federalist Papers - Word2Vec with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the papers\n",
    "import os\n",
    "from pathlib import Path\n",
    "import gensim\n",
    "\n",
    "# load the papers\n",
    "corpus_dir = '../datasets/Federalist_Papers/FedPapersCorpus/FedPapersCorpus'\n",
    "corpus_file_names = [f for f in os.listdir(corpus_dir) if f.endswith('.txt')]\n",
    "len(corpus_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our text corpus of a list of lists\n",
    "corpus = []\n",
    "for file_name in corpus_file_names:\n",
    "    with open(os.path.join(corpus_dir, file_name), 'r', encoding='utf-8') as file:\n",
    "        corpus.append(file.read())\n",
    "        \n",
    "assert len(corpus) == len(corpus_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # strip the nbsp\n",
    "    text = text.replace('&nbsp;||', ' ')\n",
    "    # strip tabs\n",
    "    text = text.replace('\\t', ' ')\n",
    "    # strip new lines\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "corpus = [clean_text(text) for text in corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the metadata\n",
    "import pandas as pd\n",
    "\n",
    "fed_df = pd.read_csv(Path(\"..\", \"datasets\", \"Federalist_Papers\", \"fedPapers85.csv\"))\n",
    "fed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the authors\n",
    "fed_df['author'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Word2Vec model with Gensim\n",
    "\n",
    "In order to train a Word2Vec model with Gensim, we need to install the Gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import gensim\n",
    "except ModuleNotFoundError:\n",
    "    !pip install gensim\n",
    "    \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert our data to a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the corpus to lemmas\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# reduce text to lemmas and strip punctuation\n",
    "def preprocessor(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sentence for sentence in doc.sents]\n",
    "    # replace the sentence with the lemmatized version\n",
    "    # lemmas = \" \".join([word.lemma_.lower() for sentence in sentences for word in sentence if not word.is_punct])\n",
    "    # alternate version - return the lemma of only nouns\n",
    "    lemmas = \" \".join([word.lemma_.lower() for sentence in sentences for word in sentence if word.pos_ == \"NOUN\"])\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = [preprocessor(text) for text in corpus]\n",
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned corpus to disk as one file per text in the corpus\n",
    "corpus_dir = '../datasets/Federalist_Papers/FedPapersCorpus/FedPapersCorpus/processed'\n",
    "if not os.path.exists(corpus_dir):\n",
    "    os.makedirs(corpus_dir)\n",
    "\n",
    "with open(os.path.join(corpus_dir, 'fed_papers_nouns.txt'), 'w', encoding='utf-8') as file:\n",
    "    file.write(\"\\n\".join(cleaned_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generator to read the file\n",
    "corpus_file = os.path.join(corpus_dir, 'fed_papers_nouns.txt')\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open(corpus_file, 'r', encoding='utf-8'):\n",
    "            yield line.split()\n",
    "            \n",
    "sentences = MyCorpus()\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the first 10 sentences\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(sentence)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train a word2vec model\n",
    "model = Word2Vec(sentences=sentences,\n",
    "                 vector_size=300,\n",
    "                 sg=1,\n",
    "                 window=5,\n",
    "                 compute_loss=True,\n",
    "                 min_count=5,\n",
    "                 workers=-1,\n",
    "                 epochs=5000)\n",
    "\n",
    "# save the model\n",
    "model.save(\"fed_papers.model\")\n",
    "\n",
    "# # load the model\n",
    "model = Word2Vec.load(\"fed_papers.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the vocabulary words in a dataframe\n",
    "vocab = list(model.wv.index_to_key)\n",
    "\n",
    "vocab_df = pd.DataFrame(vocab, columns=[\"word\"])\n",
    "vocab_df[vocab_df.word.str.contains('gover*', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most similar words\n",
    "model.wv.most_similar(\"government\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the word embeddings with tensorboard\n",
    "words = list(model.wv.index_to_key)\n",
    "vectors = model.wv.vectors\n",
    "\n",
    "# save the data to disk as embeddings and metadata\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "LOG_DIR = \"logs\"\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "# save the words to disk as metadata\n",
    "with open(os.path.join(LOG_DIR, \"metadata_noun.tsv\"), \"w\", encoding=\"utf-8\") as file:\n",
    "    for word in words:\n",
    "        file.write(f\"{word}\\n\")\n",
    "\n",
    "# save the vectors to dist as embeddings\n",
    "with open(os.path.join(LOG_DIR, \"embeddings_noun.tsv\"), \"w\", encoding=\"utf-8\") as file:\n",
    "    for vector in vectors:\n",
    "        file.write(\"\\t\".join([str(x) for x in vector]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pretrained word2vec model\n",
    "import gensim.downloader as api\n",
    "\n",
    "# get the list of available models\n",
    "api.info()\n",
    "\n",
    "# list the available models\n",
    "models = api.info()['models']\n",
    "print(\"\\n\".join(models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word2vec model google news\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# get the most similar words for government\n",
    "word2vec_model.most_similar(\"government\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to pose the question: \"What is the capital of France?\"\n",
    "# We can do this by computing the vector for \"Paris\" - \"France\" + \"Italy\"\n",
    "# We expect the most similar word to be \"Rome\"\n",
    "word2vec_model.most_similar(positive=[\"Paris\", \"Italy\"], negative=[\"France\"], topn=10)\n",
    "\n",
    "# we can to the same for \"Berlin\" - \"Germany\" + \"France\"\n",
    "word2vec_model.most_similar(positive=[\"Berlin\", \"France\"], negative=[\"Germany\"], topn=10)\n",
    "\n",
    "# and genderized words\n",
    "word2vec_model.most_similar(positive=[\"King\", \"Queen\"], negative=[\"Man\"], topn=10)\n",
    "\n",
    "# and topics\n",
    "word2vec_model.most_similar(positive=[\"Nuclear\", \"Energy\"], topn=10)\n",
    "\n",
    "# and sports\n",
    "word2vec_model.most_similar(positive=[\"Tennis\", \"Soccer\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document to Vector\n",
    "\n",
    "With our word embeddings, we can now convert our documents to vectors. This is a common technique in NLP and is used in many applications such as document classification, clustering, and information retrieval. But we have different methodologies we can use to convert our documents to vectors.\n",
    "\n",
    "### Average Word Embeddings\n",
    "\n",
    "One simple way to convert a document to a vector is to average the word embeddings of the words in the document. This is a simple way to convert a document to a vector. We can then use this vector to compare documents using cosine similarity. This method, however, is naive in several respects, and there are more sophisticated methods for converting documents to vectors. You should be aware of how algorithms generate document vectors, as you may find the results unsatisfactory - yet have ideas on how to improve them.\n",
    "\n",
    "### Doc2Vec\n",
    "\n",
    "In the `gensim` library, we can use the `Doc2Vec` model to convert documents to vectors. The `Doc2Vec` model is an extension of the `Word2Vec` model, and it is used to convert documents to vectors. The `Doc2Vec` model is an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Clinton Email Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "clinton_emails = pd.read_csv(\"../datasets/Clinton_Emails/Emails.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_emails.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's concatenate the text fields\n",
    "clinton_emails['text'] = clinton_emails['ExtractedSubject'].fillna('') + \" \" + clinton_emails['ExtractedBodyText'].fillna('')\n",
    "\n",
    "clinton_emails.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text: str) -> str:\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Word2Vec model with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Preprocess the text data\n",
    "preprocessed_texts = [preprocessor(text) for text in clinton_emails['ExtractedBodyText'].fillna('')]\n",
    "\n",
    "# Create tagged documents\n",
    "tagged_documents = [TaggedDocument(words=text.split(), tags=[i]) for i, text in enumerate(preprocessed_texts)]\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "model = Doc2Vec(tagged_documents, vector_size=300, window=5, min_count=5, workers=-1, epochs=5000)\n",
    "\n",
    "# Get the document vectors\n",
    "document_vectors = [model.infer_vector(tagged_document.words) for tagged_document in tagged_documents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the documents to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors_numpy = np.array(document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(\"clinton_emails.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = Doc2Vec.load(\"clinton_emails.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the document vectors from the model to visualize with tensorboard\n",
    "LOG_DIR = \"logs\"\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "# save the document vectors to disk as embeddings\n",
    "with open(os.path.join(LOG_DIR, \"embeddings_clinton_emails.tsv\"), \"w\", encoding=\"utf-8\") as file:\n",
    "    for vector in document_vectors:\n",
    "        file.write(\"\\t\".join([str(x) for x in vector]) + \"\\n\")\n",
    "\n",
    "# save the document vectors to disk as metadata\n",
    "with open(os.path.join(LOG_DIR, \"metadata_clinton_emails.tsv\"), \"w\", encoding=\"utf-8\") as file:\n",
    "    for i in range(len(preprocessed_texts)):\n",
    "        file.write(f\"{i}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Specify the number of clusters\n",
    "num_clusters = 8\n",
    "\n",
    "# Initialize the KMeans model\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "# Fit the model to the document vectors\n",
    "kmeans.fit(document_vectors)\n",
    "\n",
    "# Get the cluster labels for each document\n",
    "cluster_labels = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the cluster labels onto the dataframe\n",
    "clinton_emails['cluster'] = cluster_labels\n",
    "\n",
    "# merge the document vectors onto the dataframe\n",
    "clinton_emails['document_vector'] = document_vectors\n",
    "\n",
    "# drop all except the DocNumber, MetadataSent, MetadataFrom, document_vector, and cluster columns\n",
    "clinton_emails_clustered = clinton_emails[['DocNumber', 'MetadataDateSent', 'MetadataFrom', 'document_vector', 'cluster']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_emails_clustered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tSNE model\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# Fit the model to the document vectors\n",
    "document_vectors_2d = tsne.fit_transform(document_vectors_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the unique cluster IDs\n",
    "unique_clusters = clinton_emails_clustered['cluster'].unique()\n",
    "\n",
    "# Plot the document vectors\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster_id in unique_clusters:\n",
    "    cluster_vectors = document_vectors_2d[clinton_emails_clustered['cluster'] == cluster_id]\n",
    "    plt.scatter(cluster_vectors[:, 0], cluster_vectors[:, 1], label=f'Cluster {cluster_id}')\n",
    "\n",
    "plt.title('2D Document Vectors')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Reduce the dimensionality of the document vectors using PCA\n",
    "pca = PCA(n_components=25, random_state=42)\n",
    "document_vectors_pca = pca.fit_transform(document_vectors_numpy)\n",
    "\n",
    "# Perform agglomerative clustering on the document clusters\n",
    "agglomerative = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "cluster_labels_agglomerative = agglomerative.fit_predict(document_vectors_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the agglomerative clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_vectors = document_vectors_2d[cluster_labels_agglomerative == cluster_id]\n",
    "    plt.scatter(cluster_vectors[:, 0], cluster_vectors[:, 1], label=f'Cluster {cluster_id}')\n",
    "\n",
    "plt.title('Agglomerative Clusters')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the clusters\n",
    "clinton_emails_clustered[clinton_emails_clustered['cluster_x'] == 7].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_emails_clustered.iloc[280]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Embeddings Projector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d99a3f87c484a74ba405ca572f7f1b4059e93a8c4d7f8027bf5ae12e7919d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
