{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 14: 2023-21-03 Logistic Regression Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture overview\n",
    "\n",
    "- Naive Bayes assignment due\n",
    "- Project milestones check-in\n",
    "- Logistic regression model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model\n",
    "\n",
    "Logistic regression (or logit regression) is a statistical model that in its basic form uses a logistic function to estimate the probability of a binary dependent variable and its relationship to class membership. In this lecture, we will analyse the sentiment of tweets and movie reviews using logistic regression.\n",
    "\n",
    "Some important ideas we introduce in this lecture:\n",
    "\n",
    "* Sigmoid function\n",
    "* Weights and bias\n",
    "* Gradient descent\n",
    "* Cross-entropy loss function\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the supervised machine learning process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/super_ml.png\" height=\"400\" width=\"800\">\n",
    "\n",
    "Image credit: deeplearning.ai "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sigmoid function\n",
    "\n",
    "The sigmoid function is a mathematical function that has a characteristic \"S\" shape. It is defined as:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function in Python\n",
    "\n",
    "We can implement the sigmoid function in Python as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code the sigmoid function\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# plot the sigmoid function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "## plot the results\n",
    "plt.plot(x,y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the sigmoid function in logistic regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/lr.png\" height=\"400\" width=\"800\">\n",
    "\n",
    "Image credit: deeplearning.ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the sigmoid function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression:\n",
    "\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "\n",
    "The $\\theta$ values are \"weights\". In our discussion of the perceptron, we described the the weights with the 'w' vector. For logistic regression, it is common to use the $\\theta$ vector.\n",
    "\n",
    "Logistic regression\n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "\n",
    "We can consider `z` the 'logits'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Neuron.drawio.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, we can define the cost function as the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $m$ is the number of training examples `len(x)`\n",
    "* $y^{(i)}$ is the actual label of training example 'i'.\n",
    "* $h(z^{(i)})$ is the model's prediction for the training example 'i'.\n",
    "\n",
    "The loss function for a single training example is\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
    "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label 'y' is also 1, the loss for that training example is 0. \n",
    "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n",
    "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss.\n",
    "\n",
    "Ergo,\n",
    "\n",
    "$$c(\\theta) = -\\log(\\hat{p}) \\text{ if } y = 1$$\n",
    "$$c(\\theta) = -\\log(1-\\hat{p}) \\text{ if } y = 0$$\n",
    "\n",
    "credit: deeplearning.ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## How do we train a logistic regression model? Update weights with gradient descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we need to update the weights $\\theta$ using gradient descent. We can do this by calculating the gradient of the cost function with respect to the weights, and then updating the weights by subtracting a fraction of the gradient. As we interate through the training examples, we will update the weights to minimize the cost function and therefore improve the model's predictions.\n",
    " \n",
    "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j \\tag{5}$$\n",
    "\n",
    "* `i` is the index across all `m` training examples.\n",
    "* `j` is the index of the weight $\\theta_j$, so $x^{(i)}_j$ is the feature associated with weight $\\theta_j$\n",
    "\n",
    "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n",
    "\n",
    "\n",
    "* The number of iterations `num_iters` is the number of times that you'll use the entire training set.\n",
    "* For each iteration, calculate the cost function using all training examples and for all features.\n",
    "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
    "\n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "* $\\mathbf{\\theta}$ has dimensions ($n+1$, 1), where `n` is the number of features, and there is one more element for the bias term $\\theta_0$.\n",
    "* The logits, `z`, are calculated by multiplying the feature matrix `x` with the weight vector `theta`.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (`m, n+1`) \n",
    "    * $\\mathbf{\\theta}$: has dimensions (`n+1, 1`)\n",
    "    * $\\mathbf{z}$: has dimensions (`m, 1`)\n",
    "* The prediction `h`, is calculated by applying the sigmoid to each element in `z`: $h(z) = sigmoid(z)$, and has dimensions (`m,1`).\n",
    "* The cost function $J$ is calculated by taking the dot product of the vectors `y` and `log(h)`.  Since both `y` and `h` are column vectors (`m,1`), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
    "\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "\n",
    "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
    "\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "\n",
    "            if self.verbose and i % 10000 == 0:\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print(f'Loss: {self.__loss(h, y)} \\t')\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# generate a toy dataset\n",
    "X, y = make_classification(n_samples=2000, n_features=4, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create a LogisticRegression instance and fit the model to the training data\n",
    "lr = LogisticRegression(lr=0.1, num_iter=100000, fit_intercept=True, verbose=True)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# predict the probabilities and labels for the test data\n",
    "y_pred_prob = lr.predict_prob(X_val)\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "# evaluate the model's performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision:\", precision_score(y_val, y_pred))\n",
    "print(\"Recall:\", recall_score(y_val, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at our input data\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at our labels\n",
    "y[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig into our LogisticRegression class\n",
    "\n",
    "```python\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr # alpha\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "```\n",
    "\n",
    "We initialize our class with lr, num_iter, fit_intercept, and verbose. We'll use these to set the learning rate, number of iterations, whether or not we want to fit an intercept, and whether or not we want to print out the loss during training.\n",
    "\n",
    "N.B.: The intercept term allows the logistic regression model to fit a decision boundary that doesn't necessarily pass through the origin. Without the intercept term, the decision boundary would always pass through the origin, which might not be appropriate for some datasets.\n",
    "\n",
    "```python\n",
    "\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z)) # returns h = sigmoid(z)\n",
    "\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "```\n",
    "\n",
    "We have three helper functions. The first one adds a column of ones to the data to account for the intercept. The second one calculates the sigmoid function. The third one calculates the loss.\n",
    "\n",
    "```python\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta) # dot product of X (training data) and theta\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size # calculate the gradient\n",
    "            self.theta -= self.lr * gradient # update the weights theta\n",
    "\n",
    "            if self.verbose and i % 10000 == 0:\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print(f'Loss: {self.__loss(h, y)} \\t')\n",
    "\n",
    "```\n",
    "\n",
    "Our fit method takes in X and y, and if we want to fit an intercept, we add a column of ones to X. We initialize our weights to zero. Then we iterate through the number of iterations that we want to train. We calculate the logits, then the sigmoid, then the gradient, and then update the weights. We print out the loss if we desire to do so. Our loss should decrease as we train our model.\n",
    "\n",
    "```python\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_prob(X) >= threshold\n",
    "```\n",
    "\n",
    "Our predict_prob method takes in X and returns the probability that the data point belongs to the positive class. Since we know from the sigmoid function that the output is between 0 and 1, we can use a threshold of 0.5 to determine whether or not the data point belongs to the positive class. If the probability is greater than or equal to 0.5, we predict that the data point belongs to the positive class. Otherwise, we predict that the data point belongs to the negative class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Sentiment Analysis\n",
    "\n",
    "Now, let's use our LogisticRegression class to perform sentiment analysis on movie reviews. We'll use the IMDB dataset, which contains 50,000 movie reviews from the Internet Movie Database. Half of the reviews are positive, and half are negative. The dataset is split into 25,000 reviews for training and 25,000 reviews for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "## html display\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "COLAB = False\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    # mount drive\n",
    "    drive.mount('/content/drive')\n",
    "    df = pd.read_csv('/content/drive/My Drive/DATA_340_3_NLP/other/IMDB_Dataset.csv')\n",
    "    COLAB = True\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "    df = pd.read_csv('../datasets/other/IMDB_Dataset.csv')\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's convert the labels to 0 and 1\n",
    "labels = {'positive': 1, 'negative': 0}\n",
    "\n",
    "df['sentiment'] = df['sentiment'].map(labels)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the distribution of the labels\n",
    "df.groupby('sentiment').count()['review'].plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's preprocess our data\n",
    "NLP = spacy.load('en_core_web_sm') # python -m spacy download en_core_web_sm\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"Preprocess by tokenizing text and remove stopwords\"\"\"\n",
    "    # stopwords \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # tokenize the text with spacy\n",
    "    doc = NLP(text)\n",
    "    \n",
    "    tokens = [token.lemma_.lower() for token in doc if token.is_alpha and token.lemma_ not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'This movie was horrible, and I hated it. I found the plot to be very boring and the acting was terrible.'\n",
    "test_res = preprocess(test)\n",
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's preprocess our data\n",
    "df_a = df[:500].copy()\n",
    "df_a['review'] = df_a['review'].apply(preprocess)\n",
    "df_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's see the distribution of token counts\n",
    "df_a['token_count'] = df_a['review'].apply(lambda x: len(x.split()))\n",
    "df_a['token_count'].plot(kind='hist', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the distribution of the labels\n",
    "df_a.groupby('sentiment').count()['token_count'].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's convert our text to vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# X = vectorizer.fit_transform(df_a['review']).toarray()\n",
    "X = tfidf_vectorizer.fit_transform(df_a['review']).toarray()\n",
    "\n",
    "y = df_a['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the shape of the data\n",
    "len(X_train), len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(lr=0.01, num_iter=100000, fit_intercept=True, verbose=True)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model's performance\n",
    "\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at a confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cfm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "## Let's plot the confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(cfm, annot=True, fmt='d', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at the classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's write a review and see if the model can predict the sentiment\n",
    "\n",
    "pos_review = \"\"\"\n",
    "\"Encino Man\" is a comedy film released in 1992 that tells the story of two high school students,\n",
    "Dave and Stoney, who discover a frozen caveman while digging a pool in Dave's backyard in Encino,\n",
    "California. They thaw him out and attempt to integrate him into modern society, with hilarious\n",
    "and often disastrous results. The movie features a young and charismatic cast, including Sean\n",
    "Astin as Dave, Pauly Shore as Stoney, and Brendan Fraser as the titular \"Encino Man\" or Link as\n",
    "he comes to be called. Brendan Fraser's performance as Link is a standout, as he perfectly\n",
    "captures the physicality and expressive language of a caveman in a modern setting, while also\n",
    "imbuing the character with a sense of innocence and charm that makes him endearing to the audience.\n",
    "The humor in \"Encino Man\" is mostly of the slapstick and gross-out variety, with a lot of\n",
    "physical comedy and exaggerated facial expressions from Pauly Shore. While some of the jokes\n",
    "may not land with everyone, there are still plenty of laughs to be had, especially for fans of\n",
    "early 90s comedy. The plot is relatively simple and predictable, with the main conflict revolving\n",
    "around Dave's desire to fit in with the popular crowd and impress his crush, Robyn. However, the\n",
    "movie's charm and humor more than make up for any shortcomings in the plot. Overall, \"Encino Man\"\n",
    "is a fun and light-hearted comedy that delivers plenty of laughs and a memorable performance from\n",
    "Brendan Fraser. While it may not be a cinematic masterpiece, it's definitely worth a watch for\n",
    "fans of 90s nostalgia and goofy humor.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "p_rev = preprocess(pos_review)\n",
    "p_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict the sentiment with our model\n",
    "res = tfidf_vectorizer.transform([p_rev]).toarray()\n",
    "'positive' if lr.predict(res) else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_review = \"\"\"\n",
    "\"Encino Man\" is a comedy film released in 1992 that tells the story of two high school students,\n",
    "Dave and Stoney, who discover a frozen caveman while digging a pool in Dave's backyard in Encino,\n",
    "California. While the premise may seem intriguing, the execution falls short and the film comes\n",
    "across as a poorly-made attempt at slapstick comedy. The acting in the movie is lackluster at best,\n",
    "with even talented actors like Sean Astin and Brendan Fraser delivering forgettable performances.\n",
    "Pauly Shore's exaggerated facial expressions and over-the-top acting may have been entertaining for\n",
    "some, but for others, it comes across as annoying and grating. The humor in \"Encino Man\" relies\n",
    "heavily on juvenile and crude jokes that are not particularly clever or original. The constant gags\n",
    "involving bodily functions and gross-out humor quickly become tiresome and predictable. The plot is\n",
    "thin and cliched, revolving around Dave's desire to fit in with the popular crowd and impress his\n",
    "crush, Robyn. The attempts to integrate the caveman into modern society are poorly executed, with\n",
    "the characters' reactions to Link's behavior often coming across as forced and unrealistic.\n",
    "Overall, \"Encino Man\" is a forgettable and poorly executed attempt at comedy that fails to deliver\n",
    "any genuine laughs or entertainment. The juvenile humor, predictable plot, and lackluster acting\n",
    "make for an overall disappointing viewing experience.\n",
    "\"\"\"\n",
    "\n",
    "n_rev = preprocess(neg_review)\n",
    "n_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tfidf_vectorizer.transform([n_rev]).toarray()\n",
    "'positive' if lr.predict(res) else 'negative'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
