{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Language Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lecture, we will discuss how to evaluate language models. We have already discussed precision, recall, accuracy, and F1 score. We will introduce the concepts of overfitting, underfitting, generalization, and optimal training scenarios. First, We will refresh our memory on precision, recall, accuracy, and F1 score, and then we will introduce the various training scenarios. To conclude, we will discuss some Cross Validation techniques. We will also introduce the python package _Weights and Biases_."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall, Accuracy, and F1 Score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision is the number of true positives divided by the number of true positives plus the number of false positives. It is a measure of the quality of the positive predictions. It is also known as positive predictive value.\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Recall is the number of true positives divided by the number of true positives plus the number of false negatives. It is a measure of the completeness of the positive predictions. It is also known as sensitivity or true positive rate.\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Accuracy is the number of true positives plus the number of true negatives divided by the total number of predictions. It is a measure of the overall quality of the predictions.\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{True Positives}+\\text{True Negatives}+\\text{False Positives}+\\text{False Negatives}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "F1 score is the harmonic mean of precision and recall. It is a measure of the overall quality of the positive predictions.\n",
    "\n",
    "$$\\text{F1 Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation, and Test Sets\n",
    "\n",
    "* `Train` set: used to train the model - the model learns from this data\n",
    "* `Validation` set: used to evaluate the model during training - the model does not learn from this data (but it can be used to tune hyperparameters)\n",
    "* `Test` set: used to evaluate the model after training - the model does not learn from this data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, Underfitting, and Generalization\n",
    "\n",
    "* Overfitting: the model is too complex for the data\n",
    "* Underfitting: the model is too simple for the data\n",
    "* Generalization: the model is able to make accurate predictions on new data\n",
    "\n",
    "____ Resources\n",
    "\n",
    "* [Underfittng vs. Overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries to generate data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate our data\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_redundant=15, n_informative=5, random_state=4)\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# Shape of the data\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tree depth values\n",
    "values = [i for i in range(1, 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the decision tree model\n",
    "for i in values:\n",
    "    model = DecisionTreeClassifier(max_depth=i, random_state=4)\n",
    "    \n",
    "    print(f\"[INFO] training model with max_depth = {i}\")\n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    t_y_hat = model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, t_y_hat)\n",
    "    train_scores.append(train_acc)\n",
    "    \n",
    "    # evaluate on the validation set\n",
    "    test_y_hat = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_y_hat)\n",
    "    test_scores.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values, train_scores, '-o',  label=\"train\")\n",
    "plt.plot(values, test_scores, '-o', label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasons for Overfitting\n",
    "\n",
    "* High bias and low variance\n",
    "* The size of the trainig set\n",
    "* the model is too complex\n",
    "\n",
    "\n",
    "#### Bias-Variance Tradeoff\n",
    "\n",
    "<img src=\"https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/images/bias_variance/bullseye.png\" height=\"600\" width=\"800\">\n",
    "\n",
    "Image source: https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*h4XETbo3hFx9PtKKJH90Dg.jpeg\" height=\"600\" width=\"800\">\n",
    "\n",
    "Image source: https://medium.com/@rsehrawat75/bias-variance-tradeoff-f0e3afb78879"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Techniques to prevent overfitting\n",
    "\n",
    "* Decrease model complexity\n",
    "* Decrease the number of features\n",
    "* Increase the size of the training set\n",
    "* Decrease the training time\n",
    "* Regularization\n",
    "* Early stopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate some random data\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.randn(100) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=1)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot data and model predictions\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(X, model.predict(X_poly), color='red', linewidth=2)\n",
    "plt.title('Underfitting Linear Regression Model')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasons for Underfitting\n",
    "\n",
    "* Model is too simple\n",
    "* Feed better features to the model\n",
    "* High bias and low variance\n",
    "* The size of the training set is too small\n",
    "* Noise in the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Techniques to prevent underfitting\n",
    "\n",
    "* Increase model complexity\n",
    "* Increase the number of features (feature engineering)\n",
    "* Decrease noise in the data\n",
    "* Increase the number of epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "\n",
    "We want our mdl to be able to make accurate predictions on new data, and thus we want our model to generalize. To fix the underfitting of the previous model, so it generalize well we can increase the complexity of the model by using a higher degree polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=5)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot data and model predictions\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(X, model.predict(X_poly), color='red', linewidth=2)\n",
    "plt.title('Improved Linear Regression Model')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Training Scenarios\n",
    "\n",
    "<img src=\"https://meditationsonbianddatascience.files.wordpress.com/2017/05/fitgraph.jpg?w=1024\" height=\"600\" width=\"800\">\n",
    "\n",
    "Image source: https://meditationsonbianddatascience.com/2017/05/11/overfitting-underfitting-how-well-does-your-model-fit\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    TimeSeriesSplit,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    GroupKFold,\n",
    "    StratifiedGroupKFold,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Source https://www.kaggle.com/code/robikscube/cross-validation-visualized-youtube-tutorial/notebook and Scikit-Learn documentation\n",
    "\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "\n",
    "def visualize_groups(classes, groups, name):\n",
    "    # Visualize dataset groups\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(\n",
    "        range(len(groups)),\n",
    "        [0.5] * len(groups),\n",
    "        c=groups,\n",
    "        marker=\"_\",\n",
    "        lw=50,\n",
    "        cmap=cmap_data,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        range(len(groups)),\n",
    "        [3.5] * len(groups),\n",
    "        c=classes,\n",
    "        marker=\"_\",\n",
    "        lw=50,\n",
    "        cmap=cmap_data,\n",
    "    )\n",
    "    ax.set(\n",
    "        ylim=[-1, 5],\n",
    "        yticks=[0.5, 3.5],\n",
    "        yticklabels=[\"Data\\ngroup\", \"Data\\nclass\"],\n",
    "        xlabel=\"Sample index\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=25):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(\n",
    "            range(len(indices)),\n",
    "            [ii + 0.5] * len(indices),\n",
    "            c=indices,\n",
    "            marker=\"_\",\n",
    "            lw=lw,\n",
    "            cmap=cmap_cv,\n",
    "            vmin=-0.2,\n",
    "            vmax=1.2,\n",
    "        )\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\n",
    "    )\n",
    "\n",
    "    ax.scatter(\n",
    "        range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data\n",
    "    )\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\n",
    "    ax.set(\n",
    "        yticks=np.arange(n_splits + 2) + 0.5,\n",
    "        yticklabels=yticklabels,\n",
    "        xlabel=\"Sample index\",\n",
    "        ylabel=\"CV iteration\",\n",
    "        ylim=[n_splits + 2.2, -0.2],\n",
    "        xlim=[0, 100],\n",
    "    )\n",
    "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_cv(cv, X, y, groups, n_splits=5):\n",
    "    this_cv = cv(n_splits=n_splits)\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n",
    "\n",
    "    ax.legend(\n",
    "        [Patch(color=cmap_cv(0.8)), Patch(color=cmap_cv(0.02))],\n",
    "        [\"Testing set\", \"Training set\"],\n",
    "        loc=(1.02, 0.8),\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(right=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "def get_fake_X_y():\n",
    "    # Fake Generate the class/group data for an example\n",
    "    n_points = 100\n",
    "    X_ = np.random.randn(100, 10)\n",
    "\n",
    "    percentiles_classes = [0.1, 0.9]\n",
    "    y_ = np.hstack([[ii] * int(100 * perc) for ii, perc in enumerate(percentiles_classes)])\n",
    "\n",
    "    # Evenly spaced groups repeated once\n",
    "    groups_ = np.hstack([[ii] * 10 for ii in range(10)])\n",
    "    return X_, y_, groups_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some sample data\n",
    "X = np.random.randn(100, 10)\n",
    "y = np.random.randint(0, 3, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of folds for cross-validation\n",
    "k = 5\n",
    "\n",
    "# create an instance of StratifiedKFold\n",
    "skf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# create an empty list to store the cross-validation scores\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each fold\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # split the data into training and testing sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # fit a multinomial logistic regression model on the training data\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # compute the accuracy score of the model on the testing data\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # add the score to the list of cross-validation scores\n",
    "    scores.append(score)\n",
    "\n",
    "    # Print the k-fold index\n",
    "    print(f\"Fold: {len(scores)}/{k}: Accuracy: {score:.3f}\")\n",
    "    \n",
    "# compute the mean and standard deviation of the cross-validation scores\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# print the mean and standard deviation of the cross-validation scores\n",
    "print(f\"Mean accuracy: {mean_score:.3f}, Standard deviation: {std_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold()\n",
    "X_, y_, groups_ = get_fake_X_y()\n",
    "plot_cv(KFold, X_, y_, groups_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified K-Fold Cross Validation\n",
    "\n",
    "Stratified K-Fold Cross Validation is a variation of K-Fold Cross Validation. It is used when the data is imbalanced. It is used to split the data into K folds, and then it is used to train the model K times. Each time, a different fold is used as the validation set, and the remaining folds are used as the training set. The final score is the average of the K scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of folds for cross-validation\n",
    "k = 5\n",
    "\n",
    "# create an instance of StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# create an empty list to store the cross-validation scores\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each fold\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # split the data into training and testing sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # fit a multinomial logistic regression model on the training data\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # compute the accuracy score of the model on the testing data\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # add the score to the list of cross-validation scores\n",
    "    scores.append(score)\n",
    "\n",
    "    # Print the k-fold index\n",
    "    print(f\"Fold: {len(scores)}/{k}: Accuracy: {score:.3f}\")\n",
    "    \n",
    "# compute the mean and standard deviation of the cross-validation scores\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# print the mean and standard deviation of the cross-validation scores\n",
    "print(f\"Mean accuracy: {mean_score:.3f}, Standard deviation: {std_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold()\n",
    "X_, y_, groups_ = get_fake_X_y()\n",
    "plot_cv(StratifiedKFold, X_, y_, groups_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group K-Fold Cross Validation\n",
    "\n",
    "Group K-Fold Cross Validation is a variation of K-Fold Cross Validation. It is used when the data is grouped. It is used to split the data into K folds, and then it is used to train the model K times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of folds for cross-validation\n",
    "k = 5\n",
    "\n",
    "# create an instance of StratifiedKFold\n",
    "skf = GroupKFold(n_splits=k)\n",
    "\n",
    "# create an empty list to store the cross-validation scores\n",
    "scores = []\n",
    "\n",
    "# groups\n",
    "groups = np.hstack([[ii] * 10 for ii in range(10)])\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each fold\n",
    "for train_index, test_index in skf.split(X, y, groups=groups):\n",
    "    # split the data into training and testing sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # fit a multinomial logistic regression model on the training data\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # compute the accuracy score of the model on the testing data\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # add the score to the list of cross-validation scores\n",
    "    scores.append(score)\n",
    "\n",
    "    # Print the k-fold index\n",
    "    print(f\"Fold: {len(scores)}/{k}: Accuracy: {score:.3f}\")\n",
    "    \n",
    "# compute the mean and standard deviation of the cross-validation scores\n",
    "mean_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "\n",
    "# print the mean and standard deviation of the cross-validation scores\n",
    "print(f\"Mean accuracy: {mean_score:.3f}, Standard deviation: {std_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gskf = StratifiedGroupKFold()\n",
    "X_, y_, groups_ = get_fake_X_y()\n",
    "np.random.shuffle(y_)\n",
    "plot_cv(StratifiedGroupKFold, X_, y_, groups_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "\n",
    "_Weights and Biases_ is a free to use python package and website. It's a great resource for tracking your machine learning and neural network experiments. It is used to track the hyperparameters, metrics, and artifacts of your experiments. It is also used to compare the results of different experiments. It is also used to visualize the results of your experiments. It is also used to share the results of your experiments.\n",
    "\n",
    "https://wandb.ai/home\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of Weights and Biases with a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use tensorflow and work with the MNIST dataset\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## examine the data\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at some of the images\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a subplot 5x5 with random images\n",
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "for ax in axes:\n",
    "    img = random.choice(x_train)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model our data for a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Print the model summary\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 20\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log results in wandb\n",
    "WANDB_NOTEBOOK_NAME = \"mnist-classification\"\n",
    "\n",
    "wandb.init(project=\"mnist-classification\", config=hyperparams)\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparams[\"learning_rate\"])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    wandb.keras.WandbCallback(save_model=False)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=hyperparams[\"batch_size\"],\n",
    "    epochs=hyperparams[\"epochs\"],\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "wandb.log({\"val_loss\": history.history[\"val_loss\"][-1], \"val_accuracy\": history.history[\"val_accuracy\"][-1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at a confusion matrix of our predictions\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cfm = confusion_matrix(y_test.argmax(axis=1), model.predict(x_test).argmax(axis=1))\n",
    "\n",
    "## Let's plot the confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cfm, annot=True, fmt=\"d\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring MNIST in Embeddings Projector\n",
    "\n",
    "[Embeddings Projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
