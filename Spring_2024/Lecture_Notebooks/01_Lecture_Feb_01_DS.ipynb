{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized computation and data structures\n",
    "\n",
    "In the context of Natural Language Processing (NLP) and data science, efficient computation and data handling are crucial for handling large datasets and complex algorithms. Vectorized computation and data structures like NumPy and pandas play a pivotal role in this efficiency. Let's delve into the concepts, their significance, and how they relate to broader statistical concepts in data science, mathematics, and social sciences.\n",
    "\n",
    "### Vectorized Computation\n",
    "\n",
    "**Context and Importance:** Vectorized computation refers to performing operations on entire arrays or matrices in a single step, rather than using loops over individual elements. This approach leverages optimized. In NLP, where operations on large text corpora and word embeddings are common, vectorized computation can drastically reduce execution time.\n",
    "\n",
    "**NumPy:** NumPy (Numerical Python) is a foundational package for scientific computing in Python. It provides an N-dimensional array object which is a key data structure for vectorized operations. NumPy arrays support a wide range of mathematical operations that can be performed over the whole array efficiently and succinctly. This is particularly beneficial in NLP for tasks like vector operations in semantic analysis, where each word or document can be represented as a high-dimensional vector.\n",
    "\n",
    "**Tensors:** generalizes the concept of vectors and matrices to higher dimensions. Tensors are the basic building blocks in these frameworks, used to represent data or parameters (like weights and biases in neural networks). While a Numpy array is conceptually similar to a tensor, tensors in deep learning frameworks are designed with additional capabilities, such as the ability to run on GPUs for faster computation and to automatically compute gradients (a feature essential for training neural networks). \n",
    "\n",
    "### Data Structures: pandas\n",
    "\n",
    "**pandas Overview:** pandas is a Python library providing high-level data structures and a vast array of tools for data analysis. At its core, the DataFrame is the most noteworthy data structure in pandas, essentially a tabular data structure with labeled axes (rows and columns). For NLP, pandas DataFrames facilitate the manipulation of textual data, enabling tasks like data cleaning, transformation, and aggregation to be performed effortlessly.\n",
    "\n",
    "**Significance in NLP:** In NLP projects, one often starts with structured data from various sources like CSV files, databases, or JSON. pandas makes it easy to load, preprocess, and explore this textual data. For example, you can easily apply transformations to text data (like tokenization, stemming, or lemmatization) across large datasets and analyze the distribution of words or phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Super) Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example with NumPy: Calculate the word length of each word in an array\n",
    "words = np.array([\"natural\", \"language\", \"processing\"])\n",
    "word_lengths = np.vectorize(len)(words)\n",
    "print(word_lengths)\n",
    "\n",
    "# Example with pandas: Create a DataFrame and process textual data\n",
    "data = {'text': [\"Natural Language Processing\", \"Data Science\", \"Machine Learning\"]}\n",
    "df = pd.DataFrame(data)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# If you are running this notebook in Google colab, uncomment this line of code and run\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/', force_remount=True)\n",
    "# file = Path('gdrive/MyDrive/DATA_340_3_NLP/Datasets/eczema.csv')\n",
    "# eczema_df = pd.read_csv(file, nrows=10000)\n",
    "\n",
    "# load 10,000 rows of the dataset\n",
    "eczema_df = pd.read_csv('../datasets/eczema.csv', nrows=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick view of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eczema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eczema_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eczema_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eczema_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Munging in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people talk about lotions?\n",
    "eczema_df['lotion'] = eczema_df['text'].str.contains(\"lotion\")\n",
    "eczema_df[eczema_df['lotion'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eczema_df['lotion'] = eczema_df['text'].str.contains(\"lotion\", case=False)\n",
    "eczema_df[eczema_df['lotion'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eczema_df['lotion'] = eczema_df['text'].str.contains(\"lot[io]+n\", regex=True, case=False)\n",
    "eczema_df[eczema_df['lotion'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eczema_df['age'] = eczema_df['text'].str.contains(\"\\d+\", regex=True, case=False)\n",
    "eczema_df[eczema_df['age'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick tutorial on Regular Expressions\n",
    "\n",
    "In the context of Natural Language Processing (NLP), understanding and leveraging Regular Expressions (regex) is a fundamental skill. Regex is a powerful tool used for searching, manipulating, and analyzing text data by defining specific patterns. It's particularly useful in text preprocessing, data cleaning, and information extraction tasks.\n",
    "\n",
    "**Regular Expressions** are sequences of characters that form a search pattern. They can be used to check if a string contains the specified search pattern, to replace the search pattern with a specified text, or to split a string on the specified search pattern.\n",
    "\n",
    "- **Literals**: These are the simplest form of regex, where the search pattern matches the exact character sequence. For example, the regex `data` will match \"data\" in the string \"data science\".\n",
    "\n",
    "- **Metacharacters**: These are characters with a special meaning. Some of the most common metacharacters include:\n",
    "  - `.` (dot): Matches any single character except newline. For example, `a.b` matches \"acb\" or \"a2b\".\n",
    "  - `^` (caret): Matches the start of a string. For example, `^data` matches \"data\" in \"data science\" but not in \"my data science project\".\n",
    "  - `$` (dollar): Matches the end of a string. For example, `end$` matches \"the end\" in \"This is the end\".\n",
    "  - `*` (asterisk): Matches 0 or more occurrences of the preceding element. For example, `a*b` matches \"b\", \"ab\", \"aab\", etc.\n",
    "  - `+` (plus): Matches 1 or more occurrences of the preceding element. For example, `a+b` matches \"ab\", \"aab\", but not \"b\".\n",
    "  - `?` (question mark): Matches 0 or 1 occurrence of the preceding element. For example, `a?b` matches \"b\" or \"ab\".\n",
    "\n",
    "- **Brackets**: Used for specifying a set of characters to match.\n",
    "  - `[abc]`: Matches any one of the characters a, b, or c.\n",
    "  - `[^abc]`: Matches any character except a, b, or c.\n",
    "  \n",
    "- **Quantifiers**: Specify how many instances of a character, group, or character class must be present in the input for a match to be found.\n",
    "  - `{n}`: Exactly n occurrences. For example, `a{3}` matches exactly three 'a' characters.\n",
    "  - `{n,}`: At least n occurrences. For example, `a{2,}` matches two or more 'a' characters.\n",
    "  - `{n,m}`: Between n and m occurrences, inclusive. For example, `a{2,4}` matches between two and four 'a' characters.\n",
    "\n",
    "- **Parentheses** are used for grouping characters or patterns. For example, `(abc)+` matches one or more repetitions of \"abc\".\n",
    "\n",
    "Think of regex like a secret code that helps you find specific patterns in a giant wall of text. It's like playing \"Where's Waldo?\" but for words or letters. You have special symbols that tell your search tool exactly what to look for, whether it's anything that looks like \"Waldo\", or finding Waldo only if he's at the beginning or end of the page.\n",
    "\n",
    "Regular Expressions are a versatile and powerful tool in text processing and NLP. By mastering regex, you can efficiently search, replace, and manipulate text data, which is crucial for preparing and analyzing textual data in Data Science. Start small, practice with real text data, and gradually, you'll find regex to be an indispensable part of your NLP toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Finding if a pattern exists in a string\n",
    "if re.search(r'^Data', 'Data Science'):\n",
    "    print(\"Pattern found!\")\n",
    "\n",
    "# Replacing a pattern in a string\n",
    "replaced_text = re.sub(r'Science', 'Analysis', 'Data Science')\n",
    "print(replaced_text)  # Outputs: Data Analysis\n",
    "\n",
    "# Splitting a string based on a pattern\n",
    "split_text = re.split(r'\\s+', 'Data Science is cool')\n",
    "print(split_text)  # Outputs: ['Data', 'Science', 'is', 'cool']\n",
    "\n",
    "# Extracting all matches of a pattern\n",
    "matches = re.findall(r'\\bData\\b', 'Data Science and Data Analysis')\n",
    "print(matches)  # Outputs: ['Data', 'Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's dig into some more pandas features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use additional libraries with our pandas dataframe to perform more complex operations. For example, let's explore how different NLP libraries can be used with pandas to perform text preprocessing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "In NLP, tokenization serves as a preprocessing step where text is segmented into tokens that are useful for subsequent tasks like parsing, syntax analysis, or feature extraction. The process can vary in complexity depending on the language and the specific application. For instance, tokenizing a sentence in English might simply involve splitting it by spaces and punctuation, but languages without clear word boundaries (e.g., Chinese) require more sophisticated methods. The choice of what constitutes a token is crucial and depends on the task at hand; for example, in some contexts, individual words are tokens, while in others, meaningful phrases or even individual characters might be considered tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's tokenize our text column\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eczema_df['nltk_tokens'] = eczema_df['text'].apply(word_tokenize)\n",
    "eczema_df['nltk_tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the token length distribution using pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eczema_df['nltk_tokens'].apply(len).hist(bins=30)\n",
    "plt.title('Token Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the longest token\n",
    "max_token = eczema_df['nltk_tokens'].apply(len).idxmax()\n",
    "print(eczema_df.loc[max_token, 'nltk_tokens'], eczema_df.loc[max_token, 'text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy --quiet\n",
    "!python -m spacy download en_core_web_sm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize with spacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eczema_df['spacy_tokens'] = eczema_df['text'].apply(lambda x: [token.text for token in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the token length distribution using pandas\n",
    "eczema_df['spacy_tokens'].apply(len).hist(bins=30)\n",
    "plt.title('Token Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of differences between spacy and nltk tokenization\n",
    "eczema_df[eczema_df['nltk_tokens'].apply(len) != eczema_df['spacy_tokens'].apply(len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our own tokenizer\n",
    "\n",
    "We can also create our own tokenizer using regular expressions. This can be useful when we have specific requirements for tokenization that are not met by existing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eczema_df['our_tokens'] = eczema_df['text'].apply(lambda x: re.split(r'([,.?_!\"()\\']|--|\\s)', x))\n",
    "eczema_df['our_tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty tokens\n",
    "eczema_df['our_tokens'] = eczema_df['our_tokens'].apply(lambda x: [token for token in x if token.strip() != ''])\n",
    "eczema_df['our_tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD-IDF\n",
    "\n",
    "TF-IDF, which stands for Term Frequency-Inverse Document Frequency, is a statistical measure used to evaluate the importance of a word in a document, which is part of a corpus (a collection of documents). This technique is commonly used in information retrieval and text mining.\n",
    "\n",
    "### Background and Assumptions:\n",
    "- **Term Frequency (TF):** This measures how frequently a term occurs in a document. In the TF calculation, all terms are considered equally important. However, in reality, certain terms, like \"is\" or \"the,\" may appear many times but have little importance. Thus, the need for weighting the frequency of each term.\n",
    "- **Inverse Document Frequency (IDF):** This measures how important a term is within the corpus. The idea is that the significance of a term increases in proportion to the number of times a term appears in the document but is offset by the frequency of the term in the corpus. Terms that are common across multiple documents (like common English words) will have a lower IDF.\n",
    "\n",
    "##### Formula and Computation:\n",
    "1. **TF(t)** = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "2. **IDF(t)** = log_e(Total number of documents / Number of documents with term t in it)\n",
    "3. **TF-IDF(t)** = TF(t) * IDF(t)\n",
    "\n",
    "##### Applications:\n",
    "- **Information Retrieval:** Helps in scoring and ranking a document's relevance given a user query.\n",
    "- **Text Mining:** Useful for dimensionality reduction, feature selection, and identifying key terms in documents.\n",
    "- **Data Science Context:** It's a way to convert textual data into a vector space model, where each dimension corresponds to a specific term. When dealing with NLP in data science, TF-IDF can be instrumental in pre-processing data for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work with a subset\n",
    "random_seed = 42\n",
    "sample = eczema_df.sample(n=100, random_state=random_seed)\n",
    "sample = sample[['docID', 'text']]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get an understanding of the length of our conversations\n",
    "sample['word_total'] = sample['text'].apply(lambda x: len(x.split(' ')))\n",
    "sample['word_total'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our sample distribution conversation length\n",
    "(sample\n",
    " .drop(columns=['docID', 'text'])\n",
    " .plot(kind='hist', bins=50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.query('word_total > 300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f\"The 95th percentile of the conversation length is: {np.percentile(sample['word_total'].tolist(), 95)} words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the above two datasets using TF-IDF by doing the following.\n",
    "\n",
    "* Tokenize the conversation\n",
    "* We can try lemmatizing the text\n",
    "* Count the freqeuncy of words in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the tweet using SpaCy\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "NLP = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a list of the lemmas to count the frequency of words but remove the punctuation\n",
    "sample['tokens'] = sample['text'].progress_apply(lambda x: [x.lemma_.lower() for x in NLP(x) if x.lemma_.lower() not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unwind the data on the tokens\n",
    "sample_tokens = (sample\n",
    "                  .explode('tokens')\n",
    "                  .drop(columns=['text', 'word_total'])\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Term Frequency\n",
    "\n",
    "- **Term Frequency (TF):** This measures how frequently a term occurs in a document. In the TF calculation, all terms are considered equally important. However, in reality, certain terms, like \"is\" or \"the,\" may appear many times but have little importance. Thus, the need for weighting the frequency of each term.\n",
    "\n",
    "1. **TF(t)** = (Number of times term t appears in a document) / (Total number of terms in the document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word frequency dataframe\n",
    "term_frequency = (sample_tokens\n",
    "                  .groupby(by=['docID', 'tokens'])\n",
    "                  .agg({'tokens': 'count'})\n",
    "                  .rename(columns={'tokens': 'term_frequency'})\n",
    "                  .reset_index()\n",
    "                  .rename(columns={'tokens': 'term'})\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "         'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "         'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "         'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "         'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "         'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "term_frequency = term_frequency.drop(term_frequency[term_frequency['term'].isin(stop_words)].index)\n",
    "term_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Frequency\n",
    "document_frequency = (term_frequency\n",
    "                      .groupby(['docID', 'term'])\n",
    "                      .size()\n",
    "                      .unstack()\n",
    "                      .sum()\n",
    "                      .reset_index()\n",
    "                      .rename(columns={0: 'document_frequency'})\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the document freqs into the term dataframe\n",
    "term_frequency = term_frequency.merge(document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_in_corpus = term_frequency['docID'].nunique()\n",
    "documents_in_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inverse Document Frequency\n",
    "\n",
    "- **Inverse Document Frequency (IDF):** This measures how important a term is within the corpus. The idea is that the significance of a term increases in proportion to the number of times a term appears in the document but is offset by the frequency of the term in the corpus. Terms that are _common across multiple documents (like common English words) will have a lower IDF_.\n",
    "\n",
    "2. **IDF(t)** = log_e(Total number of documents / Number of documents with term t in it)\n",
    "3. **TF-IDF(t)** = TF(t) * IDF(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse document frequency\n",
    "term_frequency['idf'] = np.log((1 + documents_in_corpus) / (1 + term_frequency['document_frequency'])) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency['tfidf'] = term_frequency['term_frequency'] * term_frequency['idf']\n",
    "term_frequency.sort_values(by=['term_frequency'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "term_frequency['tfidf_norm'] = preprocessing.normalize(term_frequency[['tfidf']], axis=0, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_terms = term_frequency.sort_values(by=['docID', 'tfidf'], ascending=[True, False]).groupby(['docID']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_terms.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docIds = top_n_terms['docID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[sample['docID'] == 41]['text'].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
