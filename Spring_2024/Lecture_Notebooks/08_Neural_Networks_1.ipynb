{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basic architecture and functionality of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/Neuron.drawio.png\" width=\"800\" height=\"500\" /></center>\n",
    "\n",
    "<center><img src=\"images/nn_layers.png\" width=\"800\" height=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* $X$ - is our input features in the form of a vector\n",
    "* $w$ - is the weights we initialize randomly and will be updated during training\n",
    "* $b$ - is the bias we initialize randomly and will be updated during training\n",
    "* `Dense Layer` - one hidden layer in the network with $n$ neurons (fully connected network)\n",
    "* `Forward pass` - process of calculating the input * weights + bias for each neuron in the hidden layer and then applying an activation function to the result\n",
    "* `Activation function` - a function that is applied to the result of the forward pass to introduce non-linearity into the network\n",
    "* `Output` - the final result of the network  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation\n",
    "\n",
    "Code adapted from: nnfs.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adeline Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, n_inputs, epochs=10, learning_rate=0.01):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros(n_inputs + 1)\n",
    "        self.errors = []\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        if summation > 0:\n",
    "            activation = 1\n",
    "        else:\n",
    "            activation = 0\n",
    "        return activation\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.epochs):\n",
    "            error = 0\n",
    "            for inputs, target in zip(X, y):\n",
    "                prediction = self.predict(inputs)\n",
    "                update = self.learning_rate * (target - prediction)\n",
    "                self.weights[1:] += update * inputs\n",
    "                self.weights[0] += update\n",
    "                error += int(update != 0.0)\n",
    "            self.errors.append(error)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from nnfs.datasets import vertical_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # save a copy since we modify the original one\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            \n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values\n",
    "        # Only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # Use first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Dense(2, 3)\n",
    "\n",
    "# create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Dense(3, 3)\n",
    "\n",
    "# create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(loss_activation.output[:5])\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two layers in this neural network: the first is a dense (fully connected) layer with 2 input features and 3 output values, and the second is also a dense layer but with 3 input features and 3 output values, essentially taking the output from the first layer as its input. Between these two dense layers, a Rectified Linear Unit (ReLU) activation function is applied to introduce non-linearity and help the model learn from the data. After the data passes through the second dense layer, a Softmax function coupled with a Categorical Cross-Entropy loss function is applied to calculate the probability distribution of the classes and the loss value, respectively. A forward pass is performed through the network to compute the outputs and the loss value, which is then printed out at the end along with the output of the first few samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hackers method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = vertical_data(samples=100, classes=3)\n",
    "\n",
    "# create model\n",
    "dense1 = Dense(2, 3) # first dense layer, 2 inputs (each sample has 2 features), 3 outputs\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Dense(3, 3) # second dense layer, 3 inputs, 3 outputs\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# create loss function\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# define the variables\n",
    "lowest_loss = 9999999 # some initial value\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "\n",
    "for iteration in range(10000):\n",
    "    dense1.weights += 0.05 * np.random.randn(2, 3)\n",
    "    dense1.biases += 0.05 * np.random.randn(1, 3)\n",
    "    dense2.weights += 0.05 * np.random.randn(3, 3)\n",
    "    dense2.biases += 0.05 * np.random.randn(1, 3)\n",
    "    \n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # Takes the output of second dense layer here and returns loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    # If loss is smaller - print and save weights and biases aside\n",
    "    if loss < lowest_loss:\n",
    "        print('New set of weights found, iteration:', iteration,\n",
    "              'loss:', loss, 'acc:', accuracy)\n",
    "        best_dense1_weights = dense1.weights.copy()\n",
    "        best_dense1_biases = dense1.biases.copy()\n",
    "        best_dense2_weights = dense2.weights.copy()\n",
    "        best_dense2_biases = dense2.biases.copy()\n",
    "        lowest_loss = loss\n",
    "    # Revert weights and biases\n",
    "    else:\n",
    "        dense1.weights = best_dense1_weights.copy()\n",
    "        dense1.biases = best_dense1_biases.copy()\n",
    "        dense2.weights = best_dense2_weights.copy()\n",
    "        dense2.biases = best_dense2_biases.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of Loss\n",
    "\n",
    "To learn how to adjust weights and biases, we need to know something about their impact on loss. We can do this by calculating the derivative of loss with respect to the weights and biases. This is done using the chain rule of calculus. The chain rule is used to calculate the derivative of a function inside of a function. In this case, the derivative of the loss function with respect to the weights and biases inside the dense layer function.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z} \\frac{\\partial z}{\\partial w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return 2*x\n",
    "\n",
    "x = np.array(range(5))\n",
    "y = f(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we wanted to calculate the slope, we would need to calculate the tangent\n",
    "at a given point. We can do this by using the slope formula:\n",
    "$$slope = (y2 - y1) / (x2 - x1) = rise / run$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the impact that x has on y, so we can rewrite the slope formula as:\n",
    "\n",
    "$$slope = \\frac{\\Delta y}{\\Delta x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return 2*x**2\n",
    "\n",
    "X = np.array(np.arange(0, 5, 0.001))\n",
    "y = f(X)\n",
    "\n",
    "plt.plot(X, y)\n",
    "\n",
    "colors = ['r', 'g', 'b', 'y', 'm']\n",
    "\n",
    "def approximate_tan_line(x, approximate_derivative):\n",
    "    return (approximate_derivative*x) + b\n",
    "\n",
    "for i in range(5):\n",
    "    p2_delta = 0.0001\n",
    "    x1 = i\n",
    "    x2 = x1 + p2_delta\n",
    "    \n",
    "    y1 = f(x1)\n",
    "    y2 = f(x2)\n",
    "    \n",
    "    print((x1, y1), (x2, y2))\n",
    "    approximate_derivative = (y2 - y1) / (x2 - x1)\n",
    "    b = y2 - (approximate_derivative*x2)\n",
    "    \n",
    "    to_plot = [x1-0.9, x1, x1+0.9]\n",
    "    \n",
    "    plt.scatter(x1, y1, c=colors[i])\n",
    "    plt.plot([point for point in to_plot],\n",
    "             [approximate_tan_line(point, approximate_derivative) for point in to_plot],\n",
    "             c=colors[i])\n",
    "    \n",
    "    print('Approximate derivative for f(x)', f'where x = {x1} is {approximate_derivative}')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Import Statements**:\n",
    "   - `import matplotlib.pyplot as plt` and `import numpy as np` are standard import statements used to bring in the Matplotlib and NumPy libraries, which are often used for data visualization and numerical operations, respectively.\n",
    "\n",
    "2. **Function Definition**:\n",
    "   - `def f(x): return 2*x**2` defines a simple quadratic function \\( f(x) = 2x^2 \\).\n",
    "\n",
    "3. **Creating Data Points**:\n",
    "   - `X = np.array(np.arange(0, 5, 0.001))` creates an array of x-values ranging from 0 to 5, with a step of 0.001.\n",
    "   - `y = f(X)` computes the corresponding y-values by applying the function \\( f \\) to each x-value.\n",
    "\n",
    "4. **Initial Plot**:\n",
    "   - `plt.plot(X, y)` plots the quadratic function using the generated x and y values.\n",
    "\n",
    "5. **Colors Array**:\n",
    "   - `colors = ['r', 'g', 'b', 'y', 'm']` sets up an array of color codes to be used for different tangent lines.\n",
    "\n",
    "6. **Tangent Line Approximation and Plotting**:\n",
    "   - A for loop `for i in range(5):` iterates through the integers 0 through 4.\n",
    "   - Within the loop:\n",
    "       - Two points, $(x_1, y_1)$ and $(x_2, y_2)$, very close to each other, are chosen along the curve.\n",
    "       - The slope (approximate derivative) of the tangent line at $x_1$ is calculated using the difference quotient formula: $\\frac{{y_2 - y_1}}{{x_2 - x_1}}$.\n",
    "       - The y-intercept $b$ of the tangent line is found using the point-slope form of a linear equation.\n",
    "       - Three x-values around $x_1$ are chosen, and the tangent line is plotted over these points using the `plt.plot()` and `plt.scatter()` functions.\n",
    "       - The approximate derivative is printed to the console.\n",
    "\n",
    "7. **Displaying the Plot**:\n",
    "   - `plt.show()` displays the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation into neural network architecture: partial derivatives and chain rule\n",
    "\n",
    "In the forward pass, we need to calculate the derivative of the loss with respect to the weights and biases. This is done using the chain rule of calculus. The chain rule is used to calculate the derivative of a function inside of a function.\n",
    "\n",
    "* The partial derivative measures how much impact a given parameter has on the output of a function\n",
    "* the gradient is vector of the size of inputs with each element of the vector representing the partial derivative of the loss with respect to the input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Backpropagation is a central algorithm in the training of feedforward artificial neural networks and has foundational importance in the field of machine learning and artificial intelligence. It is based on the calculus principle of chain rule for derivatives, and is used to minimize the error in the neural network's predictions by adjusting the weights of the connections between neurons. Below is a detailed explanation and its relation to broader concepts in data science and mathematics.\n",
    "\n",
    "### Explanation:\n",
    "Backpropagation, short for \"backward propagation of errors,\" is a method used during the training of neural networks. Here are the steps broken down:\n",
    "\n",
    "1. **Forward Pass**: \n",
    "    - Input data is fed forward through the network.\n",
    "    - Each layer computes an output based on the input and its weights.\n",
    "    - The final output is compared to the target value to compute the error using a loss function (e.g., mean squared error).\n",
    "\n",
    "2. **Backward Pass**:\n",
    "    - The error is then propagated backward through the network.\n",
    "    - The gradient of the loss function with respect to each weight is computed using the chain rule of calculus.\n",
    "    - This involves computing the derivative of the loss function with respect to the network's output, multiplied by the derivative of the network's output with respect to each weight.\n",
    "\n",
    "3. **Weight Update**:\n",
    "    - The weights are then updated in a way to minimize the error, typically using a gradient descent algorithm or one of its variants.\n",
    "    - The amount by which each weight is adjusted is proportional to the negative of the gradient, scaled by a learning rate.\n",
    "\n",
    "### Summary:\n",
    "Imagine you're teaching a robot to catch a ball. At first, it misses a lot. But each time it misses, you tell it what it did wrong, and it makes a slight adjustment. Over time, it gets better at catching the ball. \n",
    "\n",
    "Backpropagation in neural networks is similar. The network makes a guess about the input data, checks how wrong or right the guess is, then goes back and adjusts the weights a bit to improve for next time. It does this many times, learning from the errors, and getting better with each iteration.\n",
    "\n",
    "### Python Code Example:\n",
    "Here's a simplified version of how backpropagation might be implemented in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume we have some input data X, and target labels y\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Randomly initialize weights\n",
    "weights = np.random.rand(2, 1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "adjustments_history = []\n",
    "\n",
    "for epoch in range(10000):  # Train for 10000 epochs\n",
    "    # Forward pass\n",
    "    input_layer = X\n",
    "    outputs = sigmoid(np.dot(input_layer, weights))\n",
    "    \n",
    "    # Compute error\n",
    "    error = y - outputs\n",
    "    \n",
    "    # Backward pass\n",
    "    adjustments = error * sigmoid_derivative(outputs)\n",
    "    adjustments_history.append(adjustments)\n",
    "    \n",
    "    # Update weights\n",
    "    weights += np.dot(input_layer.T, adjustments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjustments_history = np.array(adjustments_history)\n",
    "adjustments_history[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Broader Concepts:\n",
    "\n",
    "Backpropagation exemplifies how fundamental mathematical concepts, particularly calculus and linear algebra, are applied in the realm of machine learning and data science to optimize models for better predictions. It also provides a pathway to understanding how errors can be minimized in predictive modeling, which is a core aspect of data science. The algorithmâ€™s efficacy in minimizing error during training elucidates the importance of optimization techniques in machine learning and data science.\n",
    "\n",
    "<center><img src=\"./images/backpropagation.png\" height=\"500\" width=\"800\"></center>\n",
    "\n",
    "visualized: https://nnfs.io/pro/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Implementation of backprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with 5 elements\n",
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## require_grad=True\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## operations on the tensor\n",
    "\n",
    "# Computational Graph is created in the forward pass\n",
    "y = x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Computes the gradients w.r.t. the parameters of the model\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y #grad_fn=<PowBackward0> is the gradient function (for backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a backward pass\n",
    "z = 2*y + x\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.mean() #mean of all the elements in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dz/dx\n",
    "\n",
    "z.mean().backward() #scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad #gradient of z with respect to x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector jacobian product or chain rule\n",
    "\n",
    "The jacobian matrix is an array of partial derivatives in all of the possible combinations of the input and output variables.\n",
    "\n",
    "<center><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e343f872b676a0e64646f27593d03c77c53cbaf3\" height=\"400\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32, requires_grad=True)\n",
    "y = x**2\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jacobian matrix\n",
    "j = torch.tensor([0.1, 1.0, 0.001, 0.0001, 0.00001], dtype=torch.float32)\n",
    "x.backward(j) #jacobian matrix - if not scalar value then we must pass a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to suspend tracking of gradients\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x**2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other options to stop tracking gradients\n",
    "\n",
    "x.requires_grad_(False)\n",
    "x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training example\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3): # gradients are accumulated\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    \n",
    "    # empty the gradients\n",
    "    # weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating the gradients manually\n",
    "\n",
    "# 1. forward pass\n",
    "# 2. compute gradients\n",
    "# 3. Backward pass (d loss / d weights)\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass and compute the loss\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2 # linear regression loss\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## backward pass\n",
    "loss.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# linear regression\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = Mean Squared Error (MSE)\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "\n",
    "# gradient = d(loss)/d(w) = 1/N * 2x (xw - y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'[INFO]: epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "    \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'[INFO]: epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "* Model Design (input, output size, forward pass)\n",
    "* Create loss and optimizer\n",
    "* Training Loop\n",
    "    * Forward pass - compute prediction\n",
    "    * Backward pass - gradients\n",
    "    * Update weights - optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn #neural networks\n",
    "\n",
    "## linear regression\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape # 4, 1 (n_samples, n_features)\n",
    "\n",
    "test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# class LinearRegression(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(LinearRegression, self).__init__() # super class constructor\n",
    "#         self.lin = nn.Linear(input_dim, output_dim) # define the linear layer\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.lin(x)\n",
    "    \n",
    "model = nn.Linear(input_size, output_size) # == LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(test).item():.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "loss = nn.MSELoss() # mean squared error loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # stochastic gradient descent\n",
    "\n",
    "# training\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    optimizer.step() # update weights\n",
    "    \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'[INFO]: epoch {epoch + 1}: w = {model.weight.item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
