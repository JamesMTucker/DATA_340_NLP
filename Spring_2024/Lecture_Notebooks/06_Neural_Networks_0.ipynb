{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Overview\n",
    "\n",
    "- Where are we at the history of AI?\n",
    "- Machine learning and stochastic or probabilistic models\n",
    "- Bag of words and sequence models\n",
    "- Machine Learning and Deep Learning\n",
    "- Coding out the percpeptron\n",
    "- Numpy and vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we at the history of AI?\n",
    "\n",
    "- AlphaGo\n",
    "- AlphaZero\n",
    "- AlphaFold\n",
    "- GPT-3\n",
    "- ChatGPT\n",
    "- OpenAI Codex\n",
    "- Dall-E\n",
    "- OpenAI CLIP\n",
    "- Stable Diffusion\n",
    "- GPT-4\n",
    "\n",
    "\n",
    "<center><img src=\"../images/jurafsky_goal.png\" width=\"800\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning and stochastic or probabilistic model\n",
    "\n",
    "Machine learning models can become very powerful tools with the right data and the right model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/where_we_are.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set environment variables if in google colab\n",
    "import os\n",
    "\n",
    "IN_COLAB = False\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # mount our google drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    data_dir = \"/content/drive/MyDrive/DATA_340_NLP/Datasets\"\n",
    "else:\n",
    "    data_dir = \"../datasets\"\n",
    "    \n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words and sequence models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Step 1: Read in the text input file and store the contents as a string variable.\n",
    "with open(os.path.join(data_dir, 'LOTR', '01_LOTR_Fellowship.txt'), 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 2: Remove any punctuation, numbers, or special characters from the text string.\n",
    "text = re.sub('[^A-Za-z ]+', '', text)\n",
    "\n",
    "# Step 3: Convert the string to lowercase to ensure consistency.\n",
    "text = text.lower()\n",
    "\n",
    "# Step 4: Split the text string into individual words, and store each word in a list.\n",
    "words = text.split()\n",
    "\n",
    "# Step 5: Create an empty dictionary to store the word counts.\n",
    "word_counts = {}\n",
    "\n",
    "# Step 6: Remove stopwords from the list of words.\n",
    "words = [word for word in words if word not in list(stopwords.words('english'))]\n",
    "\n",
    "# Step 6: Iterate through the list of words, and for each word:\n",
    "for word in words:\n",
    "    # a. If the word is already in the dictionary, increment its count by 1.\n",
    "    if word in word_counts:\n",
    "        word_counts[word] += 1\n",
    "    # b. If the word is not in the dictionary, add it to the dictionary with a count of 1.\n",
    "    else:\n",
    "        word_counts[word] = 1\n",
    "\n",
    "# Step 7: Sort the dictionary by count, in descending order.\n",
    "sorted_word_counts = dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Step 8: Display the top words in the word cloud, using larger font sizes for words with higher counts.\n",
    "wordcloud = WordCloud(width=800, height=800, background_color='white', max_words=100).generate_from_frequencies(sorted_word_counts)\n",
    "plt.figure(figsize=(8,8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We created N-Grams Models \n",
    "* We feature engineered our data using bag of words and TF-IDF\n",
    "* We used Naive Bayes and Logistic Regression to classify text\n",
    "* We introduced concepts of feature engineering, model architecture, model evaluation, hyperparameter tuning, and model selection\n",
    "\n",
    "Now we want to take it a step further and use neural networks to analyze text. The importance of NLP as a data scientists is that nearly every analysis we do will involve text in some way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words and sequence models\n",
    "\n",
    "Langauge, however, is not just a bag of words. It is a sequence of words. So, we can ask ourselves how we can improve our models by taking into account the sequence of words? What architectures can we use to do this? And what architectures can we engineer to model the sequence of words? How can our data structures be improved to model the sequence of words?\n",
    "\n",
    "* Neural networks\n",
    "* Recurrent Neural Networks\n",
    "* Long Short Term Memory (LSTM)\n",
    "* Gated Recurrent Unit (GRU)\n",
    "* Transformers\n",
    "\n",
    "\n",
    "* Vectorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning and Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://images.anandtech.com/doci/12673/AI-Circle.png\" width=\"800\" height=\"600\" /></center>\n",
    "\n",
    "Image source: https://www.anandtech.com/show/12673/titan-v-deep-learning-deep-dive/2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding out the percpeptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/Neuron.drawio.png\" width=\"800\" height=\"400\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's code the perceptron out from basic python\n",
    "\n",
    "inputs = [1, 2, 3, 2.5] # tokens (the output of a tokenization process: tiktoken)\n",
    "\n",
    "weights = [0.2, 0.8, -0.5, 1.0] # random weights - we will learn how to adjust these weights\n",
    "\n",
    "bias = 2 # random bias - we will learn how to adjust this bias\n",
    "\n",
    "outputs = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + inputs[3] * weights[3] + bias\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a linear plot for weight and bias\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = 2 * x + 1\n",
    "\n",
    "# Plot the line\n",
    "plt.plot(x, y, '-r', label='y=2x+1')\n",
    "\n",
    "# add a grid\n",
    "plt.grid()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus conceptualize the `output = weight*input+bias` is similar to a linear relationship: $y = mx + b$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make create a layer of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "\n",
    "# Every input has a weight with it\n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "\n",
    "\n",
    "# Every neuron has one bias.\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "\n",
    "# Add up inputs + weights + bias\n",
    "output = [inputs[0] * weights1[0] + inputs[1] * weights1[1] + inputs[2] * weights1[2] + inputs[3] * weights1[3] + bias1,\n",
    "          inputs[0] * weights2[0] + inputs[1] * weights2[1] + inputs[2] * weights2[2] + inputs[3] * weights2[3] + bias2,\n",
    "          inputs[0] * weights3[0] + inputs[1] * weights3[1] + inputs[2] * weights3[2] + inputs[3] * weights3[3] + bias3]\n",
    "\n",
    "\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a Layer of Neurons\n",
    "\n",
    "https://www.youtube.com/watch?v=fXSRfzhHPm0&t=18s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can rewrite our code to be more efficient to handle multiple neurons\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    "            [0.5, -0.91, 0.26, -0.5],\n",
    "            [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = []\n",
    "\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    \"\"\"process the neuron weights with their biases: output = input*weight\"\"\"\n",
    "    neuron_output = 0\n",
    "\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input*weight\n",
    "    \n",
    "    neuron_output += neuron_bias\n",
    "\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "layer_outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We bring additonal optimization by working with with vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1]\n",
    "\n",
    "biases = 2\n",
    "\n",
    "## We can use numpy to make our code more efficient\n",
    "layer_outputs = np.dot(weights, inputs) + biases\n",
    "\n",
    "layer_outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Numpy dot product defined:\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^n a_1 b_2 + a_2 b_2 + \\dots a_n b_n$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    "            [0.5, -0.91, 0.26, -0.5],\n",
    "            [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "ayer_outputs = np.dot(weights, inputs) + biases\n",
    "\n",
    "layer_outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need an activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rectified Linear Activation Function\n",
    "\n",
    "## plot the function\n",
    "X = np.linspace(-10, 10, 100)\n",
    "y = np.maximum(X, 0)\n",
    "\n",
    "plt.plot(X, y, '-r', label='ReLU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches\n",
    "\n",
    "Before we can code out a neural network, we need to introduce the idea of batches. Batches are a way to speed up our neural network. We can think of batches as a way to vectorize our entire neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Batches and shape errors\n",
    "\n",
    "inputs = [[1.0,2.0,3.0,2.5],\n",
    "          [2.0,5.0,-1.0,2.0],\n",
    "          [-1.5,2.7,3.3,-0.8]]\n",
    "\n",
    "weights = [[0.2,0.8,-0.5,1.0],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "weights2 = [[0.1,-0.14,0.5],\n",
    "            [-0.5,0.12,0.33],\n",
    "            [-0.44,0.73,-0.13]]\n",
    "\n",
    "biases = [2.0,3.0,0.5]\n",
    "biases2 = [-1,2,-0.5]\n",
    "\n",
    "# if we try to forward pass over the weights, we get an error   \n",
    "outputs = np.dot(inputs, weights) + biases\n",
    "\n",
    "# ValueError: shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)\n",
    "\n",
    "# we have to reshape the data for the proper function to work\n",
    "output_layer1 = np.dot(inputs, np.array(weights).T) + biases\n",
    "output_layer2 = np.dot(output_layer1, np.array(weights2).T) + biases2\n",
    "\n",
    "print(output_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication and Transpose\n",
    "\n",
    "https://nnfs.io/jei/\n",
    "\n",
    "\n",
    "Transposition is reshaping the array and/or broadcasting the array to a new shape. The rows become the columns and the columns become the rows. The transpose of a matrix is denoted by $A^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [\n",
    "    [1, 5, 7, 6],\n",
    "    [2, 1, 3, 5],\n",
    "    [4, 7, 2, 9],\n",
    "    [1, 2, 4, 5],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how can we transpose this matrix?\n",
    "\n",
    "class tensor:\n",
    "    \"\"\"This class let's us transpose a matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, matrix):\n",
    "        self.matrix = matrix\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        return [[row[i] for row in self.matrix] for i in range(len(self.matrix[0]))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = tensor(matrix)\n",
    "matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [\n",
    "    [1, 5, 7, 6],\n",
    "    [2, 1, 3, 5],\n",
    "    [4, 7, 2, 9],\n",
    "    [1, 2, 4, 5],\n",
    "]\n",
    "\n",
    "def T(matrix: list[list[int]]) -> list[list[int]]:\n",
    "    \"\"\"This function let's us transpose a matrix\n",
    "    \"\"\"\n",
    "    T = []\n",
    "    \n",
    "    for col in range(len(matrix[0])):\n",
    "        new_row = []\n",
    "        for row in range(len(matrix)):\n",
    "            new_row.append(matrix[row][col])\n",
    "        T.append(new_row)\n",
    "    return T  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "\n",
    "When we multiply two matrices, we need to make sure that the inner dimensions are the same. The inner dimensions are the number of columns in the first matrix and the number of rows in the second matrix. The result of the multiplication will be the outer dimensions. The outer dimensions are the number of rows in the first matrix and the number of columns in the second matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication in vanilla python\n",
    "\n",
    "def matmul(matrix_a, matrix_b):\n",
    "    \"\"\"multiply two matrices together\n",
    "    \"\"\"\n",
    "    result = [[0 for i in range(len(matrix_b[0]))] for j in range(len(matrix_a))]\n",
    "    \n",
    "    for i in range(len(matrix_a)):\n",
    "        for j in range(len(matrix_b[0])):\n",
    "            for k in range(len(matrix_b)):\n",
    "                result[i][j] += matrix_a[i][k] * matrix_b[k][j]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_a = [\n",
    "    [1, 5, 7, 6],\n",
    "    [2, 1, 3, 5],\n",
    "]\n",
    "\n",
    "matrix_b = [\n",
    "    [1, 5],\n",
    "    [2, 1],\n",
    "    [4, 7],\n",
    "    [1, 2],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply matrix_a by matrix_b\n",
    "matmul(matrix_a, matrix_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy matrix multiplication\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "matrix_a = np.array(matrix_a)\n",
    "matrix_b = np.array(matrix_b)\n",
    "\n",
    "method_1 = np.dot(matrix_a, matrix_b)\n",
    "method_2 = matrix_a @ matrix_b\n",
    "\n",
    "assert (method_1 == method_2).all()\n",
    "\n",
    "method_1, method_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Layers with batch processing to our Neural Network\n",
    "\n",
    "Understanding the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Batches and shape errors\n",
    "\n",
    "inputs = [[1.0,2.0,3.0,2.5],\n",
    "          [2.0,5.0,-1.0,2.0],\n",
    "          [-1.5,2.7,3.3,-0.8]]\n",
    "\n",
    "weights = [[0.2,0.8,-0.5,1.0],\n",
    "           [0.5,-0.91,0.26,-0.5],\n",
    "           [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "weights2 = [[0.1,-0.14,0.5],\n",
    "            [-0.5,0.12,0.33],\n",
    "            [-0.44,0.73,-0.13]]\n",
    "\n",
    "biases = [2.0,3.0,0.5]\n",
    "biases2 = [-1,2,-0.5]\n",
    "\n",
    "\n",
    "## The forward pass\n",
    "output_layer1 = np.dot(inputs, np.array(weights).T) + biases\n",
    "output_layer2 = np.dot(output_layer1, np.array(weights2).T) + biases2\n",
    "\n",
    "print(output_layer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the realm of neural networks, the forward pass denotes the process of input data being propagated through the network to eventually generate an output. This involves a series of linear transformations and non-linear activations. The linear transformations are often captured by the dot product between the input data and the weights of the network. Understanding the rationale behind transposing the weight matrix during this dot product requires delving into the dimensions and algebraic operations involved.\n",
    "\n",
    "1. **Matrix Dimensions**:\n",
    "    - In a neural network, each layer's input and weights are represented as matrices. Suppose the input matrix has dimensions $(m, n)$ where $m$ is the number of instances and $n$ is the number of features, and the weight matrix has dimensions $(n, p)$ where $p$ is the number of neurons in the next layer.\n",
    "    - The aim is to compute a resultant matrix of dimensions $(m, p)$ which will then be passed through an activation function. This resultant matrix represents the linear transformation of the input data through the current layer.\n",
    "\n",
    "2. **Dot Product Operations**:\n",
    "    - The dot product between two matrices requires that the number of columns in the first matrix match the number of rows in the second matrix. Hence, in order to perform the dot product between the input matrix and the weight matrix, one of them needs to be transposed.\n",
    "    - Typically, it's the weight matrix that is transposed to conform to this requirement. Hence, the weight matrix dimensions become $(p, n)$.\n",
    "\n",
    "3. **Algebraic Representation**:\n",
    "    - The algebraic operation can be represented as: $\\text{Resultant Matrix} = \\text{Input Matrix} \\cdot \\text{Transposed Weight Matrix}$ or $\\text{Result} = \\text{Input} \\cdot \\text{Weights}^T$.\n",
    "    - This operation ensures that each neuron's weights are correctly aligned with the input features during the dot product, facilitating the linear transformation aimed for in this layer of the network.\n",
    "\n",
    "4. **Coding Perspective**:\n",
    "    - From a coding standpoint, transposing the weight matrix and performing the dot product is a straightforward operation in languages like Python, utilizing libraries like NumPy.\n",
    "  \n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    # Assume input_matrix is of shape (m, n) and weights_matrix is of shape (n, p)\n",
    "    input_matrix = np.array([[...], [...], ...])\n",
    "    weights_matrix = np.array([[...], [...], ...])\n",
    "\n",
    "    # Performing the dot product\n",
    "    resultant_matrix = np.dot(input_matrix, weights_matrix.T)\n",
    "    ```\n",
    "\n",
    "**An analogy**:\n",
    "Think of the forward pass like a relay race where data is passed from one layer to the next. Now, when the input data reaches a layer, it's like a runner (input) trying to hand over the baton to the next runner (neurons in the next layer). But they can only successfully hand over the baton if their hands (dimensions) align properly. Transposing the weight matrix is like adjusting the hands so that the baton can be passed smoothly, ensuring that each feature in the input data properly interacts with the corresponding weights of the neurons in the layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's increase the sophistication of our neural network design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "dense1.forward(X)\n",
    "\n",
    "print(dense1.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding our activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "layer1 = Layer_Dense(2, 5) # inputs and neurons\n",
    "activation1 = ReLU()\n",
    "\n",
    "layer1.forward(X) # inputs * weights + bias\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU and Hidden Layers\n",
    "\n",
    "We can use the ReLU activation function to add non-linearity to our neural network. We can also add multiple hidden layers to our neural network. This will allow our neural network to learn more complex relationships between our inputs and outputs.\n",
    "\n",
    "https://nnfs.io/mvp/\n",
    "\n",
    "https://nnfs.io/moo/\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation Function\n",
    "\n",
    "Let's now add an activation function to our neural network. We will use the softmax activation function. The softmax activation function is used for classification problems. It will output a probability distribution over the possible classes.\n",
    "\n",
    "$$S_{i,j} = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In Python:\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# Step 1: exponetiate the values with Euler's number\n",
    "\n",
    "E = 2.71828182846 # math.e\n",
    "\n",
    "exp_values = []\n",
    "for o in layer_outputs:\n",
    "    exp_values.append(E**o)\n",
    "exp_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: normalize the values\n",
    "norm = sum(exp_values) # sum of all the values\n",
    "\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value/norm)\n",
    "\n",
    "norm_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: sum of all the values should be 1\n",
    "sum(norm_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Softmax activate class\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        # save the output\n",
    "        self.output = probabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add the softmax activation function to our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Our training data\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 5 output values\n",
    "dense1 = Layer_Dense(2, 5) # inputs and neurons\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = ReLU()\n",
    "\n",
    "# Create second Dense layer with 5 input features\n",
    "dense2 = Layer_Dense(5, 3) # inputs and neurons\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Softmax()\n",
    "\n",
    "# Complete forward pass\n",
    "dense1.forward(X) # inputs * weights + bias\n",
    "\n",
    "# Complete forward pass\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Complete forward pass\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Complete forward pass\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "activation2.output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Does the output make sense?\n",
    "\n",
    "# let's plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='brg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding out the percpeptron\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, n_inputs, epochs=10, learning_rate=0.01):\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.zeros(n_inputs + 1)\n",
    "        self.errors = []\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        if summation > 0:\n",
    "            activation = 1\n",
    "        else:\n",
    "            activation = 0\n",
    "        return activation\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.epochs):\n",
    "            error = 0\n",
    "            for inputs, target in zip(X, y):\n",
    "                prediction = self.predict(inputs)\n",
    "                update = self.learning_rate * (target - prediction)\n",
    "                self.weights[1:] += update * inputs\n",
    "                self.weights[0] += update\n",
    "                error += int(update != 0.0)\n",
    "            self.errors.append(error)\n",
    "        return self\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
