{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e8baef-44b9-4323-91cd-e2eb90e3ad9a",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/Notebooks/Lecture_04_2023_02_07.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300f09b-e852-48a1-8233-5ebcfc2158c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "Properties_of_Langauage"
    ]
   },
   "source": [
    "# Lecture 04: Properties of Language, Statistics, Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055be336-7d8c-40b7-a0b1-1b28c59eba89",
   "metadata": {
    "tags": [
     "Properties_of_Langauage"
    ]
   },
   "source": [
    "# Properties of Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965dd51-5408-4fb5-a74b-5a795a1a554e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Our shared humanity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c192913-605a-4ec1-9e6c-abc28f8962c3",
   "metadata": {},
   "source": [
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049087/\n",
    "\n",
    "| Article | Pubmed Link |\n",
    "| ---- | ---- |\n",
    "| ![17_Languages](./images/17_languages.png) | ![17_Languages](./images/17_languages_qr.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d793f012-cda7-4e96-962f-82825c5ceb44",
   "metadata": {
    "tags": [
     "Swadesh_Terms"
    ]
   },
   "source": [
    "> We studied the frequencies of use in each of these languages for the 200 words that make up the Swadesh fundamental vocabulary word list. (p. 1102)\n",
    "\n",
    "> Our results point to a surprising regularity in the way that human speakers use language. It might be that the way we use language and its structure means that some words inevitably will be used more than others.\n",
    "\n",
    "* [Swadesh terms](https://en.wikipedia.org/wiki/Swadesh_list)\n",
    "* [Lexicostatistics](https://en.wikipedia.org/wiki/Lexicostatistics) (comparative linguistics - lexical cognates between languages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bb299-e587-4c7c-9d77-ef98629efc71",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Similarities of Swadesh term usage in different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55fa8d-5efb-4545-92a4-f4a7505a1531",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/figure_3.png\"  width=\"800\" height=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0042df28-4721-4d1e-88fb-870e8d0ff691",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "LOTR"
    ]
   },
   "source": [
    "### Let's explore this in more detail\n",
    "\n",
    "Let's compare the three volumes of _The Lord of the Rings_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62781d5e-23f0-4f42-9efd-f5df66395433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81dc5e4-0bfb-401e-bae2-49474c6fff26",
   "metadata": {},
   "source": [
    "N.B.: If you are running this notebook in Jupyter Lab, then uncomment the below code accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc639d8-ca32-4f5f-b5ad-bead6d110050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Google colab, uncomment this line of code and run\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/', force_remount=True)\n",
    "# files = Path('gdrive/MyDrive/DATA_340_3_NLP/Datasets').glob('*.txt')\n",
    "# We can use the Path package to create a generator of all patternized items\n",
    "files = Path('../datasets').glob('*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549e341-29dd-4e20-9a86-e5582b9cf955",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets iterate over the generator and create a list of lists with a Short Volume name and its text\n",
    "corpus = []\n",
    "\n",
    "# Iterate over the files\n",
    "for f in files:\n",
    "    print(f)\n",
    "    # Let's grab the short name from the file name\n",
    "    base_name = os.path.basename(f)\n",
    "    f_name, _ = os.path.splitext(base_name)\n",
    "    \n",
    "    # Open the file and read its content\n",
    "    with open(f, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "        # Append the short name and the text to the corpus list\n",
    "        corpus.append([f_name, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb311366-c112-424c-8b46-75f1ec305b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's look at our corpus\n",
    "corpus[0][0], corpus[0][1][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f4702-5b0f-47c1-869f-098388188ac4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Zipf's Law\n",
    "\n",
    "[George Kingsley Zipf](https://en.wikipedia.org/wiki/George_Kingsley_Zipf) argued that most words are not used that often. He formally defined his theorem as\n",
    "$$P_n \\sim \\frac{1}{n^a}$$\n",
    "\n",
    "It is a power law distribution. The frequency of any word is inverse in porportion to its rank in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0026d963-d7e6-4154-8269-4581408d3578",
   "metadata": {},
   "source": [
    "Let's write a function to compute the frequency of vocabulary items over a volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633f4a9-05fa-4495-8825-8948b7590685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def zipf_analysis(text, book):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_freq = Counter(words) # this one line of code does the same as the following for loop\n",
    "    \n",
    "    # vanilla python implementation\n",
    "    # word_freq = {}\n",
    "    # for word in words:\n",
    "        # word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    \n",
    "    # Sort the words by frequency - highest occuring terms are at the top\n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Plot the word frequency and rank to check for Zipf's law\n",
    "    word_rank = np.arange(1, len(sorted_word_freq)+1) # X variable\n",
    "    word_frequency = [i[1] for i in sorted_word_freq] # Y variable\n",
    "    \n",
    "    # Plot log to visualize the power law distribution\n",
    "    plt.loglog(word_rank, word_frequency, marker='o')\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f\"Zipf's Law for {book}\")\n",
    "    plt.show()\n",
    "    return sorted_word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e81cda2-7cff-4ee2-945c-68b44fa8625a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Fellowship of the Ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fellowship_name = corpus[0][0]\n",
    "felloship_text = corpus[0][1]\n",
    "\n",
    "fellowship_name, felloship_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f3215-ab51-4e06-a2f4-c5f82d7d2284",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fellowship = zipf_analysis(felloship_text, 'Fellowship of the Ring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49adadd7-93a1-4684-a6f6-3dd03ee0b94b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here let's use the output to explore the words that occur only once\n",
    "# We can use pandas to explore our data\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the tuples to a dataframe\n",
    "df = pd.DataFrame(fellowship, columns=['word', 'frequency'])\n",
    "\n",
    "# Let's query the dataframe for words that occure only once\n",
    "fellowship_hapax_legomenon = df.query('frequency == 1')\n",
    "fellowship_hapax_legomenon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502efb38-03ac-4163-abf2-d953b84d509d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Two Towers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdaf30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[1][0], corpus[1][1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7900b-4c90-4183-907a-4089909328b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's check the distribution of the two towers\n",
    "two_towers = zipf_analysis(corpus[1][1], 'The Two Towers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b804930-aad3-4de7-b344-6264ecd7a05a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's use Pandas again to look at some word frequencies\n",
    "df = pd.DataFrame(two_towers, columns=['word', 'frequency'])\n",
    "\n",
    "two_towers_hapax_legomenon = df.query('frequency == 1')\n",
    "two_towers_hapax_legomenon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e9a76-fc15-48b2-a319-17ce03c3327a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Return of the King"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d776cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[2][0], corpus[2][1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab395a4a-2b4d-42ea-b42c-dedc86d946be",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of terms\n",
    "return_king = zipf_analysis(corpus[2][1], 'The Return of the King')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f106242-ee10-4698-83bf-adc75287b75c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's explore again some of the lower frequency terms\n",
    "df = pd.DataFrame(return_king, columns=['word', 'frequency'])\n",
    "\n",
    "return_king_hapax_legomenon = df.query('frequency == 1')\n",
    "return_king_hapax_legomenon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nietzsche\n",
    "corpus[3][0], corpus[3][1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8010f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nietzsche = zipf_analysis(corpus[3][1], 'Nietzsche')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore again some of the lower frequency terms\n",
    "df = pd.DataFrame(Nietzsche, columns=['word', 'frequency'])\n",
    "\n",
    "Nietzsche_hapax_legomenon = df.query('frequency == 1')\n",
    "Nietzsche_hapax_legomenon.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e1c15-128c-4597-9eb0-09b723c6bdde",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Most used word in the USA:\n",
    "\n",
    "The above demonstration of _The Lord of the Rings_ is generalizable to any English text, and as discussed above to many languages for certain kinds of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb314aa-15e8-4a59-899e-b300b65a84df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<img src=\"./images/most_used_01.png\"  width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d872736-56f2-4ea2-a8c3-315f0f4c385a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "But ... <br>\n",
    "<img src=\"./images/most_used_02.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0afd64-e640-45bd-a661-e894ae9c5a06",
   "metadata": {},
   "source": [
    "N.B.: Notice the study of Manning and Schutze, _Foundations of Statistical Natural Language Processing_, who demonstrate that a randomly created text follows the power law observation as discussed by Mandelbrot. They conclude their discussion observing that:\n",
    "\n",
    "> what makes frequency-based approaches to language hard is that almost all words are rare. Zipf's law is a good way to encapsulate this insight. (p. 29)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026cea7b",
   "metadata": {},
   "source": [
    "## Let's Tokenize, Lemmatize, and Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86763ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use NLTK to tokenize and lemmatize our text\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Create instances of the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# For stopwords we will add punctuation\n",
    "punct = list(string.punctuation) + list(string.digits)\n",
    "stop_words = stopwords.words('english') + punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23389d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to append the tokens and not stopwords\n",
    "lemmas = []\n",
    "\n",
    "# Iterate over the text to extract our lemmas\n",
    "def tokenize_lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        else:\n",
    "            lemmas.append(stemmer.stem(token))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad6b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass our text to the above function so we can then create a bigram dictionary\n",
    "fellowship_token_lemmas = tokenize_lemmatize_text(corpus[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fellowship_token_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a bi-token dictionary\n",
    "bigram_freqs = {}\n",
    "\n",
    "# List comprehension to create a list of bigrams\n",
    "bigrams = [(fellowship_token_lemmas[i], fellowship_token_lemmas[i + 1]) for i in range(len(fellowship_token_lemmas) - 1)]\n",
    "\n",
    "# The bigrams are repeated so we want to count the frequency of terms\n",
    "for bigram in bigrams:\n",
    "    bigram_freqs[bigram] = bigram_freqs.get(bigram, 0) + 1\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64115ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_sorted = list(sorted(bigram_freqs.items(), key=lambda kv: -kv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dataframe of the bigrams using pandas\n",
    "import pandas as pd\n",
    "\n",
    "# to create the dataframe we need to use pd.DataFrame and pass it our data and give it some column names\n",
    "df = pd.DataFrame(bigrams_sorted, columns=['bigram', 'freq'])\n",
    "\n",
    "# Let's expand the bigrams to their own columns and keep the index so we can retain the frequencies\n",
    "df[['first_term', 'second_term']] = pd.DataFrame(df['bigram'].tolist(), index=df.index)\n",
    "\n",
    "# And drop the bigram column since we now have the lemmas in their own columns\n",
    "df = df.drop(columns=['bigram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"first_term == 'frodo'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd75dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_frodo = df.query(\"second_term == 'frodo'\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6307744",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_frodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the frequencies to get the total count\n",
    "x_frodo.freq.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
