{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11: 2021-07-03 Naive Bayes Classifier II"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Lecture Outline\n",
    "\n",
    "- Assignments and Grades\n",
    "- Example of NLP research [ChatGPT](https://arxiv.org/pdf/2302.13795.pdf)\n",
    "- Naive Bayes Classification\n",
    "  - Continuous\n",
    "  - Multinomial\n",
    "- Naive Bayes Classifier\n",
    "  - Example\n",
    "  - Implementation\n",
    "  - Evaluation\n",
    "  - Comparison with Logistic Regression\n",
    "\n",
    "Keyterms: #model, #heuristic, #Bernoulli, #Prior, #Likelihood, #Evidence, #Posterior, #Vocabulary, #Laplace_Smoothing, #Tokenization, #Features, #TF-IDF, #Vectorization, #Stopwords, #Stemming, #Lemmatization, #Naive_Bayes_Classifier, #Multinomial, #Continuous, #Evaluation, #Accuracy, #Precision, #Recall, #F1_Score, #Confusion_Matrix, #ROC_Curve, #AUC, #Logistic_Regression, #Comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Summary of the Naive Bayes Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Background Assumptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes theorem is a fundamental result in statistics and machine learning. It is used to compute the posterior probability of an event given some evidence. To understand Bayes theorem, let's assume we have two events $A$ and $B$. We let $A$ stand for a movie review being positive and $B$ stand for the review being negative. We want to compute the probability that a review is positive given that the review contains the word \"excellent\". Thus, we have two classes where the membership of $x$ is deteremined by the words contained in the review or `document`: $c_i = \\{d \\space | \\space d \\space is \\space positive\\}$ and $c_j = \\{d \\space | \\space d \\space is \\space negative\\}$.\n",
    "\n",
    "So we can ask the question: \"What is the probability that a review is positive given that the review contains the word \"excellent\"?\n",
    "\n",
    "$$P('excellent'|positive)$$\n",
    "\n",
    "If we assumed the presence of one word is dependent on the presence of any other word in our vocabulary, we would therefore have to compute the chain probability of an array of words with every word in our vocabulary:\n",
    "\n",
    "$$P(d|v) = P(w_1, w_2, \\ldots, w_n|v) = P(w_1|v)P(w_2|v)\\ldots P(w_n|v)$$\n",
    "\n",
    "This of course is computationally infeasible. So we make the assumption that the presence of one word is independent of the presence of any other word in our vocabulary. This is called the Naive Bayes assumption. This assumption is naive because it is not true in general. However, it is a good approximation in many cases. We can write the probability of a document $d$ given a class $c$ as:\n",
    "\n",
    "$$P(d|c) = P(w_1|c)P(w_2|c)\\ldots P(w_n|c)$$\n",
    "\n",
    "\n",
    "We have simplified the problem, yet we have introduced a `bias`. The `bias` betrays our understanding of linguistic experience where it is natural to assume the presence of one word is conditionally dependent on the presence of other words: e.g, `Richmond, Virginia`.\n",
    "\n",
    "We can now compute the probability that a review is positive given that the review contains the word \"excellent\":\n",
    "\n",
    "$$\\frac{P(positive) \\times P('excellent'|positive)}{P(positive) \\times P('excellent'|positive) + P(negative) \\times P('excellent'|negative)}$$\n",
    "\n",
    "So we can understand the proir, likelihood, evidence, and posterior as follows:\n",
    "\n",
    "`Priors` = $P(positive)$ and $P(negative)$ <br>\n",
    "\n",
    "`likelihoods` = $P('excellent'|positive)$ and $P('excellent'|negative)$\n",
    "\n",
    "`evidence` = $P(positive) \\times P('excellent'|positive) + P(negative) \\times P('excellent'|negative)$\n",
    "\n",
    "`posterior` = $\\frac{P(positive) \\times P('excellent'|positive)}{P(positive) \\times P('excellent'|positive) + P(negative) \\times P('excellent'|negative)}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Data Preprocessing steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now featurize our data by the following steps:\n",
    "\n",
    "- Tokenization\n",
    "- Stopwords removed (low information words)\n",
    "- Strip punctuation (low semantic value)\n",
    "- Stemming or Lemmatization\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes Classifier for Sentiment Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Twitter Sentiment Analysis\n",
    "\n",
    "In this example, we will use the [Twitter US Airline Sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment) dataset from Kaggle. The dataset contains 14,640 tweets from 6 US airlines. The tweets are labeled as positive, negative, or neutral. The goal is to predict the sentiment of a tweet given the text of the tweet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1. Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import our libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2. Defining our negative and positive classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "## check the number of positive and negative tweets\n",
    "len(pos_tweets), len(neg_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweet: Stats for the day have arrived. 4 new followers and NO unfollowers :) via http://t.co/Hk8VpyVCu6.\n",
      "Negative tweet: why so sudden :((\n"
     ]
    }
   ],
   "source": [
    "## examine some random samples\n",
    "print('Positive tweet:', pos_tweets[random.randint(0,5000)])\n",
    "print('Negative tweet:', neg_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.3. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Input: tweet a string containing a tweet\n",
    "    Return:\n",
    "    tweets_clean: a list of words containing the processed tweet\n",
    "    \"\"\"\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # remove retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # Instantiate stemmer class\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Create stopwords list\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # Tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    \n",
    "    # Tokenize the tweets\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "    \n",
    "    return tweets_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.4. Let's create our train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pos_tweets + neg_tweets,\n",
    "                                                    np.append(np.ones(len(pos_tweets)),\n",
    "                                                    np.zeros(len(neg_tweets))),\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000, 8000, 2000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Counts of positive and negative tweets\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: @xfiIes :() IS there A CAT CAFE IN MELBOURNE\n",
      "Tweet label: 0.0\n"
     ]
    }
   ],
   "source": [
    "## Examine some random samples\n",
    "idx = random.randint(0, len(X_train))\n",
    "\n",
    "print('Tweet:', X_train[idx])\n",
    "print('Tweet label:', y_train[idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.5. Let's define our frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create a dictionary of words and their frequencies\n",
    "def count_tweets(result, tweets, ys):\n",
    "    \"\"\"Input:\n",
    "    result: a dictionary that will contain the frequency of each pair (word, label)\n",
    "    tweets: a list of tweets\n",
    "    ys: an m x 1 array with the sentiment label of each tweet (either 0 or 1)\n",
    "    \"\"\"\n",
    "    # iterate through each tweet and its label\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        # process the tweet to get the words in the form of a list\n",
    "        for word in process_tweet(tweet):\n",
    "            # increment the word count for the pair (word, label)\n",
    "            pair = (word, y)\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = count_tweets({}, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs[('happi', 1.0)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.6. Let's define our Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create a function to extract the features from the tweets\n",
    "\n",
    "def train_naive_bayes(freqs, X, y):\n",
    "    \"\"\"Train a Naive Bayes classifier on twitter data.\n",
    "\n",
    "    Args:\n",
    "        freqs (dict): dictionary of (word, label): frequency pairs\n",
    "        X_train (list): list of tweets\n",
    "        y_train (list): list of tweets\n",
    "        \n",
    "    returns:\n",
    "    logprior (float): log prior\n",
    "    loglikelihood (dict): dictionary of (word, label): log likelihood pairs\n",
    "    \"\"\"\n",
    "    ## Compare the code here with Jurafsky and Martin's pseudocode\n",
    "    \n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    \n",
    "    vocab = set([pair[0] for pair in freqs.keys()]) # words in the vocabulary\n",
    "    V = len(vocab) # number of unique words in the vocabulary\n",
    "    \n",
    "    # Calculate N_pos and N_neg tweets (number of positive and negative tweets)\n",
    "    N_pos, N_neg = 0, 0 # number of positive and negative tweets\n",
    "    \n",
    "    # Calculate the number of positive and negative tweets\n",
    "    for pair in freqs.keys():\n",
    "        if pair[1] > 0:\n",
    "            N_pos += freqs[pair] # positive tweets\n",
    "        else:\n",
    "            N_neg += freqs[pair] # negative tweets\n",
    "    \n",
    "    # Documents = total number of tweets\n",
    "    D = len(X)\n",
    "    \n",
    "    # Calculate # of positive and negative documents\n",
    "    D_pos = np.sum(y)\n",
    "    D_neg = D - D_pos\n",
    "    \n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    \n",
    "    for word in vocab:\n",
    "        \n",
    "        freq_pos = freqs.get((word, 1.0), 0)\n",
    "        freq_neg = freqs.get((word, 0.0), 0)\n",
    "        \n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "        \n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "    \n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.7. Let's train our Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test our function\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.7. Let's test our Naive Bayes Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_bayes(tweet, logprior, loglikelihood):\n",
    "    \"\"\"Input:\n",
    "    tweet: a string\n",
    "    logprior: a number\n",
    "    loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "    p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "    \"\"\"\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    p = 0\n",
    "    p += logprior\n",
    "    \n",
    "    for word in word_l:\n",
    "        if word in loglikelihood:\n",
    "            p += loglikelihood[word]\n",
    "    \n",
    "    return p\n",
    "\n",
    "def test_naive_bayes(X_test, y_test, logprior, loglikelihood):\n",
    "    \"\"\"Input:\n",
    "    X_test: a list of tweets\n",
    "    y_test: (m, 1) array with the sentiment label of each tweet (either 0 or 1)\n",
    "    logprior: a number\n",
    "    loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "    accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Let's score the accuracy of our model\n",
    "    accuracy = 0\n",
    "    \n",
    "    # Our predictions will be stored in y_hat\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in X_test:\n",
    "        if predict_naive_bayes(tweet, logprior, loglikelihood) > 0:\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            y_hat.append(0)\n",
    "    \n",
    "    # error is the average of the absolute values of the differences between y_hat and y_test\n",
    "    error = np.mean(np.abs(y_hat - y_test))\n",
    "    \n",
    "    accuracy = 1 - error\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.989500\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %f\" % test_naive_bayes(X_test, y_test, logprior, loglikelihood))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes Classifier for Binary Classification: Movie Review Sentiment Analysis\n",
    "\n",
    "\n",
    "In the above, we precomputed the frequency of each word in our vocabulary. We can, however, implement a Naive Bayes classifier in a different manner. In this example, we will create some classes that will allow us to train a Naive Bayes classifier on the fly. We will use the [IMDB Movie Review Dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) from Kaggle. The dataset contains 50,000 movie reviews from IMDB. The reviews are labeled as positive or negative. The goal is to predict the sentiment of a movie review given the text of the review."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1. Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## html display\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2. Read data and create train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's create a dataframe to store the results\n",
    "df = pd.read_csv('data/IMDB_Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create our train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 10000, 40000, 10000)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the count of our train and test sets\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Tweet: As someone who was born to a German mother and English father (who spent five years in a prisoner of war camp) I come from unique position. One of having to deal with the various Nazis on one side of the family and the victors of WW2 on the other. This miniseries cannot delve into every single part of Hitler's psyche and must give the viewer a general flavor of the situation at the time and as best as one can Hitler's state of mind. In this the series does quite well. Carlyle is very good as is O'Toole, I would however liked to have got more information on the relationships with others in party Because Hitler did not do anything on his own. He had people around him that followed him to the letter often without question and certainly without question later on in his murderous career. What was going through Goebbels, Goring and Hess's mind? It would have been helpful to see more of these relationships. But I hope it will make people research the subject more. It might also make people understand why someone like Saddam Hussein cannot be allowed to continue in power.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "label: positive\n"
     ]
    }
   ],
   "source": [
    "## Randomly select a sample from the training set\n",
    "idx = random.randint(0, len(X_train))\n",
    "\n",
    "# display the sample in HTML format\n",
    "print(display(HTML(f\"<p>Tweet: {X_train.iloc[idx]}</p>\")))\n",
    "print('label:', y_train.iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.positive_word_counts = {}\n",
    "        self.negative_word_counts = {}\n",
    "        self.positive_total_count = 0\n",
    "        self.negative_total_count = 0\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, data):\n",
    "        for text, label in data:\n",
    "            if label == 'positive':\n",
    "                self.positive_total_count += 1\n",
    "                for word in text.split():\n",
    "                    self.positive_word_counts[word] = self.positive_word_counts.get(word, 0) + 1\n",
    "                    self.vocab.add(word)\n",
    "            elif label == 'negative':\n",
    "                self.negative_total_count += 1\n",
    "                for word in text.split():\n",
    "                    self.negative_word_counts[word] = self.negative_word_counts.get(word, 0) + 1\n",
    "                    self.vocab.add(word)\n",
    "\n",
    "    def predict(self, text):\n",
    "        # Calculate the prior probability of each class\n",
    "        positive_prior = self.positive_total_count / (self.positive_total_count + self.negative_total_count + 1e-10)\n",
    "        negative_prior = self.negative_total_count / (self.positive_total_count + self.negative_total_count + 1e-10)\n",
    "\n",
    "        # Calculate the likelihood of the text given each class\n",
    "        positive_likelihood = 0\n",
    "        negative_likelihood = 0\n",
    "        for word in text.split():\n",
    "            if word in self.vocab:\n",
    "                # Add Laplace smoothing to avoid zero probability\n",
    "                positive_likelihood += math.log((self.positive_word_counts.get(word, 0) + 1) / (self.positive_total_count + len(self.vocab) + 1))\n",
    "                negative_likelihood += math.log((self.negative_word_counts.get(word, 0) + 1) / (self.negative_total_count + len(self.vocab) + 1))\n",
    "\n",
    "        # Calculate the posterior probability of each class\n",
    "        positive_posterior = math.exp(positive_likelihood) * positive_prior\n",
    "        negative_posterior = math.exp(negative_likelihood) * negative_prior\n",
    "\n",
    "        # Return the class with the highest posterior probability\n",
    "        if positive_posterior > negative_posterior:\n",
    "            return 'positive'\n",
    "        else:\n",
    "            return 'negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shape our data\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.train(zip(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Tweet: I watch this movie every time it plays on TV. A simply brilliant film. Three men return home from war and try to return to civilian life with great difficulty. All three led opposite lives during the war (Executive Banker became an army corporal, a soda jerk became an Air Force Captain and the High School Football hero loses both his arms in battle)and now each must reconstruct his life and connect with a new reality. The homes they return to, with grown children and independent, working women along with a depressed economy, only add to the strife. It's the scenes just off camera and the unspoken dialog which resonates the most loudly, however. The awkward intimacy of Frederich March and Myrna Loy and his struggle to return to his place as leader (both at home and at work) are heartbreaking.<br /><br />Dana Andrews is riveting as the handsome, decorated Captain who struggles to keep his life together without the uniform.<br /><br />The film is filled with honest characters and each is portrayed by a gifted actor.<br /><br />This film, however, took on a whole other level after seeing, \"Saving Private Ryan.\" The reality and magnitude of what these men lived through for love and country......and obviously it didn't end on the battlefield.<br /><br />This is an essential for any collection.<br /><br /></p> <p>positive</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "prediction: negative\n"
     ]
    }
   ],
   "source": [
    "## Test random sample\n",
    "random_idx = random.randint(0, len(X_test))\n",
    "result = nb.predict(X_test.iloc[random_idx])\n",
    "print(display(HTML(f\"<p>Tweet: {X_test.iloc[random_idx]}</p> <p>{y_test.iloc[random_idx]}</p>\")))\n",
    "print('prediction:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6517\n"
     ]
    }
   ],
   "source": [
    "## Test accuracy\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = len(X_test)\n",
    "\n",
    "for text, label in zip(X_test, y_test):\n",
    "    pred = nb.predict(text)\n",
    "    if pred == label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3. Can we improve our model with vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's create a dataframe to store the results\n",
    "df = pd.read_csv('data/IMDB_Dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's convert the sentiment to labels\n",
    "sentiment = {'positive': 1, 'negative': 0}\n",
    "\n",
    "df['sentiment'] = df['sentiment'].map(sentiment)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 10000, 40000, 10000)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's define our X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using the CountVectorizer class\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "validation_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Train a Naive Bayes classifier using the MultinomialNB class\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_vectors, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "predictions = classifier.predict(validation_vectors)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy:.2f}')  # Output: Accuracy: 0.80"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes Classifier for Multiclass Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Naive Bayes Classifier for Multiclass Classification: Airline review sentiment analysis\n",
    "\n",
    "In this example, we will use the [Twitter US Airline Sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment) dataset from Kaggle. The dataset contains 14,640 tweets from 6 US airlines. The tweets are labeled as positive, negative, or neutral. The goal is to predict the sentiment of a tweet given the text of the tweet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Read our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/Tweets.csv')\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Keep the tweet and label the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  airline_sentiment\n",
       "0                @VirginAmerica What @dhepburn said.                  1\n",
       "1  @VirginAmerica plus you've added commercials t...                  2\n",
       "2  @VirginAmerica I didn't today... Must mean I n...                  1\n",
       "3  @VirginAmerica it's really aggressive to blast...                  0\n",
       "4  @VirginAmerica and it's a really big bad thing...                  0"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "\n",
    "df = df[['text', 'airline_sentiment']]\n",
    "\n",
    "df['airline_sentiment'] = df['airline_sentiment'].map(sentiment)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4. Create our train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11712, 2928, 11712, 2928)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['airline_sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5. Create our text vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on our corpus\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5. Train and evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Train a Naive Bayes classifier\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "nb.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = nb.predict(X_test_vectors)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6. Let's see where our model is making mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create a confusion matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## visualize the confusion matrix\n",
    "cfm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "## plot the confusion matrix\n",
    "sns.heatmap(cfm, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.7. Let's see some of the mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified tweets: 628\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5460</th>\n",
       "      <td>@SouthwestAir it was 3472</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>@SouthwestAir thanks for the quick response. S...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12542</th>\n",
       "      <td>@AmericanAir and btwn gate a8 &amp;amp; a15 I lost...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>@VirginAmerica Just bought tix for ATX - Dalla...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>@JetBlue haha no need to apologize ðŸ˜ I'll be F...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  actual  predicted\n",
       "5460                           @SouthwestAir it was 3472       1          0\n",
       "4590   @SouthwestAir thanks for the quick response. S...       2          0\n",
       "12542  @AmericanAir and btwn gate a8 &amp; a15 I lost...       1          0\n",
       "379    @VirginAmerica Just bought tix for ATX - Dalla...       2          0\n",
       "8121   @JetBlue haha no need to apologize ðŸ˜ I'll be F...       2          0"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output some of the misclassified tweets\n",
    "misclassified = np.where(y_pred != y_test)[0]\n",
    "print('Misclassified tweets:', len(misclassified))\n",
    "\n",
    "## create a dataframe to store the misclassified tweets\n",
    "misclassified_df = pd.DataFrame({'text': X_test.iloc[misclassified], 'actual': y_test.iloc[misclassified], 'predicted': y_pred[misclassified]})\n",
    "misclassified_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes Optimizations with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's create a TF-IDF vectorizer\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      1.00      0.81      1889\n",
      "     neutral       0.80      0.13      0.23       580\n",
      "    positive       0.92      0.12      0.22       459\n",
      "\n",
      "    accuracy                           0.69      2928\n",
      "   macro avg       0.80      0.42      0.42      2928\n",
      "weighted avg       0.74      0.69      0.60      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gold labels = human labels of the input data\n",
    "* predicted labels = labels predicted by the model\n",
    "\n",
    "* True positive (TP) = predicted positive and gold positive\n",
    "* False positive (FP) = predicted positive and gold negative (Type I error)\n",
    "* True negative (TN) = predicted negative and gold negative \n",
    "* False negative (FN) = predicted negative and gold positive (Type II error)\n",
    "\n",
    "* precision = $\\frac{TP}{TP + FP}$\n",
    "* recall = $\\frac{TP}{TP + FN}$\n",
    "* F1 or F-measure = $F\\beta \\frac{(\\beta^2 + 1)\\times precision \\times recall}{\\beta^2 precision + recall}$\n",
    "* accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
