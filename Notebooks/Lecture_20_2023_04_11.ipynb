{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/Notebooks/Lecture_20_2023_04_11.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 20: 2023-04-11 - Recurrent Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Overview\n",
    "\n",
    "* Word2Vec Assignment (description and questions)\n",
    "* Word embeddings and Neural Networks (Masked Language Models)\n",
    "* Recurrent Neural Networks (RNNs)\n",
    "* Long Short-Term Memory (LSTM) Networks\n",
    "* Gated Recurrent Units (GRUs)\n",
    "* Bidirectional Recurrent Neural Networks (BRNNs)\n",
    "* Attention Mechanisms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Assignment\n",
    "\n",
    "* [Description](../assignment_descriptions/08_Word_Embeddings.md)\n",
    "* [Notebook](../assignment_notebooks/Word_Embeddings.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings and Neural Networks\n",
    "\n",
    "* [Notebook](./Lecture_19_2023_04_06.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent neural networks are a class of neural networks that are particularly well suited to processing sequential data such as text. They are able to remember information for a long period of time, and are thus applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence data\n",
    "\n",
    "* Sequence data is data that is ordered in some way. For example, a sequence of words in a sentence, a sequence of characters in a word, a sequence of pixels in an image, a sequence of notes in a song, a sequence of frames in a video, and so on.\n",
    "\n",
    "* Unlike Bag-of-Words models, sequence models can take into account the order of the words in a sentence. This makes them ideal for tasks such as machine translation, speech recognition, and text summarization.\n",
    "\n",
    "* We will follow the standard conventions and model sequence data as follows:\n",
    "\n",
    "$$x^{(i)} = (x_1^{(i)}, x_2^{(i)}, \\ldots, x_T^{(i)})$$\n",
    "\n",
    "Where $T$ is the length of the sequence and $x_t^{(i)}$ is the $t^{th}$ element of the $i^{th}$ sequence in the training set.\n",
    "\n",
    "### Different categories of sequence models\n",
    "\n",
    "* one to one - input layer is a single value (vector or scalar), output layer is a single value (vector or scalar). For example, image classification is a one to one model.\n",
    "* one to many - input layer is a single value (vector or scalar), output layer is a sequence. For example, image captioning is a one to many model.\n",
    "* many to one - input layer is a sequence, output layer is a single value (vector or scalar). For example, sentiment analysis is a many to one model.\n",
    "* many to many - input layer is a sequence, output layer is a sequence. For example, machine translation is a many to many model. Some variants of this model depend on the synchronization of the input and output sequences. For example, in video classification, the input and output sequences are synchronized, whereas in machine translation, the input and output sequences are not synchronized.\n",
    "\n",
    "<center><img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width=\"800\" height=\"300\"></center>\n",
    "\n",
    "N.B.: a rectangle is a vector and arrows are functions. \n",
    "\n",
    "source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.load_ipython_extensions([\n",
    "  \"nb-mermaid/nb-mermaid\"\n",
    "]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Architecture\n",
    "\n",
    "#### Standard feedforward neural network\n",
    "\n",
    "```mermaid\n",
    "graph BT\n",
    "    i[Input] --> h((Hidden Layer))\n",
    "    h --> o[Output]\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network\n",
    "\n",
    "```mermaid\n",
    "\n",
    "graph BT\n",
    "    i[Input] --> h((Hidden State))\n",
    "    h --> h\n",
    "    h --> o[Output]\n",
    "\n",
    "```\n",
    "\n",
    "Recall that in standard neural network data is processed by passing the inputs to the forward layer (or hidden layer) and then to the output layer. In a recurrent neural network, the hidden layer receives the input and the current time step from the previous step. This allows the network to process the data sequentially."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single and Multiple layer RNNs\n",
    "\n",
    "<center><img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_04.png?raw=true\" width=\"800\" height=\"600\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "* The hidden state $h_t$ is the output of the hidden layer at time step $t$. It is also the input to the hidden layer at time step $t+1$.\n",
    "* layer = 1 is represented as $h^{(t)}_{1}$, layer = 2 is represented as $h^{(t)}_{2}$, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations in RNN\n",
    "\n",
    "<center><img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_05.png?raw=true\" width=\"800\" height=\"400\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $W_{xh}$ is the weight matrix for the input to the hidden layer\n",
    "* $W_{hh}$ is the weight matrix for the hidden layer to the hidden layer (recurrent edge)\n",
    "* $W_{ho}$ is the weight matrix for the hidden layer to the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from Rashka, 2020, Deep Learning with PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# our rnn layer\n",
    "rnn_layer = nn.RNN(input_size=5, hidden_size=2, num_layers=1, batch_first=True)\n",
    "\n",
    "# weights\n",
    "w_xh = rnn_layer.weight_ih_l0\n",
    "w_hh = rnn_layer.weight_hh_l0\n",
    "\n",
    "# biases\n",
    "b_xh = rnn_layer.bias_ih_l0\n",
    "b_hh = rnn_layer.bias_hh_l0\n",
    "\n",
    "print('W_xh shape: ', w_xh.shape)\n",
    "print('W_hh shape: ', w_hh.shape)\n",
    "print('b_xh shape: ', b_xh.shape)\n",
    "print('b_hh shape: ', b_hh.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B.: Input shape (batch_size, sequence_length, input_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a forward pass\n",
    "x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n",
    "\n",
    "## output of the RNN layer\n",
    "output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n",
    "print('output shape: ', output.shape)\n",
    "print('hn shape: ', hn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing the RNN layer in comparison to manual computation\n",
    "out = []\n",
    "for t in range(3):\n",
    "    xt = torch.reshape(x_seq[t], (1, 5))\n",
    "    print(f'Time step {t} =>')\n",
    "    print('    Input      :', xt.numpy())\n",
    "    ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_xh\n",
    "    print('    Hidden     :', ht.detach().numpy())\n",
    "    \n",
    "    if t > 0:\n",
    "        prev_h = out[t-1]\n",
    "    else:\n",
    "        prev_h = torch.zeros((ht.shape))\n",
    "    ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) + b_hh\n",
    "    ot = torch.tanh(ot)\n",
    "    out.append(ot)\n",
    "    print('    Output     :', ot.detach().numpy())\n",
    "    print('    RNN output :', output[:, t].detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden state tensor `ht` is computed using the matrix multiplication of the input tensor `xt` and the weight matrix `w_xh` plus the bias term `b_xh`. The `detach()` method is called on `ht` to remove any gradients associated with it, and the resulting tensor is converted to a NumPy array using the `numpy()` method. This hidden state tensor represents the current state of the RNN at time step `t`.\n",
    "\n",
    "If the current time step is greater than 0, the previous hidden state tensor `prev_h` is set to the value of `out[t-1]`. Otherwise, `prev_h` is initialized to a tensor of zeros with the same shape as `ht`.\n",
    "\n",
    "The output tensor `ot` is then computed by adding `ht` to the matrix multiplication of `prev_h` and the weight matrix `w_hh` plus the bias term `b_hh`. The resulting tensor is passed through the `tanh()` activation function and the resulting tensor is stored in `ot`.\n",
    "\n",
    "The current hidden state tensor `ht` is appended to the output list out, and the values of `ot` and the corresponding element of output are printed to the console."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with RNNs\n",
    "\n",
    "* Vanishing gradients\n",
    "* Exploding gradients\n",
    "* Long-term dependencies\n",
    "\n",
    "<center><img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_08.png?raw=true\" height=\"400\" width=\"800\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing gradients\n",
    "\n",
    "As the length of the sequence increases, the gradient of the loss function with respect to the weights of the RNN decreases. This is because the gradient is computed as the product of the gradients of the loss function with respect to the output of the RNN and the gradients of the output of the RNN with respect to the weights of the RNN. As a consequence, the network will take a long time to learn the weights of the RNN.\n",
    "\n",
    "\n",
    "#### Exploding gradients\n",
    "\n",
    "Gradients can explode if the weights of the RNN are increasing. The problem of exploding gradients occurs when the weights in the network are updated using the chain rule during backpropagation. In RNNs, the gradients are propagated through the same set of weights over multiple time steps, which can lead to very large gradients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient clipping\n",
    "\n",
    "To address the issue of exploding gradients in RNNs, several techniques have been proposed. One such technique is gradient clipping, which involves setting a maximum threshold on the gradient to prevent it from growing too large."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long-term dependencies\n",
    "\n",
    "RNNs struggle to learn long-term dependencies. This is because the hidden state of the RNN at time step $t$ is computed using the hidden state of the RNN at time step $t-1$. As a consequence, the hidden state of the RNN at time step $t$ is dependent on the hidden state of the RNN at time step $t-1$, which is in turn dependent on the hidden state of the RNN at time step $t-2$, and so on. This means that the hidden state of the RNN at time step $t$ is dependent on the hidden state of the RNN at all previous time steps. As a consequence, the RNN is unable to learn long-term dependencies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions to the problems with RNNs\n",
    "\n",
    "* Long short-term memory (LSTM)\n",
    "* Gated recurrent unit (GRU)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Long short-term memory (LSTM)\n",
    "\n",
    "The LSTM is a type of RNN that addresses the problems of vanishing gradients, exploding gradients, and long-term dependencies. The LSTM is a type of RNN that has a memory cell that can store information for long periods of time. The LSTM has three gates that control the flow of information into and out of the memory cell. The three gates are the input gate, the forget gate, and the output gate.\n",
    "\n",
    "<center><img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_09.png?raw=true\" height=\"400\" width=\"800\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cell state - the recurrent edge is the memory of the network\n",
    "* input gate (i) - controls the flow of information into the cell state\n",
    "* output gate (o) - updates the values of the hidden state\n",
    "* forget gate (f) - controls the flow of information out of the cell state\n",
    "\n",
    "Good news: the LSTM architecture is super easy to implement in either PyTorch or TensorFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gated recurrent unit (GRU)\n",
    "\n",
    "The GRU is a type of RNN that addresses the problems of vanishing gradients, exploding gradients, and long-term dependencies. The GRU is a type of RNN that has a memory cell that can store information for long periods of time. The GRU has two gates that control the flow of information into and out of the memory cell. The two gates are the update gate and the reset gate.\n",
    "\n",
    "* For further information, see Rashka et al. (2022) and Bansal (2021)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Example in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_text --quiet --exists-action i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB reviews dataset\n",
    "dataset, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train, validate = dataset['train'], dataset['test']\n",
    "\n",
    "# Examine the dataset\n",
    "train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a review\n",
    "for eg, label in train.take(1):\n",
    "  print(\"text: \", eg.numpy())\n",
    "  print(\"label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch the data\n",
    "BUFFER_SIZE = 10_000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# create a dataset of batches - see https://www.tensorflow.org/guide/data_performance#prefetching\n",
    "train_dataset = train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validate_dataset = validate.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eg, label in train_dataset.take(1):\n",
    "  print(\"texts: \", eg.numpy()[:3])\n",
    "  print(\"labels: \", label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our vocabulary size\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Create a text vectorization layer\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the vocabulary\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the encoded text\n",
    "encoder_example = encoder(eg)[:3].numpy()\n",
    "encoder_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the original text to the encoded text\n",
    "for n in range(3):\n",
    "  print(\"Original: \", eg[n].numpy())\n",
    "  print(\"Round-trip: \", \" \".join(vocab[encoder_example[n]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=validate_dataset, validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# validate our model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m val_loss, val_acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(validate_dataset)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest Loss:\u001b[39m\u001b[39m'\u001b[39m, val_loss)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest Accuracy:\u001b[39m\u001b[39m'\u001b[39m, val_acc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# validate our model\n",
    "val_loss, val_acc = model.evaluate(validate_dataset)\n",
    "\n",
    "print('Test Loss:', val_loss)\n",
    "print('Test Accuracy:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "  \n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test our model\n",
    "sample_text = ('The movie was a joke. The animation and the graphics '\n",
    "               'were out of this world, but the acting was horrendous. I would not recommend this movie.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the sentiment\n",
    "prediction = model.predict([sample_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our LSTM model\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_bilstm = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile our bidirectional LSTM model\n",
    "model_bilstm.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our model\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=validate_dataset, validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "  \n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character RNN\n",
    "\n",
    "[Notebook](https://colab.research.google.com/drive/1Et8IO-BCBdSYkhkTcCbo624gqfJD9H7h#scrollTo=cTqhw4K0qIBx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
