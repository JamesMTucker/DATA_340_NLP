{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 16: 2023-30-03 Vector Semantics II (cont.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lecture overview\n",
    "\n",
    "- CBOW and Skip-gram analysis\n",
    "- Using vectors to analyze sentence or document similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW and Skip-gram analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/mikolov.png\" width=\"900\" height=\"500\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Number of iterations to train the model\n",
    "ITERATIONS = ...\n",
    "# Tokenized text size\n",
    "tokenized_text_size = ...\n",
    "# Sliding window size for context words\n",
    "WINDOW_SIZE = ...\n",
    "\n",
    "# Start of the main training loop\n",
    "for iter in tqdm(range(ITERATIONS)):\n",
    "    # Variable to store the cumulative loss per epoch\n",
    "    loss_per_epoch = 0\n",
    "\n",
    "    # Sliding window over the tokenized text\n",
    "    for start in range(tokenized_text_size - WINDOW_SIZE):\n",
    "        # Extracting the window of words as indices\n",
    "        indices = text_as_int[start:start + WINDOW_SIZE]\n",
    "\n",
    "        # GradientTape is used to record the gradients during the forward pass\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Variable to store the combined context vector\n",
    "            combined_context = 0\n",
    "\n",
    "            # Create a context slider by iterating through the indices\n",
    "            for count, index in enumerate(indices):\n",
    "                if count != WINDOW_SIZE // 2:\n",
    "                    # Add the context word's vector to the combined context vector\n",
    "                    combined_context += context_vector_matrix[index, :]\n",
    "\n",
    "            # Normalize the combined context vector by dividing by the number of context words\n",
    "            combined_context /= (WINDOW_SIZE - 1)\n",
    "\n",
    "            # Calculate the dot product between the center vector matrix and the combined context vector\n",
    "            output = tf.matmul(center_vector_matrix, tf.expand_dims(combined_context, 1))\n",
    "\n",
    "            # Compute the softmax output for the center word\n",
    "            softout = tf.nn.softmax(output, axis=0)\n",
    "            # Calculate the loss for the center word\n",
    "            loss = softout[indices[WINDOW_SIZE // 2]]\n",
    "\n",
    "            # Calculate the log loss for the current window\n",
    "            logloss = -tf.math.log(loss)\n",
    "\n",
    "            # Update the cumulative loss for the current epoch\n",
    "            loss_per_epoch += logloss.numpy()\n",
    "\n",
    "            # Calculate the gradients for the context and center vector matrices\n",
    "            grad = tape.gradient(logloss, [context_vector_matrix, center_vector_matrix])\n",
    "\n",
    "            # Update the context and center vector matrices using the calculated gradients\n",
    "            optimizer.apply_gradients(zip(grad, [context_vector_matrix, center_vector_matrix]))\n",
    "\n",
    "    # Append the cumulative loss for the current epoch to the loss list\n",
    "    loss_list.append(loss_per_epoch)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Gradient Tape](https://www.tensorflow.org/api_docs/python/tf/GradientTape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  y = x * x\n",
    "dy_dx = g.gradient(y, x)\n",
    "print(dy_dx)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]], shape=(2, 3), dtype=int32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]], shape=(3, 2), dtype=int32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[ 58  64]\n",
      " [139 154]], shape=(2, 2), dtype=int32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[ 58  64]\n",
      " [139 154]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 2-D tensor\n",
    "a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
    "\n",
    "# 2-D tensor\n",
    "b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
    "\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "d = tf.tensordot(a, b, axes=1)\n",
    "\n",
    "print(a, b, c, d, sep='\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Expand Dim](https://www.tensorflow.org/api_docs/python/tf/expand_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 10, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = tf.zeros([10,10,3])\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 10, 10, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded = tf.expand_dims(image, axis=0)\n",
    "expanded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10, 3), dtype=float32, numpy=\n",
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = tf.nn.softmax([-1, 0., 1.])\n",
    "softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(softmax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram\n",
    "\n",
    "```python\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Number of iterations to train the model\n",
    "ITERATIONS = ...\n",
    "# Tokenized text size\n",
    "tokenize_text_size = ...\n",
    "# Sliding window size for context words\n",
    "WINDOW_SIZE = ...\n",
    "\n",
    "# Start of the main training loop\n",
    "for iter in tqdm(range(ITERATIONS)):\n",
    "    # Variable to store the cumulative loss per epoch\n",
    "    loss_per_epoch = 0\n",
    "\n",
    "    # Sliding window over the tokenized text\n",
    "    for start in range(tokenize_text_size - WINDOW_SIZE):\n",
    "        # Extracting the window of words as indices\n",
    "        indices = text_as_int[start:start + WINDOW_SIZE]\n",
    "\n",
    "        # GradientTape is used to record the gradients during the forward pass\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Initialize the loss for this window to 0\n",
    "            loss = 0\n",
    "\n",
    "            # Extract the center word's vector from the center vector matrix\n",
    "            center_vector = center_vector_matrix[indices[WINDOW_SIZE // 2], :]\n",
    "            # Calculate the dot product between context vector matrix and center vector\n",
    "            output = tf.matmul(context_vector_matrix, tf.expand_dims(center_vector, 1))\n",
    "\n",
    "            # Compute the softmax output for each context word\n",
    "            softmax_output = tf.nn.softmax(output, axis=0)\n",
    "\n",
    "            # Calculate the loss for each context word in the window\n",
    "            for (count, index) in enumerate(indices):\n",
    "                if count != WINDOW_SIZE // 2:\n",
    "                    loss += softmax_output[index]\n",
    "\n",
    "            # Calculate the log loss for the current window\n",
    "            logloss = -tf.math.log(loss)\n",
    "\n",
    "            # Update the cumulative loss for the current epoch\n",
    "            loss_per_epoch += logloss.numpy()\n",
    "\n",
    "            # Calculate the gradients for the context and center vector matrices\n",
    "            grad = tape.gradient(logloss, [context_vector_matrix, center_vector_matrix])\n",
    "\n",
    "            # Update the context and center vector matrices using the calculated gradients\n",
    "            optimizer.apply_gradients(zip(grad, [context_vector_matrix, center_vector_matrix]))\n",
    "\n",
    "    # Append the cumulative loss for the current epoch to the loss list\n",
    "    loss_list.append(loss_per_epoch)\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
