{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Text Normalization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Outline\n",
    "\n",
    "* What is text normalization?\n",
    "    * Tokenization\n",
    "    * Stemming\n",
    "    * Lemmatization\n",
    "    * Stopwords\n",
    "\n",
    "We have used these ideas in the past, but now we will go into more detail. More specifically, we will examine the differences between some of the leading libraries for text normalization.\n",
    "\n",
    "Since we have to represent our text in numbers, we want to get a good idea on what's happening to our corpus as we process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the same text to illustrate the differences among the libraries\n",
    "import pandas as pd\n",
    "\n",
    "text = \"\"\"\n",
    "Human infants have the remarkable ability to learn any human language. One proposed mechanism for this ability \n",
    "is distributional learning, where learners infer the underlying cluster structure from unlabeled input. Computational\n",
    "models of distributional learning have historically been principled but psychologically-implausible\n",
    "computational-level models, or ad hoc but psychologically plausible algorithmic-level models. Approximate rational\n",
    "models like particle filters can potentially bridge this divide, and allow principled, but psychologically plausible\n",
    "models of distributional learning to be specified and evaluated. As a proof of concept, I evaluate one such particle\n",
    "filter model, applied to learning English voicing categories from distributions of voice-onset times (VOTs). \n",
    "I find that this model learns well, but behaves somewhat differently from the standard, unconstrained Gibbs\n",
    "sampler implementation of the underlying rational model.\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Libraries\n",
    "\n",
    "* [NLTK](https://www.nltk.org/)\n",
    "* [spaCy](https://spacy.io/)\n",
    "* [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
    "* [Gensim](https://radimrehurek.com/gensim/)\n",
    "* [Stanford CoreNLP (Stanza)](https://stanfordnlp.github.io/stanza/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep tabs on our packages\n",
    "packages = ['nltk', 'spacy', 'textblob', 'gensim', 'stanza']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Example\n",
    "\n",
    "https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenize the text using the NLTK library\n",
    "\n",
    "# Import the NLTK library\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the text\n",
    "nltk_tokens = word_tokenize(text)\n",
    "\n",
    "## Total tokens in document\n",
    "len(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Return a tokenized copy of *text*,\u001b[0m\n",
      "\u001b[0;34m    using NLTK's recommended word tokenizer\u001b[0m\n",
      "\u001b[0;34m    (currently an improved :class:`.TreebankWordTokenizer`\u001b[0m\n",
      "\u001b[0;34m    along with :class:`.PunktSentenceTokenizer`\u001b[0m\n",
      "\u001b[0;34m    for the specified language).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    :param text: text to split into words\u001b[0m\n",
      "\u001b[0;34m    :type text: str\u001b[0m\n",
      "\u001b[0;34m    :param language: the model name in the Punkt corpus\u001b[0m\n",
      "\u001b[0;34m    :type language: str\u001b[0m\n",
      "\u001b[0;34m    :param preserve_line: A flag to decide whether to sentence tokenize the text or not.\u001b[0m\n",
      "\u001b[0;34m    :type preserve_line: bool\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      /media/james/Projects/GitHub/DATA_340_NLP/Notebooks/venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "## Explore the docs\n",
    "word_tokenize??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Example\n",
    "\n",
    "https://spacy.io/api/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenize the text using the spaCy library\n",
    "\n",
    "# Import the spaCy library\n",
    "import spacy\n",
    "\n",
    "# Create a spaCy object\n",
    "NLP = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenize the text\n",
    "spacy_tokens = [token.text for token in NLP(text)]\n",
    "len(spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'Human',\n",
       " 'infants',\n",
       " 'have',\n",
       " 'the',\n",
       " 'remarkable',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'any',\n",
       " 'human',\n",
       " 'language',\n",
       " '.',\n",
       " 'One',\n",
       " 'proposed',\n",
       " 'mechanism',\n",
       " 'for',\n",
       " 'this',\n",
       " 'ability',\n",
       " '\\n',\n",
       " 'is',\n",
       " 'distributional',\n",
       " 'learning',\n",
       " ',',\n",
       " 'where',\n",
       " 'learners',\n",
       " 'infer',\n",
       " 'the',\n",
       " 'underlying',\n",
       " 'cluster',\n",
       " 'structure',\n",
       " 'from',\n",
       " 'unlabeled',\n",
       " 'input',\n",
       " '.',\n",
       " 'Computational',\n",
       " '\\n',\n",
       " 'models',\n",
       " 'of',\n",
       " 'distributional',\n",
       " 'learning',\n",
       " 'have',\n",
       " 'historically',\n",
       " 'been',\n",
       " 'principled',\n",
       " 'but',\n",
       " 'psychologically',\n",
       " '-',\n",
       " 'implausible',\n",
       " '\\n',\n",
       " 'computational',\n",
       " '-',\n",
       " 'level',\n",
       " 'models',\n",
       " ',',\n",
       " 'or',\n",
       " 'ad',\n",
       " 'hoc',\n",
       " 'but',\n",
       " 'psychologically',\n",
       " 'plausible',\n",
       " 'algorithmic',\n",
       " '-',\n",
       " 'level',\n",
       " 'models',\n",
       " '.',\n",
       " 'Approximate',\n",
       " 'rational',\n",
       " '\\n',\n",
       " 'models',\n",
       " 'like',\n",
       " 'particle',\n",
       " 'filters',\n",
       " 'can',\n",
       " 'potentially',\n",
       " 'bridge',\n",
       " 'this',\n",
       " 'divide',\n",
       " ',',\n",
       " 'and',\n",
       " 'allow',\n",
       " 'principled',\n",
       " ',',\n",
       " 'but',\n",
       " 'psychologically',\n",
       " 'plausible',\n",
       " '\\n',\n",
       " 'models',\n",
       " 'of',\n",
       " 'distributional',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'be',\n",
       " 'specified',\n",
       " 'and',\n",
       " 'evaluated',\n",
       " '.',\n",
       " 'As',\n",
       " 'a',\n",
       " 'proof',\n",
       " 'of',\n",
       " 'concept',\n",
       " ',',\n",
       " 'I',\n",
       " 'evaluate',\n",
       " 'one',\n",
       " 'such',\n",
       " 'particle',\n",
       " '\\n',\n",
       " 'filter',\n",
       " 'model',\n",
       " ',',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'learning',\n",
       " 'English',\n",
       " 'voicing',\n",
       " 'categories',\n",
       " 'from',\n",
       " 'distributions',\n",
       " 'of',\n",
       " 'voice',\n",
       " '-',\n",
       " 'onset',\n",
       " 'times',\n",
       " '(',\n",
       " 'VOTs',\n",
       " ')',\n",
       " '.',\n",
       " '\\n',\n",
       " 'I',\n",
       " 'find',\n",
       " 'that',\n",
       " 'this',\n",
       " 'model',\n",
       " 'learns',\n",
       " 'well',\n",
       " ',',\n",
       " 'but',\n",
       " 'behaves',\n",
       " 'somewhat',\n",
       " 'differently',\n",
       " 'from',\n",
       " 'the',\n",
       " 'standard',\n",
       " ',',\n",
       " 'unconstrained',\n",
       " 'Gibbs',\n",
       " '\\n',\n",
       " 'sampler',\n",
       " 'implementation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'underlying',\n",
       " 'rational',\n",
       " 'model',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of the tokens\n",
    "spacy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m           Tokenizer\n",
      "\u001b[0;31mString form:\u001b[0m    <spacy.tokenizer.Tokenizer object at 0x7fe616a09090>\n",
      "\u001b[0;31mFile:\u001b[0m           /media/james/Projects/GitHub/DATA_340_NLP/Notebooks/venv/lib/python3.10/site-packages/spacy/tokenizer.cpython-310-x86_64-linux-gnu.so\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Tokenizer(Vocab vocab, rules=None, prefix_search=None, suffix_search=None, infix_finditer=None, token_match=None, url_match=None, faster_heuristics=True)\n",
      "Segment text, and create Doc objects with the discovered segment\n",
      "    boundaries.\n",
      "\n",
      "    DOCS: https://spacy.io/api/tokenizer\n",
      "    \n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Create a `Tokenizer`, to create `Doc` objects given unicode text.\n",
      "\n",
      "vocab (Vocab): A storage container for lexical types.\n",
      "rules (dict): Exceptions and special-cases for the tokenizer.\n",
      "prefix_search (callable): A function matching the signature of\n",
      "    `re.compile(string).search` to match prefixes.\n",
      "suffix_search (callable): A function matching the signature of\n",
      "    `re.compile(string).search` to match suffixes.\n",
      "infix_finditer (callable): A function matching the signature of\n",
      "    `re.compile(string).finditer` to find infixes.\n",
      "token_match (callable): A function matching the signature of\n",
      "    `re.compile(string).match`, for matching strings to be\n",
      "    recognized as tokens.\n",
      "url_match (callable): A function matching the signature of\n",
      "    `re.compile(string).match`, for matching strings to be\n",
      "    recognized as urls.\n",
      "faster_heuristics (bool): Whether to restrict the final\n",
      "    Matcher-based pass for rules to those containing affixes or space.\n",
      "    Defaults to True.\n",
      "\n",
      "EXAMPLE:\n",
      "    >>> tokenizer = Tokenizer(nlp.vocab)\n",
      "\n",
      "DOCS: https://spacy.io/api/tokenizer#init\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Tokenize a string.\n",
      "\n",
      "string (str): The string to tokenize.\n",
      "RETURNS (Doc): A container for linguistic annotations.\n",
      "\n",
      "DOCS: https://spacy.io/api/tokenizer#call"
     ]
    }
   ],
   "source": [
    "NLP.tokenizer??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Example\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/_modules/textblob/tokenizers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenize the text using the TextBlob library\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Tokenize the textblob\n",
    "len(blob.words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        TextBlob\n",
      "\u001b[0;31mString form:\u001b[0m\n",
      "Human infants have the remarkable ability to learn any human language. One proposed mechanism for this ability \n",
      "is distributional learning, where learners infer the underlying cluster structure from unlabeled input. Computational\n",
      "models of distributional learning have historically been principled but psychologically-implausible\n",
      "computational-level models, or ad hoc but psychologically plausible algorithmic-level models. Approximate rational\n",
      "models like particle filters can potentially bridge this divide, and allow principled, but psychologically plausible\n",
      "models of distributional learning to be specified and evaluated. As a proof of concept, I evaluate one such particle\n",
      "filter model, applied to learning English voicing categories from distributions of voice-onset times (VOTs). \n",
      "I find that this model learns well, but behaves somewhat differently from the standard, unconstrained Gibbs\n",
      "sampler implementation of the underlying rational model.\n",
      "\u001b[0;31mLength:\u001b[0m      955\n",
      "\u001b[0;31mFile:\u001b[0m        /media/james/Projects/GitHub/DATA_340_NLP/Notebooks/venv/lib/python3.10/site-packages/textblob/blob.py\n",
      "\u001b[0;31mSource:\u001b[0m     \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseBlob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"A general text block, meant for larger bodies of text (esp. those\u001b[0m\n",
      "\u001b[0;34m    containing sentences). Inherits from :class:`BaseBlob <BaseBlob>`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    :param str text: A string.\u001b[0m\n",
      "\u001b[0;34m    :param tokenizer: (optional) A tokenizer instance. If ``None``, defaults to\u001b[0m\n",
      "\u001b[0;34m        :class:`WordTokenizer() <textblob.tokenizers.WordTokenizer>`.\u001b[0m\n",
      "\u001b[0;34m    :param np_extractor: (optional) An NPExtractor instance. If ``None``,\u001b[0m\n",
      "\u001b[0;34m        defaults to :class:`FastNPExtractor() <textblob.en.np_extractors.FastNPExtractor>`.\u001b[0m\n",
      "\u001b[0;34m    :param pos_tagger: (optional) A Tagger instance. If ``None``, defaults to\u001b[0m\n",
      "\u001b[0;34m        :class:`NLTKTagger <textblob.en.taggers.NLTKTagger>`.\u001b[0m\n",
      "\u001b[0;34m    :param analyzer: (optional) A sentiment analyzer. If ``None``, defaults to\u001b[0m\n",
      "\u001b[0;34m        :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.\u001b[0m\n",
      "\u001b[0;34m    :param classifier: (optional) A classifier.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Return list of :class:`Sentence <Sentence>` objects.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_sentence_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Return a list of word tokens. This excludes punctuation characters.\u001b[0m\n",
      "\u001b[0;34m        If you want to include punctuation characters, access the ``tokens``\u001b[0m\n",
      "\u001b[0;34m        property.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        :returns: A :class:`WordList <WordList>` of word tokens.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mWordList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_punc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mraw_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"List of strings, the raw sentences in the blob.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Returns a list of each sentence's dict representation.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m'''Return a json representation (str) of this blob.\u001b[0m\n",
      "\u001b[0;34m        Takes the same arguments as json.dumps.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionadded:: 0.5.1\u001b[0m\n",
      "\u001b[0;34m        '''\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m'''The json representation of this blob.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        .. versionchanged:: 0.5.1\u001b[0m\n",
      "\u001b[0;34m            Made ``json`` a property instead of a method to restore backwards\u001b[0m\n",
      "\u001b[0;34m            compatibility that was broken after version 0.4.0.\u001b[0m\n",
      "\u001b[0;34m        '''\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_create_sentence_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m'''Returns a list of Sentence objects from the raw text.\u001b[0m\n",
      "\u001b[0;34m        '''\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msentence_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mchar_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# Keeps track of character index within the blob\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Compute the start and end indices of the sentence\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# within the blob\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mchar_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mend_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Sentences share the same models as their parent blob\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_extractor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_extractor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mpos_tagger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msentence_objects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0msentence_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "blob??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim Example\n",
    "\n",
    "https://tedboy.github.io/nlps/generated/generated/gensim.utils.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the gensim library\n",
    "import gensim\n",
    "\n",
    "## Tokenize the text using the gensim library\n",
    "gensim_tokens = list(gensim.utils.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gensim_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human',\n",
       " 'infants',\n",
       " 'have',\n",
       " 'the',\n",
       " 'remarkable',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'any',\n",
       " 'human',\n",
       " 'language',\n",
       " 'One',\n",
       " 'proposed',\n",
       " 'mechanism',\n",
       " 'for',\n",
       " 'this',\n",
       " 'ability',\n",
       " 'is',\n",
       " 'distributional',\n",
       " 'learning',\n",
       " 'where',\n",
       " 'learners',\n",
       " 'infer',\n",
       " 'the',\n",
       " 'underlying',\n",
       " 'cluster',\n",
       " 'structure',\n",
       " 'from',\n",
       " 'unlabeled',\n",
       " 'input',\n",
       " 'Computational',\n",
       " 'models',\n",
       " 'of',\n",
       " 'distributional',\n",
       " 'learning',\n",
       " 'have',\n",
       " 'historically',\n",
       " 'been',\n",
       " 'principled',\n",
       " 'but',\n",
       " 'psychologically',\n",
       " 'implausible',\n",
       " 'computational',\n",
       " 'level',\n",
       " 'models',\n",
       " 'or',\n",
       " 'ad',\n",
       " 'hoc',\n",
       " 'but',\n",
       " 'psychologically',\n",
       " 'plausible',\n",
       " 'algorithmic',\n",
       " 'level',\n",
       " 'models',\n",
       " 'Approximate',\n",
       " 'rational',\n",
       " 'models',\n",
       " 'like',\n",
       " 'particle',\n",
       " 'filters',\n",
       " 'can',\n",
       " 'potentially',\n",
       " 'bridge',\n",
       " 'this',\n",
       " 'divide',\n",
       " 'and',\n",
       " 'allow',\n",
       " 'principled',\n",
       " 'but',\n",
       " 'psychologically',\n",
       " 'plausible',\n",
       " 'models',\n",
       " 'of',\n",
       " 'distributional',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'be',\n",
       " 'specified',\n",
       " 'and',\n",
       " 'evaluated',\n",
       " 'As',\n",
       " 'a',\n",
       " 'proof',\n",
       " 'of',\n",
       " 'concept',\n",
       " 'I',\n",
       " 'evaluate',\n",
       " 'one',\n",
       " 'such',\n",
       " 'particle',\n",
       " 'filter',\n",
       " 'model',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'learning',\n",
       " 'English',\n",
       " 'voicing',\n",
       " 'categories',\n",
       " 'from',\n",
       " 'distributions',\n",
       " 'of',\n",
       " 'voice',\n",
       " 'onset',\n",
       " 'times',\n",
       " 'VOTs',\n",
       " 'I',\n",
       " 'find',\n",
       " 'that',\n",
       " 'this',\n",
       " 'model',\n",
       " 'learns',\n",
       " 'well',\n",
       " 'but',\n",
       " 'behaves',\n",
       " 'somewhat',\n",
       " 'differently',\n",
       " 'from',\n",
       " 'the',\n",
       " 'standard',\n",
       " 'unconstrained',\n",
       " 'Gibbs',\n",
       " 'sampler',\n",
       " 'implementation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'underlying',\n",
       " 'rational',\n",
       " 'model']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP Example | Now Stanza\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/installation_usage.html#getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 20.4MB/s]                    \n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "INFO:stanza:Use device: gpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "## Tokenize the text using the Stanford CoreNLP library\n",
    "\n",
    "# Import the Stanford CoreNLP library\n",
    "import stanza\n",
    "\n",
    "# Pipeline for English\n",
    "stan_NLP = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "\n",
    "## Tokenize the text using the stanza library\n",
    "stan_tokens = [token.text for sent in stan_NLP(text).sentences for token in sent.tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stan_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m   \u001b[0mstan_NLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m        Pipeline\n",
      "\u001b[0;31mString form:\u001b[0m <Pipeline: tokenize=TokenizeProcessor(/home/james/stanza_resources/en/tokenize/combined.pt)>\n",
      "\u001b[0;31mFile:\u001b[0m        /media/james/Projects/GitHub/DATA_340_NLP/Notebooks/venv/lib/python3.10/site-packages/stanza/pipeline/core.py\n",
      "\u001b[0;31mSource:\u001b[0m     \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_MODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mpackage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mlogging_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mdownload_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDownloadMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOWNLOAD_RESOURCES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mresources_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_RESOURCES_URL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mresources_branch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mresources_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_RESOURCES_VERSION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdir\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDEFAULT_MODEL_DIR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# set global logging level\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mset_logging_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# processors can use this to save on the effort of loading\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# large sub-models, such as pretrained embeddings, bert, etc\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfoundation_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFoundationCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdownload_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_download_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdownload_method\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDownloadMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOWNLOAD_RESOURCES\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0mdownload_method\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDownloadMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREUSE_RESOURCES\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"resources.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdownload_resources_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                    \u001b[0mresources_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresources_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                    \u001b[0mresources_branch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresources_branch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                    \u001b[0mresources_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresources_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                    \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# process different pipeline parameters\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_pipeline_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Load resources.json to obtain latest packages.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading resource file...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_resources_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;34m'alias'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\"{lang}\" is an alias for \"{resources[lang][\"alias\"]}\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlang_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lang_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'lang_name'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Unsupported language: {lang}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Maintain load list\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenize_pretokenized\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mand\u001b[0m \u001b[0mTOKENIZE\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mand\u001b[0m \u001b[0mMWT\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0madd_mwt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaintain_processor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mdownload_method\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mDownloadMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# skip processors which aren't downloaded from our collection\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdownload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# skip variants\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdownload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_variants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# gather up the model list...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdownload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_processor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# download_models will skip models we already have\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdownload_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mresources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mresources_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresources_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mlog_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No processors to load for language {}.  Please check if your language or package is correctly set.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mload_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Processor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Package'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackage\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_spec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loading these models for language: {lang} ({lang_name}):\\n{load_table}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_default_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Load processors\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# configs that are the same for all processors\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpipeline_level_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'lang'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mode'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use device: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpu\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpu\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# set up processors\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpipeline_reqs_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mprocessor_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprocessor_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcurr_processor_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcurr_processor_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_level_configs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# TODO: this is obviously a hack\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# a better solution overall would be to make a pretagged version of the pos annotator\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# and then subsequent modules can use those tags without knowing where those tags came from\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;34m\"pretagged\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"pretagged\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurr_processor_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mcurr_processor_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pretagged\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pretagged\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'With settings: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_processor_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# try to build processor, throw an exception if there is a requirements issue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNAME_TO_PROCESSOR_CLASS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_processor_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                                                                          \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                                                                          \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mexcept\u001b[0m \u001b[0mProcessorRequirementsException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# if there was a requirements issue, add it to list which will be printed at end\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mpipeline_reqs_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# add the broken processor to the loaded processors for the sake of analyzing the validity of the\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# entire proposed pipeline, but at this point the pipeline will not be built successfully\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merr_processor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# For a FileNotFoundError, we try to guess if there's\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# a missing model directory or file.  If so, we\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# suggest the user try to download the models\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0;34m'model_path'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurr_processor_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_processor_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;31m# model files for this language can't be found in the expected directory\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;32mraise\u001b[0m \u001b[0mLanguageNotDownloadedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mprocessor_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;31m# user asked for a model which doesn't exist for this language?\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;32mraise\u001b[0m \u001b[0mUnsupportedProcessorError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;31m# TODO: before recommending this, check that such a thing exists in resources.json.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;31m# currently that case is handled by ignoring the model, anyway\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Could not find model file %s, although there are other models downloaded for language %s.  Perhaps you need to download a specific model.  Try: stanza.download(lang=\"%s\",package=None,processors={\"%s\":\"%s\"})'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# if we couldn't find a more suitable description of the\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# FileNotFoundError, just raise the old error\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# if there are any processor exceptions, throw an exception to indicate pipeline build failure\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mpipeline_reqs_exceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mPipelineRequirementsException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_reqs_exceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done loading processors!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mupdate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprocessor_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'package'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dependencies'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdependencies\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_spec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_specs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                          \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_specs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessor_list\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpieces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpieces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpieces\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'model_path'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mpackage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m25\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'...'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0moriginal_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_spec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dependencies'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'package'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dependencies'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprocessor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mModelSpecification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'package'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dependencies'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_spec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprocessor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_processors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mprocessor_list\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfilter_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfiltered_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpieces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# split tokenize_pretokenize to tokenize+pretokenize\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpieces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpieces\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mfiltered_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_dict\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mloaded_processors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Return all currently loaded processors in execution order.\u001b[0m\n",
      "\u001b[0;34m        :return: list of Processor instances\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprocessor_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPIPELINE_NAMES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Run the pipeline\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        processors: allow for a list of processors used by this pipeline action\u001b[0m\n",
      "\u001b[0;34m          can be list, tuple, set, or comma separated string\u001b[0m\n",
      "\u001b[0;34m          if None, use all the processors this pipeline knows about\u001b[0m\n",
      "\u001b[0;34m          MWT is added if necessary\u001b[0m\n",
      "\u001b[0;34m          otherwise, no care is taken to make sure prerequisites are followed...\u001b[0m\n",
      "\u001b[0;34m            some of the annotators, such as depparse, will check, but others\u001b[0m\n",
      "\u001b[0;34m            will fail in some unusual manner or just have really bad results\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input should be either str, list or Document'\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# determine whether we are in bulk processing mode for multiple documents\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbulk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# various options to limit the processors used by this pipeline action\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mprocessors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPELINE_NAMES\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot process {} as a list of processors to run\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mTOKENIZE\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessors\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mMWT\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mMWT\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Requested processors for pipeline did not have mwt, but pipeline needs mwt, so mwt is added\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMWT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPIPELINE_NAMES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mprocessor_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbulk_process\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbulk\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocessor_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Assemble the processors in order to make a simple description of the pipeline\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"%s=%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPIPELINE_NAMES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"<Pipeline: %s>\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "stan_NLP??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| NLTK | spaCy | TextBlob | Gensim | Stanza |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| 140 | 158 | 124 | 128 | 148 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "# Display the token counts\n",
    "display(Markdown(\"| NLTK | spaCy | TextBlob | Gensim | Stanza |\\n| --- | --- | --- | --- | --- |\\n| {} | {} | {} | {} | {} |\".format(len(nltk_tokens), len(spacy_tokens), len(blob.words), len(gensim_tokens), len(stan_tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| NLTK | spaCy | TextBlob | Gensim | Stanza |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| ['Human', 'infants', 'have', 'the', 'remarkable', 'ability', 'to', 'learn', 'any', 'human', 'language', '.', 'One', 'proposed', 'mechanism', 'for', 'this', 'ability', 'is', 'distributional', 'learning', ',', 'where', 'learners', 'infer', 'the', 'underlying', 'cluster', 'structure', 'from', 'unlabeled', 'input', '.', 'Computational', 'models', 'of', 'distributional', 'learning', 'have', 'historically', 'been', 'principled', 'but', 'psychologically-implausible', 'computational-level', 'models', ',', 'or', 'ad', 'hoc', 'but', 'psychologically', 'plausible', 'algorithmic-level', 'models', '.', 'Approximate', 'rational', 'models', 'like', 'particle', 'filters', 'can', 'potentially', 'bridge', 'this', 'divide', ',', 'and', 'allow', 'principled', ',', 'but', 'psychologically', 'plausible', 'models', 'of', 'distributional', 'learning', 'to', 'be', 'specified', 'and', 'evaluated', '.', 'As', 'a', 'proof', 'of', 'concept', ',', 'I', 'evaluate', 'one', 'such', 'particle', 'filter', 'model', ',', 'applied', 'to', 'learning', 'English', 'voicing', 'categories', 'from', 'distributions', 'of', 'voice-onset', 'times', '(', 'VOTs', ')', '.', 'I', 'find', 'that', 'this', 'model', 'learns', 'well', ',', 'but', 'behaves', 'somewhat', 'differently', 'from', 'the', 'standard', ',', 'unconstrained', 'Gibbs', 'sampler', 'implementation', 'of', 'the', 'underlying', 'rational', 'model', '.'] | ['\\n', 'Human', 'infants', 'have', 'the', 'remarkable', 'ability', 'to', 'learn', 'any', 'human', 'language', '.', 'One', 'proposed', 'mechanism', 'for', 'this', 'ability', '\\n', 'is', 'distributional', 'learning', ',', 'where', 'learners', 'infer', 'the', 'underlying', 'cluster', 'structure', 'from', 'unlabeled', 'input', '.', 'Computational', '\\n', 'models', 'of', 'distributional', 'learning', 'have', 'historically', 'been', 'principled', 'but', 'psychologically', '-', 'implausible', '\\n', 'computational', '-', 'level', 'models', ',', 'or', 'ad', 'hoc', 'but', 'psychologically', 'plausible', 'algorithmic', '-', 'level', 'models', '.', 'Approximate', 'rational', '\\n', 'models', 'like', 'particle', 'filters', 'can', 'potentially', 'bridge', 'this', 'divide', ',', 'and', 'allow', 'principled', ',', 'but', 'psychologically', 'plausible', '\\n', 'models', 'of', 'distributional', 'learning', 'to', 'be', 'specified', 'and', 'evaluated', '.', 'As', 'a', 'proof', 'of', 'concept', ',', 'I', 'evaluate', 'one', 'such', 'particle', '\\n', 'filter', 'model', ',', 'applied', 'to', 'learning', 'English', 'voicing', 'categories', 'from', 'distributions', 'of', 'voice', '-', 'onset', 'times', '(', 'VOTs', ')', '.', '\\n', 'I', 'find', 'that', 'this', 'model', 'learns', 'well', ',', 'but', 'behaves', 'somewhat', 'differently', 'from', 'the', 'standard', ',', 'unconstrained', 'Gibbs', '\\n', 'sampler', 'implementation', 'of', 'the', 'underlying', 'rational', 'model', '.', '\\n'] | ['Human', 'infants', 'have', 'the', 'remarkable', 'ability', 'to', 'learn', 'any', 'human', 'language', 'One', 'proposed', 'mechanism', 'for', 'this', 'ability', 'is', 'distributional', 'learning', 'where', 'learners', 'infer', 'the', 'underlying', 'cluster', 'structure', 'from', 'unlabeled', 'input', 'Computational', 'models', 'of', 'distributional', 'learning', 'have', 'historically', 'been', 'principled', 'but', 'psychologically-implausible', 'computational-level', 'models', 'or', 'ad', 'hoc', 'but', 'psychologically', 'plausible', 'algorithmic-level', 'models', 'Approximate', 'rational', 'models', 'like', 'particle', 'filters', 'can', 'potentially', 'bridge', 'this', 'divide', 'and', 'allow', 'principled', 'but', 'psychologically', 'plausible', 'models', 'of', 'distributional', 'learning', 'to', 'be', 'specified', 'and', 'evaluated', 'As', 'a', 'proof', 'of', 'concept', 'I', 'evaluate', 'one', 'such', 'particle', 'filter', 'model', 'applied', 'to', 'learning', 'English', 'voicing', 'categories', 'from', 'distributions', 'of', 'voice-onset', 'times', 'VOTs', 'I', 'find', 'that', 'this', 'model', 'learns', 'well', 'but', 'behaves', 'somewhat', 'differently', 'from', 'the', 'standard', 'unconstrained', 'Gibbs', 'sampler', 'implementation', 'of', 'the', 'underlying', 'rational', 'model'] | ['Human', 'infants', 'have', 'the', 'remarkable', 'ability', 'to', 'learn', 'any', 'human', 'language', 'One', 'proposed', 'mechanism', 'for', 'this', 'ability', 'is', 'distributional', 'learning', 'where', 'learners', 'infer', 'the', 'underlying', 'cluster', 'structure', 'from', 'unlabeled', 'input', 'Computational', 'models', 'of', 'distributional', 'learning', 'have', 'historically', 'been', 'principled', 'but', 'psychologically', 'implausible', 'computational', 'level', 'models', 'or', 'ad', 'hoc', 'but', 'psychologically', 'plausible', 'algorithmic', 'level', 'models', 'Approximate', 'rational', 'models', 'like', 'particle', 'filters', 'can', 'potentially', 'bridge', 'this', 'divide', 'and', 'allow', 'principled', 'but', 'psychologically', 'plausible', 'models', 'of', 'distributional', 'learning', 'to', 'be', 'specified', 'and', 'evaluated', 'As', 'a', 'proof', 'of', 'concept', 'I', 'evaluate', 'one', 'such', 'particle', 'filter', 'model', 'applied', 'to', 'learning', 'English', 'voicing', 'categories', 'from', 'distributions', 'of', 'voice', 'onset', 'times', 'VOTs', 'I', 'find', 'that', 'this', 'model', 'learns', 'well', 'but', 'behaves', 'somewhat', 'differently', 'from', 'the', 'standard', 'unconstrained', 'Gibbs', 'sampler', 'implementation', 'of', 'the', 'underlying', 'rational', 'model'] | ['Human', 'infants', 'have', 'the', 'remarkable', 'ability', 'to', 'learn', 'any', 'human', 'language', '.', 'One', 'proposed', 'mechanism', 'for', 'this', 'ability', 'is', 'distributional', 'learning', ',', 'where', 'learners', 'infer', 'the', 'underlying', 'cluster', 'structure', 'from', 'unlabeled', 'input', '.', 'Computational', 'models', 'of', 'distributional', 'learning', 'have', 'historically', 'been', 'principled', 'but', 'psychologically', '-', 'implausible', 'computational', '-', 'level', 'models', ',', 'or', 'ad', 'hoc', 'but', 'psychologically', 'plausible', 'algorithmic', '-', 'level', 'models', '.', 'Approximate', 'rational', 'models', 'like', 'particle', 'filters', 'can', 'potentially', 'bridge', 'this', 'divide', ',', 'and', 'allow', 'principled', ',', 'but', 'psychologically', 'plausible', 'models', 'of', 'distributional', 'learning', 'to', 'be', 'specified', 'and', 'evaluated', '.', 'As', 'a', 'proof', 'of', 'concept', ',', 'I', 'evaluate', 'one', 'such', 'particle', 'filter', 'model', ',', 'applied', 'to', 'learning', 'English', 'voicing', 'categories', 'from', 'distributions', 'of', 'voice', '-', 'onset', 'times', '(', 'VOTs', ')', '.', 'I', 'find', 'that', 'this', 'model', 'learns', 'well', ',', 'but', 'behaves', 'somewhat', 'differently', 'from', 'the', 'standard', ',', 'unconstrained', 'Gibbs', 'sampler', 'implementation', 'of', 'the', 'underlying', 'rational', 'model', '.'] |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the texts\n",
    "display(Markdown(\"| NLTK | spaCy | TextBlob | Gensim | Stanza |\\n| --- | --- | --- | --- | --- |\\n| {} | {} | {} | {} | {} |\".format(nltk_tokens, spacy_tokens, blob.words, gensim_tokens, stan_tokens)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Libraries\n",
    "\n",
    "Stemming is the computational process of reducing inflected (or sometimes derived) words to their word stem, base or root formgenerally a written word form."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Example\n",
    "\n",
    "https://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK Stemming\n",
    "\n",
    "## import the PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "## Create an instance of the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "## Stem the tokens\n",
    "nltk_stemmed_tokens = [stemmer.stem(token) for token in nltk_tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human infant have the remark abil to learn ani human languag . one propos mechan for thi abil is distribut learn , where learner infer the underli cluster structur from unlabel input . comput model of distribut learn have histor been principl but psychologically-implaus computational-level model , or ad hoc but psycholog plausibl algorithmic-level model . approxim ration model like particl filter can potenti bridg thi divid , and allow principl , but psycholog plausibl model of distribut learn to be specifi and evalu . as a proof of concept , i evalu one such particl filter model , appli to learn english voic categori from distribut of voice-onset time ( vot ) . i find that thi model learn well , but behav somewhat differ from the standard , unconstrain gibb sampler implement of the underli ration model .\n"
     ]
    }
   ],
   "source": [
    "## Examine the stemmed tokens\n",
    "print(\" \".join(nltk_stemmed_tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Example\n",
    "\n",
    "SpaCy does not have a built-in stemmer, but it does have a lemmatizer. See below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TextBlob Stemming\n",
    "\n",
    "## Stem the tokens\n",
    "blob = TextBlob(text)\n",
    "\n",
    "blob_stemmed_tokens = [word.stem() for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human infant have the remark abil to learn ani human languag one propos mechan for thi abil is distribut learn where learner infer the underli cluster structur from unlabel input comput model of distribut learn have histor been principl but psychologically-implaus computational-level model or ad hoc but psycholog plausibl algorithmic-level model approxim ration model like particl filter can potenti bridg thi divid and allow principl but psycholog plausibl model of distribut learn to be specifi and evalu as a proof of concept i evalu one such particl filter model appli to learn english voic categori from distribut of voice-onset time vot i find that thi model learn well but behav somewhat differ from the standard unconstrain gibb sampler implement of the underli ration model\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(blob_stemmed_tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genism Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gensim Stemming\n",
    "import gensim\n",
    "from gensim.parsing import stem_text\n",
    "\n",
    "# Stem the tokens\n",
    "gensim_stemmed_tokens = stem_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'human infant have the remark abil to learn ani human language. on propos mechan for thi abil is distribut learning, where learner infer the underli cluster structur from unlabel input. comput model of distribut learn have histor been principl but psychologically-implaus computational-level models, or ad hoc but psycholog plausibl algorithmic-level models. approxim ration model like particl filter can potenti bridg thi divide, and allow principled, but psycholog plausibl model of distribut learn to be specifi and evaluated. as a proof of concept, i evalu on such particl filter model, appli to learn english voic categori from distribut of voice-onset time (vots). i find that thi model learn well, but behav somewhat differ from the standard, unconstrain gibb sampler implement of the underli ration model.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_stemmed_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP Example\n",
    "\n",
    "Stanza does not have a built-in stemmer, but it does have a lemmatizer. See below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/james/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "## NLTK Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Create an instance of the WordNetLemmatizer\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokens\n",
    "nltk_lemmas = [nltk_lemmatizer.lemmatize(token) for token in nltk_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human infant have the remarkable ability to learn any human language . One proposed mechanism for this ability is distributional learning , where learner infer the underlying cluster structure from unlabeled input . Computational model of distributional learning have historically been principled but psychologically-implausible computational-level model , or ad hoc but psychologically plausible algorithmic-level model . Approximate rational model like particle filter can potentially bridge this divide , and allow principled , but psychologically plausible model of distributional learning to be specified and evaluated . As a proof of concept , I evaluate one such particle filter model , applied to learning English voicing category from distribution of voice-onset time ( VOTs ) . I find that this model learns well , but behaves somewhat differently from the standard , unconstrained Gibbs sampler implementation of the underlying rational model .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(nltk_lemmas))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Example\n",
    "\n",
    "https://spacy.io/api/lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SpaCy Lemmatization\n",
    "import spacy\n",
    "\n",
    "## Create an instance of the spaCy library\n",
    "spacy_NLP = spacy.load('en_core_web_sm')\n",
    "\n",
    "## Lemmatize the tokens\n",
    "spacy_lemmas = [token.lemma_ for token in spacy_NLP(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " human infant have the remarkable ability to learn any human language . one propose mechanism for this ability \n",
      " be distributional learning , where learner infer the underlie cluster structure from unlabeled input . computational \n",
      " model of distributional learning have historically be principle but psychologically - implausible \n",
      " computational - level model , or ad hoc but psychologically plausible algorithmic - level model . approximate rational \n",
      " model like particle filter can potentially bridge this divide , and allow principle , but psychologically plausible \n",
      " model of distributional learning to be specify and evaluate . as a proof of concept , I evaluate one such particle \n",
      " filter model , apply to learn english voicing category from distribution of voice - onset time ( VOTs ) . \n",
      " I find that this model learn well , but behave somewhat differently from the standard , unconstrained Gibbs \n",
      " sampler implementation of the underlying rational model . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(spacy_lemmas))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "## Create an instance of the TextBlob library\n",
    "blob = TextBlob(text)\n",
    "\n",
    "## Lemmatize the blob\n",
    "blob_lemmas = [word.lemmatize() for word in blob.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human infant have the remarkable ability to learn any human language One proposed mechanism for this ability is distributional learning where learner infer the underlying cluster structure from unlabeled input Computational model of distributional learning have historically been principled but psychologically-implausible computational-level model or ad hoc but psychologically plausible algorithmic-level model Approximate rational model like particle filter can potentially bridge this divide and allow principled but psychologically plausible model of distributional learning to be specified and evaluated As a proof of concept I evaluate one such particle filter model applied to learning English voicing category from distribution of voice-onset time VOTs I find that this model learns well but behaves somewhat differently from the standard unconstrained Gibbs sampler implementation of the underlying rational model\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(blob_lemmas))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genism Example\n",
    "\n",
    "\n",
    "Gensim no longer hosts a lemmatizer. They used to port the code in from `Pattern` but it has since been removed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP Example\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/lemma.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 20.6MB/s]                    \n",
      "INFO:stanza:Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "INFO:stanza:Use device: gpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: lemma\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Human \tlemma: human\n",
      "word: infants \tlemma: infant\n",
      "word: have \tlemma: have\n",
      "word: the \tlemma: the\n",
      "word: remarkable \tlemma: remarkable\n",
      "word: ability \tlemma: ability\n",
      "word: to \tlemma: to\n",
      "word: learn \tlemma: learn\n",
      "word: any \tlemma: any\n",
      "word: human \tlemma: human\n",
      "word: language \tlemma: language\n",
      "word: . \tlemma: .\n",
      "word: One \tlemma: one\n",
      "word: proposed \tlemma: propose\n",
      "word: mechanism \tlemma: mechanism\n",
      "word: for \tlemma: for\n",
      "word: this \tlemma: this\n",
      "word: ability \tlemma: ability\n",
      "word: is \tlemma: be\n",
      "word: distributional \tlemma: distributional\n",
      "word: learning \tlemma: learning\n",
      "word: , \tlemma: ,\n",
      "word: where \tlemma: where\n",
      "word: learners \tlemma: learner\n",
      "word: infer \tlemma: infer\n",
      "word: the \tlemma: the\n",
      "word: underlying \tlemma: underlying\n",
      "word: cluster \tlemma: cluster\n",
      "word: structure \tlemma: structure\n",
      "word: from \tlemma: from\n",
      "word: unlabeled \tlemma: unlabel\n",
      "word: input \tlemma: input\n",
      "word: . \tlemma: .\n",
      "word: Computational \tlemma: Computational\n",
      "word: models \tlemma: model\n",
      "word: of \tlemma: of\n",
      "word: distributional \tlemma: distributional\n",
      "word: learning \tlemma: learning\n",
      "word: have \tlemma: have\n",
      "word: historically \tlemma: historically\n",
      "word: been \tlemma: be\n",
      "word: principled \tlemma: principle\n",
      "word: but \tlemma: but\n",
      "word: psychologically \tlemma: psychologically\n",
      "word: - \tlemma: -\n",
      "word: implausible \tlemma: implausible\n",
      "word: computational \tlemma: computational\n",
      "word: - \tlemma: -\n",
      "word: level \tlemma: level\n",
      "word: models \tlemma: model\n",
      "word: , \tlemma: ,\n",
      "word: or \tlemma: or\n",
      "word: ad \tlemma: ad\n",
      "word: hoc \tlemma: have\n",
      "word: but \tlemma: but\n",
      "word: psychologically \tlemma: psychologically\n",
      "word: plausible \tlemma: plausible\n",
      "word: algorithmic \tlemma: algorithmic\n",
      "word: - \tlemma: -\n",
      "word: level \tlemma: level\n",
      "word: models \tlemma: model\n",
      "word: . \tlemma: .\n",
      "word: Approximate \tlemma: approximate\n",
      "word: rational \tlemma: rational\n",
      "word: models \tlemma: model\n",
      "word: like \tlemma: like\n",
      "word: particle \tlemma: particle\n",
      "word: filters \tlemma: filter\n",
      "word: can \tlemma: can\n",
      "word: potentially \tlemma: potentially\n",
      "word: bridge \tlemma: bridge\n",
      "word: this \tlemma: this\n",
      "word: divide \tlemma: divide\n",
      "word: , \tlemma: ,\n",
      "word: and \tlemma: and\n",
      "word: allow \tlemma: allow\n",
      "word: principled \tlemma: principle\n",
      "word: , \tlemma: ,\n",
      "word: but \tlemma: but\n",
      "word: psychologically \tlemma: psychologically\n",
      "word: plausible \tlemma: plausible\n",
      "word: models \tlemma: model\n",
      "word: of \tlemma: of\n",
      "word: distributional \tlemma: distributional\n",
      "word: learning \tlemma: learning\n",
      "word: to \tlemma: to\n",
      "word: be \tlemma: be\n",
      "word: specified \tlemma: specify\n",
      "word: and \tlemma: and\n",
      "word: evaluated \tlemma: evaluate\n",
      "word: . \tlemma: .\n",
      "word: As \tlemma: as\n",
      "word: a \tlemma: a\n",
      "word: proof \tlemma: proof\n",
      "word: of \tlemma: of\n",
      "word: concept \tlemma: concept\n",
      "word: , \tlemma: ,\n",
      "word: I \tlemma: I\n",
      "word: evaluate \tlemma: evaluate\n",
      "word: one \tlemma: one\n",
      "word: such \tlemma: such\n",
      "word: particle \tlemma: particle\n",
      "word: filter \tlemma: filter\n",
      "word: model \tlemma: model\n",
      "word: , \tlemma: ,\n",
      "word: applied \tlemma: apply\n",
      "word: to \tlemma: to\n",
      "word: learning \tlemma: learning\n",
      "word: English \tlemma: English\n",
      "word: voicing \tlemma: voice\n",
      "word: categories \tlemma: category\n",
      "word: from \tlemma: from\n",
      "word: distributions \tlemma: distribution\n",
      "word: of \tlemma: of\n",
      "word: voice \tlemma: voice\n",
      "word: - \tlemma: -\n",
      "word: onset \tlemma: onset\n",
      "word: times \tlemma: time\n",
      "word: ( \tlemma: (\n",
      "word: VOTs \tlemma: vot\n",
      "word: ) \tlemma: )\n",
      "word: . \tlemma: .\n",
      "word: I \tlemma: I\n",
      "word: find \tlemma: find\n",
      "word: that \tlemma: that\n",
      "word: this \tlemma: this\n",
      "word: model \tlemma: model\n",
      "word: learns \tlemma: learn\n",
      "word: well \tlemma: well\n",
      "word: , \tlemma: ,\n",
      "word: but \tlemma: but\n",
      "word: behaves \tlemma: behave\n",
      "word: somewhat \tlemma: somewhat\n",
      "word: differently \tlemma: differently\n",
      "word: from \tlemma: from\n",
      "word: the \tlemma: the\n",
      "word: standard \tlemma: standard\n",
      "word: , \tlemma: ,\n",
      "word: unconstrained \tlemma: unconstrained\n",
      "word: Gibbs \tlemma: Gibbs\n",
      "word: sampler \tlemma: sampler\n",
      "word: implementation \tlemma: implementation\n",
      "word: of \tlemma: of\n",
      "word: the \tlemma: the\n",
      "word: underlying \tlemma: underlying\n",
      "word: rational \tlemma: rational\n",
      "word: model \tlemma: model\n",
      "word: . \tlemma: .\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "## Create an instance of the stanza library\n",
    "stanza_NLP = stanza.Pipeline(lang='en', processors='tokenize,lemma')\n",
    "\n",
    "## Create a document object\n",
    "doc = stanza_NLP(text)\n",
    "\n",
    "## Lemmatize the tokens\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Libraries\n",
    "\n",
    "`Stopwords` are words that are so common that they are not useful for analysis. For example, the word `the` is a stopword. To nomralize our text with stopwords, we remove them from our corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genism Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP Example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d99a3f87c484a74ba405ca572f7f1b4059e93a8c4d7f8027bf5ae12e7919d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
