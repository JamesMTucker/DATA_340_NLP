{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 18: 2023-04-04 Word Embeddings and Sequence Models\n",
    "\n",
    "## Lecture Overview\n",
    "\n",
    "* Word Embeddings (Word2Vec, GLoVe, FastText, and ELMo)\n",
    "* Sequence Models (RNNs, LSTMs, and BiLSTMs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "### Static Word Embeddings\n",
    "\n",
    "1. Lack of contextual representations: Static embeddings assign a single vector representation to each word, which fails to capture the different meanings or senses of a word when used in different contexts.\n",
    "\n",
    "2. Polysemy: Since static embeddings provide a single representation for each word, they cannot differentiate between multiple senses of words (words with multiple meanings). This can lead to misinterpretation and decreased performance in NLP tasks.\n",
    "\n",
    "3. Limited vocabulary and out-of-vocabulary (OOV) words: Static embeddings are generated from a fixed vocabulary. Words not present in the training corpus are treated as out-of-vocabulary words, and the model struggles to provide meaningful representations for them.\n",
    "\n",
    "4. Suboptimal handling of phrases and idiomatic expressions: Static word embeddings often struggle to capture the meanings of phrases or idiomatic expressions, as they are designed to work with individual words rather than multi-word units.\n",
    "\n",
    "5. Static in nature: Once trained, the word embeddings remain static and do not evolve or adapt to new contexts or updates in language use. This can limit their performance in applications that require up-to-date language understanding.\n",
    "\n",
    "6. No explicit morphological information: Static word embeddings do not explicitly account for morphological information, such as prefixes, suffixes, or inflections, which can be important for understanding word meanings.\n",
    "\n",
    "### Dynamic or Contextual Word Embeddings\n",
    "\n",
    "1. ELMo: Embeddings from Language Models\n",
    "2. BERT: Bidirectional Encoder Representations from Transformers\n",
    "3. GPT: Generative Pre-Training\n",
    "\n",
    "\n",
    "## Caveat: Large Language Models BIG-Bench\n",
    "\n",
    "* BIG-Bench: A benchmark for general-purpose language models [REPO](https://github.com/google/BIG-bench)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence data\n",
    "\n",
    "* Sequence data is data that is ordered in some way. For example, a sequence of words in a sentence, a sequence of characters in a word, a sequence of pixels in an image, a sequence of notes in a song, a sequence of frames in a video, and so on.\n",
    "\n",
    "* Unlike Bag-of-Words models, sequence models can take into account the order of the words in a sentence. This makes them ideal for tasks such as machine translation, speech recognition, and text summarization.\n",
    "\n",
    "* We will follow the standard conventions and model sequence data as follows:\n",
    "\n",
    "$$x^{(i)} = (x_1^{(i)}, x_2^{(i)}, \\ldots, x_T^{(i)})$$\n",
    "\n",
    "Where $T$ is the length of the sequence and $x_t^{(i)}$ is the $t^{th}$ element of the $i^{th}$ sequence in the training set.\n",
    "\n",
    "## Different categories of sequence models\n",
    "\n",
    "* one to one - input layer is a single value (vector or scalar), output layer is a single value (vector or scalar). For example, image classification is a one to one model.\n",
    "* one to many - input layer is a single value (vector or scalar), output layer is a sequence. For example, image captioning is a one to many model.\n",
    "* many to one - input layer is a sequence, output layer is a single value (vector or scalar). For example, sentiment analysis is a many to one model.\n",
    "* many to many - input layer is a sequence, output layer is a sequence. For example, machine translation is a many to many model. Some variants of this model depend on the synchronization of the input and output sequences. For example, in video classification, the input and output sequences are synchronized, whereas in machine translation, the input and output sequences are not synchronized.\n",
    "\n",
    "<center><img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width=\"800\" height=\"300\"></center>\n",
    "\n",
    "N.B.: a rectangle is a vector and arrows are functions. \n",
    "\n",
    "source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN Architecture\n",
    "\n",
    "```mermaid\n",
    "\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources (Continued Reading)\n",
    "\n",
    "* [Andrej Karpathy's blog post on word embeddings](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* [A Visual Introduction to Word Embeddings](https://jalammar.github.io/illustrated-word2vec/)\n",
    "* [A Visual Introduction to Machine Learning](https://jalammar.github.io/visual-interactive-guide-basics-machine-learning-algorithms/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d99a3f87c484a74ba405ca572f7f1b4059e93a8c4d7f8027bf5ae12e7919d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
