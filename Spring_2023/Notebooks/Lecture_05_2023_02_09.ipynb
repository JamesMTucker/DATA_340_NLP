{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c768b3d8-aa8e-4915-855d-40d54862f708",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/Notebooks/Lecture_05_2023_02_09.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d97a68-4310-43a1-b088-3fa2030bb842",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lecture 04: Statistics and Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45cfaaf-c78b-48f0-9eb3-8ab09d67a913",
   "metadata": {},
   "source": [
    "In the previous lecture, we examined some general statistical features of words and/or tokens. We observed that there the frequency of terms in document follow a power law distribution. We noticed that the most frequent words are often words that are hardly germane to the ideas of the text. What is more, we often don't think about ideas as associated with one word, but we can create noun phrases or prepositional phrases to communicate our ideas. For example, the phrase \"bacon and eggs\" might mean, in a given context, the entities to which the words are referencing. In a different context, however, the noun phrase, \"bacon and eggs\", could mean the event of breakfast or the items one would prefer to eat at the event of a morning meal. Thus, we need a strategy to assess the co-occurrence of n-terms and whether the co-occurence is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b2b39-bbad-4011-8f32-5433286357ca",
   "metadata": {},
   "source": [
    "## Let's look the LOTR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf504c-bc9a-4864-be6f-79d66a659201",
   "metadata": {},
   "source": [
    "Q: What do you think is most frequently occuring group of two words or tokens in the Lord of the Rings? Do think the answer provides insight into the theme(s) or topic(s) of the story? What do we expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952af4c-873c-407e-b9d0-f42c1ddf9a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lets import some packages and configure our notebook\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916b79a-60e7-4f2b-ac0d-2fe22de95dbe",
   "metadata": {},
   "source": [
    "N.B.: Comment out the below and run the colab block if you are working in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b1da5-b30a-442b-a0e3-da93d10d3efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want the Lord of the Rings text\n",
    "files = Path('./data/').glob('*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31a1e0-396d-456b-ae06-8c3d49a57943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Google colab, uncomment this line of code and run\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/', force_remount=True)\n",
    "# files = Path('gdrive/MyDrive/DATA_340_3_NLP/Datasets').glob('*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839bf5d2-9ffd-4172-9218-03dd416dc205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want the text of the _The Fellowship of the Ring_ so let's extract it from our files\n",
    "fellowship = \"\"\n",
    "\n",
    "for f in files:\n",
    "    # Parse the file name using the os package\n",
    "    base_name = os.path.basename(f)\n",
    "    f_name, _ = os.path.splitext(base_name)\n",
    "    \n",
    "    # We are only concerned with the Fellowship\n",
    "    if not f_name == '01_LOTR_Fellowship':\n",
    "        continue\n",
    "    else:\n",
    "        with open(f, 'r', encoding=\"utf-8\") as FIN:\n",
    "            fellowship = FIN.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d3a98-b44f-4086-b0bc-d6632eaf21dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Let's Tokenize, Lemmatize, and Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05d936-e8ec-4e0c-93ca-580f1e747ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can use NLTK to tokenize and lemmatize our text\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Create instances of the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# For stopwords we will add punctuation\n",
    "punct = list(string.punctuation) + list(string.digits)\n",
    "stop_words = stopwords.words('english') + punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33922e-fb3a-41fb-a6e0-9e6d6f0a4228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty list to append the tokens and not stopwords\n",
    "lemmas = []\n",
    "\n",
    "# Iterate over the text to extract our lemmas\n",
    "def tokenize_lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token in stop_words:\n",
    "            continue\n",
    "        else:\n",
    "            lemmas.append(stemmer.stem(token))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f592c26-208b-415d-8819-76daec0c1fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass our text to the above function so we can then create a bigram dictionary\n",
    "fellowship_token_lemmas = tokenize_lemmatize_text(fellowship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680b404-e196-4848-999b-8fa2d0a4cb73",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fellowship_token_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1401cbc-42a7-48ea-816d-26983c8bdeea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's build a bi-token dictionary\n",
    "bigram_freqs = {}\n",
    "\n",
    "# List comprehension to create a list of bigrams\n",
    "bigrams = [(fellowship_token_lemmas[i], fellowship_token_lemmas[i + 1]) for i in range(len(fellowship_token_lemmas) - 1)]\n",
    "\n",
    "# The bigrams are repeated so we want to count the frequency of terms\n",
    "for bigram in bigrams:\n",
    "    bigram_freqs[bigram] = bigram_freqs.get(bigram, 0) + 1\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ecf2f8-645d-41d4-b915-fa836b83f932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bigrams_sorted = list(sorted(bigram_freqs.items(), key=lambda kv: -kv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed23f57-e425-4489-a6be-a3849504f262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's create a dataframe of the bigrams using pandas\n",
    "import pandas as pd\n",
    "\n",
    "# to create the dataframe we need to use pd.DataFrame and pass it our data and give it some column names\n",
    "df = pd.DataFrame(bigrams_sorted, columns=['bigram', 'freq'])\n",
    "\n",
    "# Let's expand the bigrams to their own columns and keep the index so we can retain the frequencies\n",
    "df[['first_term', 'second_term']] = pd.DataFrame(df['bigram'].tolist(), index=df.index)\n",
    "\n",
    "# And drop the bigram column since we now have the lemmas in their own columns\n",
    "df = df.drop(columns=['bigram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b245e6-e5f8-4e20-b25a-0fab89694cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312617b9-cad8-4ce0-9b94-b05998ef66ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.query(\"first_term == 'frodo'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc1ce8-5959-4e89-b348-e1a63f53f75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_frodo = df.query(\"second_term == 'frodo'\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a5fd2-c19c-45b8-8ded-c4c45c194a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_frodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63c3f3-3554-4889-82cc-0bc283e3e963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sum the frequencies to get the total count\n",
    "x_frodo.freq.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c98b5b-d7b3-4930-86c8-8d43cf1ef2a8",
   "metadata": {},
   "source": [
    "## Shannon's Entropy\n",
    "\n",
    "A lot of what we do with written communication is comparison. We as humans come to understand information and ideas through comparisons. The same is true for Natural Language Processing. We want to compare. An important metric for comparison and discerning similarity between things is Shannon's entropy.\n",
    "\n",
    "$$H(X) := -\\sum_{x\\in{X}} p(x) log p(x)$$\n",
    "\n",
    "We can make this more intuitive by rewriting it to describe surprise:\n",
    "\n",
    "$$\\sum{p(x)} log(\\frac{1}{\\frac{1}{p(x)}})$$\n",
    "\n",
    "As a statement of surprise, we can see that probability and surprise are inversely related."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25f87b-8bb0-449c-b52c-527e9b126dd8",
   "metadata": {},
   "source": [
    "## We can use the scipy module to calculate entropy\n",
    "\n",
    "[Entropy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8844af-d5f5-4de6-9a4b-eb9e8c0c881b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's import the entropy function from scipy\n",
    "from scipy.stats import entropy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01657ee5-0d0b-4ba6-829c-1c8d4c8b738a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Docs for entropy\n",
    "entropy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f97843-d693-4fe9-9fb0-15010d2025b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To understand the scipy code, think of the pk as the probabilities for the surprise of events, as defined above. The bigram (x, 'frodo') has a probability of 488/total bigrams.\n",
    "# To calculate the entropy of the bigram, we can plug in the probabilities for selecting the bigram (x, 'frodo') with the probabilities of not selecting (x, 'frodo'). \n",
    "# This gives us the entropy of the (x, 'frodo') bigram within the text of the Fellowship.\n",
    "\n",
    "base=2\n",
    "pk = np.array([488/76560, 76072/76560])\n",
    "entropy(pk, base=base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1391c-2c9c-43f9-8317-4cb0fb173adc",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* Expected Values, Main Ideas!!! Directed by StatQuest with Josh Starmer, 2021. YouTube, https://www.youtube.com/watch?v=KLs_7b7SKi4.\n",
    "* Entropy (for Data Science) Clearly Explained!!! Directed by StatQuest with Josh Starmer, 2021. YouTube, https://www.youtube.com/watch?v=YtebGVx-Fxw.\n",
    "* Jurafsky and Martin, Chapter 3: [N-Gram Language Models](../course_readings/Jurafsky_Martin_chapter_3_39-65.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89a038-7263-4c66-9fc4-a8fe68ef4b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1626dfd1f59ab4e4380357488b4e73453692d701376229aa2051b38a01b9640"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
