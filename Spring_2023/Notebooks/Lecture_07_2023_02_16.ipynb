{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/Notebooks/Lecture_07_2023_02_16.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 07: Regular Expressions and Webscraping\n",
    "\n",
    "## Lecture Overview\n",
    "\n",
    "* Regular Expressions\n",
    "* Webscraping\n",
    "* API Usage\n",
    "* Dataset Creation\n",
    "* Assignment 2 and 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://imgs.xkcd.com/comics/regular_expressions.png\"  width=\"600\" height=\"400\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions: Taking quasi-structured data and making it structured"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Regular Expressions?\n",
    "\n",
    "Regular expressions are a way to match patterns in text in the data type of a string. It takes some time to get used to reading regexes, but they are very powerful and can be used to extract pertinent information, clean up messy data, and/or format data for downstream analyses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example\n",
    "\n",
    "Let's say we have a string that contains a date in the format of `YYYY-MM-DD`. We want to extract the year, month, and day from this string. We can do this with the following regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "date = \"2020-02-16\"\n",
    "\n",
    "# Extract the year, month, and day from the date string\n",
    "year = re.findall(r\"\\d{4}\", date)[0]\n",
    "month = re.findall(r\"\\d{2}\", date)[1]\n",
    "day = re.findall(r\"\\d{2}\", date)[2]\n",
    "\n",
    "print(year, month, day)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our expression is written relative to the string from which we want to extract. If our data was more complex, we would need to use more complex regexes. For example, if we wanted to extract the year, month, and day from a string that contained multiple dates, we would need to use a regex that looks like this: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_023 = \"\"\"\n",
    "On 2020-02-16, I had a conversation with my friend.\n",
    "She was visiting family in Germany, so I had a hard time understanding the German phone number +49 30 901820.\n",
    "I also had a conversation with my mother on 2020-02-17.\n",
    "\"\"\"\n",
    "\n",
    "# Extract the year, month, and day from the date string\n",
    "year = re.findall(r\"\\d{4}\", conversation_023)\n",
    "month = re.findall(r\"\\d{2}\", conversation_023)\n",
    "day = re.findall(r\"\\d{2}\", conversation_023)\n",
    "\n",
    "print(year, month, day, sep = \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to have some familarity with our data to know how simple or complex our regex needs to be. Regexes can be very complex, as we can see here:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Perl\n",
    "\n",
    "my @matches;\n",
    "\n",
    "say \"Matched!\" if m/\n",
    "    (?(DEFINE)\n",
    "        (?<QUOTE_MARK> ['\"])\n",
    "        (?<NOT_QUOTE_MARK> [^'\"])\n",
    "        (?<QUOTE>)\n",
    "            (\n",
    "                (?<quote>(?&QUOTE_MARK))\n",
    "                (?:\n",
    "                    (?&NOT_QUOTE_MARK)++\n",
    "                    |\n",
    "                    (?&QUOTE_MARK)\n",
    "            )*\n",
    "            \\g{quote}\n",
    "        )\n",
    "        (?{ @{$^R}, = $^N}})\n",
    "    )\n",
    "    (?&QUOTE) (?( @matches = @{$^R} ) )\n",
    "    /x;\n",
    "```\n",
    "\n",
    "Example from Brain D. Foy's [Mastering Regular Expressions](https://www.oreilly.com/library/view/mastering-regular-expressions/0596528124/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Regex syntax\n",
    "\n",
    "Regexes are written in a special syntax that is different from the syntax of Python. The syntax is as follows:\n",
    "\n",
    "* `.` matches any character\n",
    "* `^` matches the beginning of the string\n",
    "* `$` matches the end of the string\n",
    "* `*` matches zero or more of the preceding character\n",
    "* `+` matches one or more of the preceding character\n",
    "* `?` matches zero or one of the preceding character\n",
    "* `{n}` matches exactly n of the preceding character\n",
    "* `{n,}` matches n or more of the preceding character\n",
    "* `{,n}` matches 0 to n of the preceding character\n",
    "* `{m,n}` matches at least m and at most n of the preceding character\n",
    "* `[abc]` matches any character in the brackets\n",
    "* `[^abc]` matches any character not in the brackets\n",
    "* `|` matches either the preceding or the following character\n",
    "* `()` groups the preceding match\n",
    "\n",
    "### Regex functions\n",
    "\n",
    "There are a few functions in Python that can be used to work with regexes. The most common are `re.search()` and `re.findall()`. `re.search()` will return the first match of a regex in a string. `re.findall()` will return all matches of a regex in a string. Let's look at an example of each using some sample data from the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get the HTML from the webpage\n",
    "url = 'https://en.wikipedia.org/wiki/ISBN'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# Use BeautifulSoup to parse the HTML\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Get the text from the HTML\n",
    "text = soup.get_text()\n",
    "\n",
    "# Use regex to find all the ISBNs in the text\n",
    "regex = r'ISBN: [0-9-]+'\n",
    "matches = re.findall(regex, text)\n",
    "print(matches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'a take a look at this in more detail:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An SBN may be converted to an ISBN by prefixing the digit \"0\". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has \"SBN 340 01381 8\", where \"340\" indicates the publisher, \"01381\" is the serial number assigned by the publisher, and \"8\" is the check digit. By prefixing a zero, this can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated. Some publishers, such as Ballantine Books, would sometimes use 12-digit SBNs where the last three digits indicated the price of the book;[12] for example, Woodstock Handmade Houses had a 12-digit Standard Book Number of 345-24223-8-595 (valid SBN: 345-24223-8, ISBN: 0-345-24223-8),[13] and it cost US$5.95.[14]\n",
    "\n",
    "\n",
    "We can examine how this regex is working with the wonderful tool [Regex101](https://regex101.com/). And it's always best practice to consult the [Python documentation](https://docs.python.org/3/library/re.html) for more information about the `re` module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping: legalized data theft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"There is no glory in data.\" Something said to me by my MA thesis advisor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/dss_abegg.png\" height=\"400\", width=\"800\"></img></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the process of web scraping is not illegal, it is often frowned upon by the websites that you are scraping. It is important to be respectful of the websites that you are scraping and to make sure that you are not violating any terms of service. In addition, some websites will block your IP address if they want to prevent you from scraping their data. Web developers and web technologies can also be used to deter you from quick access from their data. For more information, see this [Join TechCrunch article](https://techcrunch.com/2022/04/18/web-scraping-legal-court/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAANzYrguMdHcwNL6AKW_UCTQORBn4CH1Idbtrp6YD-aNUv_Basl5x3ZyIgM54Z5bTbzuImyx5P0biFVkhswFoUQjUTRE-HDCFd_KOSGP1UT15q2N9JxZRYXBhjOkXJ1r7JDRlwm0fM4JVOeeJv6UfeuJGV3LFlKSn-UXMDWfP667p)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Scraping\n",
    "\n",
    "Simple scraping is extracting data from websites that do not use the more advanced web technologies, such as JavaScript, databases, and/or APIs. These pages are often static webpages. We can use simple methods to extract such kind of data.\n",
    "\n",
    "For example, we can use the `pandas` module to read in a table from a website. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
    "# url = \"https://en.wikipedia.org/wiki/Minnesota\"\n",
    "\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "tables[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium Compexity Scraping\n",
    "\n",
    "Medium complexity scraping is extracting data from websites that use hyperlinks to organize their data. These pages are often static webpages generated from Perl or PHP scripts. We will have to use more steps to accomplish our goal to build a dataset.\n",
    "\n",
    "Two really import modules we can use are BeautifulSoup and Selenium. BeautifulSoup is a Python library for pulling data out of HTML and XML files. Selenium is a Python library that allows us to automate web browser interactions. We can use these two modules to scrape data from websites that use hyperlinks to organize their data.\n",
    "\n",
    "* BeautifulSoup documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "* Selenium documentation: https://selenium-python.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a website HTML\n",
    "import requests\n",
    "\n",
    "url = \"https://www.gutenberg.org/ebooks/search/?sort_order=downloads\"\n",
    "\n",
    "gutenberg_most_read = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_most_read\n",
    "\n",
    "\"\"\"\n",
    "<!DOCTYPE html>\\n<!--\\n\\nDON\\'T USE THIS PAGE FOR SCRAPING.\\n\\nSeriously. You\\'ll only get your IP blocked.\\n\\nDownload https://www.gutenberg.org/feeds/catalog.rdf.bz2 instead,\\nwhich contains *all* Project Gutenberg metadata in one RDF/XML file.\\n\\n--><html lang=\"en\">\\n\\n\\n\\n\\n<head>\\n<style>\\n.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's download some books and store the file in our data directory\n",
    "# ! wget https://www.gutenberg.org/cache/epub/feeds/rdf-files.tar.bz2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the data\n",
    "\n",
    "N.B.: The downloaded archive is a little over 1 GB. For the purpose of our course, I have extracted the first e-text from the archive. The file is called `pg1.txt`. The file is in the `data` folder of this repository, and it is in the `Datasets` folder in the gdrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this notebook in Google colab, uncomment this line of code and run\n",
    "# import re\n",
    "# from bs4 import BeautifulSoup as bs4\n",
    "#\n",
    "# from pathlib import Path\n",
    "# from google.colab import drive\n",
    "# \n",
    "# drive.mount('/content/gdrive/', force_remount=True)\n",
    "# file = Path('gdrive/MyDrive/DATA_340_3_NLP/Datasets/pg1.rdf')\n",
    "# with open(file, 'r') as f:\n",
    "#    gutenberg = f.read().decode('utf-8')\n",
    "# gutenberg = bs4(gutenberg, features='lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "gutenberg_file = Path(\"data/pg1.rdf\")\n",
    "\n",
    "# Read file and create a beautiful soup object\n",
    "with open(gutenberg_file, \"rb\") as f:\n",
    "    gutenberg = f.read().decode(\"utf-8\")\n",
    "\n",
    "gutenberg = bs4(gutenberg, features=\"lxml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the title of the book, description, and author\n",
    "title = gutenberg.find(\"dcterms:title\").text\n",
    "description = gutenberg.find(\"dcterms:description\").text\n",
    "author = gutenberg.find(\"pgterms:agent\").text\n",
    "\n",
    "title, description, author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the link to the text\n",
    "links = []\n",
    "\n",
    "file_types = gutenberg.find_all(\"pgterms:file\")\n",
    "for f in file_types:\n",
    "    if f[\"rdf:about\"].endswith(\".txt\"):\n",
    "        links.append(f[\"rdf:about\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our links\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create another beautiful soup object for the text\n",
    "text = bs4(requests.get(links[0]).text, features=\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.body.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data quality check and cleaning\n",
    "\n",
    "It turns out the above is corpus of documents. Each document is headed by a date along with an e-text and number. Thankfully we know regular expressions, so we can use them to clean up the data and extract the e-texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_check = re.findall(r\"\\[Etext #\\d+\\]\", text.body.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Complexity Scraping with Selenium\n",
    "\n",
    "Selenium enables us to automate web browser interactions. We can use Selenium to scrape data from websites that use JavaScript to generate their webpages. If we go the following website, we can see that the data is generated by JavaScript: https://osf.io/preprints/discover?subject=bepress%7CSocial%20and%20Behavioral%20Sciences%7CLinguistics. If we use the requests module to get the HTML of the page, we will not get the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example from preprints archive\n",
    "## use requests to get the html\n",
    "import requests\n",
    "\n",
    "linguistics_preprints = \"https://osf.io/preprints/discover?subject=bepress%7CSocial%20and%20Behavioral%20Sciences%7CLinguistics\"\n",
    "\n",
    "html = requests.get(lingguistics_preprints)\n",
    "\n",
    "if html.status_code == 200:\n",
    "    linguistics_html_text = html.text\n",
    "    \n",
    "with open('preprints_wo_content.html', 'w') as html_out:\n",
    "    html_out.write(linguistics_html_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we get the data from the loaded webpage?\n",
    "\n",
    "In order to get the data from the loaded webpage, we need to use Selenium. Selenium is a Python library that allows us to automate web browser interactions. We can use Selenium to scrape data from websites that use JavaScript to generate their webpages. When we use Selenium, we need to use a web driver. A web driver is a program that allows Selenium to interact with a web browser. We can use the following web drivers:\n",
    "\n",
    "* ChromeDriver\n",
    "* EdgeDriver\n",
    "* SafariDriver\n",
    "* FirefoxDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download the Chrome driver\n",
    "! wget https://chromedriver.storage.googleapis.com/110.0.5481.77/chromedriver_linux64.zip && unzip chromedriver_linux64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Selenium to get the html\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "\n",
    "# Set the path to the Chrome driver\n",
    "driver_path = \"chromedriver\"\n",
    "\n",
    "# Create an instance of the Chrome driver\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "# Get the HTML from the webpage\n",
    "driver.get(linguistics_preprints)\n",
    "time.sleep(2)\n",
    "\n",
    "with open('preprints_w_content.html', 'w') as html_out:\n",
    "    html_out.write(driver.page_source)\n",
    "\n",
    "html_source = driver.page_source\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the beautiful soup object to parse the html and extract all the h4 tags\n",
    "soup = bs4(html_source, \"lxml\")\n",
    "\n",
    "# Use soup to find all the h4 tags\n",
    "articles = soup.find_all(\"h4\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip the text from the h4 tags\n",
    "entries = [a.text.strip() for a in articles]\n",
    "\n",
    "# Strip the hyperlinks from the h4 tags\n",
    "links = [a.find(\"a\") for a in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# our query results for `little is known` in PubMed\n",
    "df = pd.read_csv('data/pubmed_little.csv')\n",
    "df.plot(kind='bar', x='Year', y='Count', figsize=(15, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding our API\n",
    "\n",
    "The National Center for Biotechnology Information [NCBI] documents the PubMed API at: https://www.ncbi.nlm.nih.gov/home/develop/api/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use the API to get varios kinds of data related to each Publication (PMID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "\n",
    "# let's store the base url for the PubMed API\n",
    "pubmed_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id={}\"\n",
    "\n",
    "\n",
    "# Let's get the list of PubMed IDs\n",
    "PMID = 18652081\n",
    "\n",
    "# Format the url\n",
    "our_pmid = pubmed_url.format(PMID)\n",
    "\n",
    "# Let's take a look at the PMID\n",
    "our_pmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the title and abstract\n",
    "response = requests.get(our_pmid)\n",
    "pmid_data = html.fromstring(response.content)\n",
    "\n",
    "pmid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to do some extra work to get the data we want\n",
    "title = \" \".join([title.text for title in pmid_data.xpath(\"//articletitle\")])\n",
    "abstract = \"||||\".join([line.text for line in pmid_data.xpath(\"//abstract/abstracttext\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title, abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Grimmer et al.](./course_readings/Grimmer-Grimmer_48-62.pdf):\n",
    "\n",
    "* Principle 1: Data should be collected in a way that is consistent with the research question.\n",
    "    * Usefulness depends on the question to be answered\n",
    "    * Revision: usefulness depends on the creativity of the researcher\n",
    "* Principle 2: Data should be collected in a way that is consistent with the research design.\n",
    "    * No values free corpus construction\n",
    "    * This is in my opinion the most important principle\n",
    "* Principle 3: Data should be collected in a way that is consistent with the research context.\n",
    "    * There is no `right` way to represent a text but there are considerations\n",
    "    * Follow the insights and conventions of the statstical community\n",
    "* Principle 4: Data should be collected in a way that is consistent with the research ethics.\n",
    "    * Privacy, consent, and anonymity\n",
    "    * Data equally representative of the population - sampling\n",
    "    * Conclusions should be validated by measurements of science and not by the researcher's intuition\n",
    "* Principle 5: Code and results should be reproducable (the curious case of _Arming America_ by Michael Bellesiles; won Bancroft Prize. His thesis that the second ammendment was not about personal ownership of guns, as evidenced by probate records. He was later found to have fabricated his data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff598827a98688517ffbe680bec5493f272109220e95a133fdc272280f9c7acf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
