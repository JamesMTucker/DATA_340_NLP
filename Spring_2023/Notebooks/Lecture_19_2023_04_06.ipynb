{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 19: 2023-04-06 Vector Semantics and Word Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Overview\n",
    "\n",
    "* Recap on Word2Vec, GloVe, and FastText\n",
    "* ELMo and BERT Embeddings\n",
    "* NLP Applications of Word Embeddings\n",
    "* Word Embeddings as inputs to Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap on Word2Vec, GloVe, and FastText\n",
    "\n",
    "How can we represent words as vectors? We can use one-hot vectors, but they are very sparse and do not capture any semantic information. We can also use dense vectors, but they are not sparse and do not capture any syntactic information. We have to engineer a method that captures both semantic and syntactic information.\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "<img src=\"./images/mikolov.png\" width=\"900\" height=\"500\" />\n",
    "\n",
    "### GloVe (Global Vectors for Word Representation)\n",
    "\n",
    "<img src=\"./images/glove-2.png\" width=\"900\" height=\"200\" />\n",
    "\n",
    "### FastText\n",
    "\n",
    "<img src=\"./images/fasttext.png\" width=\"900\" height=\"500\" />\n",
    "\n",
    "___\n",
    "\n",
    "references:\n",
    "* [Mikolov et al. (2013)](https://arxiv.org/abs/1301.3781)\n",
    "* [Pennington et al. (2014)](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "* [Joulin et al. (2016)](https://arxiv.org/abs/1607.04606)\n",
    "* [Bojanowski et al. (2017)](https://arxiv.org/abs/1707.04651)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec, GloVe, and FastText in Python\n",
    "\n",
    "The following libraries can be used to train word embeddings in Python:\n",
    "\n",
    "* [gensim](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "* [spaCy](https://spacy.io/usage/vectors-similarity)\n",
    "* [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "* [fastText](https://fasttext.cc/docs/en/python-module.html)\n",
    "\n",
    "\n",
    "#### Word2Vec with gensim\n",
    "\n",
    "```python\n",
    "\n",
    "# from the docs: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")\n",
    "```\n",
    "\n",
    "#### GloVe with Python\n",
    "\n",
    "Download the pretrained GloVe embeddings from [here](https://nlp.stanford.edu/projects/glove/). The following code loads the embeddings into a dictionary.\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "glove_file = 'glove.6B.50d.txt'\n",
    "\n",
    "def load_glove_embeddings(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float32)\n",
    "    return words, word_to_vec_map\n",
    "\n",
    "```\n",
    "\n",
    "#### FastText with Python\n",
    "\n",
    "```python\n",
    "\n",
    "# from the docs: https://fasttext.cc/docs/en/python-module.html\n",
    "\n",
    "import fasttext\n",
    "skgrm_model = fasttext.train_unsupervised('data.txt', model='skipgram')\n",
    "\n",
    "cbow_model = fasttext.train_unsupervised('data.txt', model='cbow')\n",
    "\n",
    "## Text Classification with FastText\n",
    "\n",
    "model = fasttext.train_supervised('train.txt', 'model') # train.txt is path to the training file with one sentence per line with its label\n",
    "\n",
    "model.predict('This is a test sentence', k=3) # returns a tuple (labels, probabilities) with k-labels\n",
    "\n",
    "## predict more than one sentence\n",
    "model.predict(['This is a test sentence', 'This is another test sentence'], k=3)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo and BERT Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec, GloVe, and FastText are static embeddings, meaning that there is a fixed vector representation for each word. ELMo and BERT are contextual embeddings, meaning that the vector representation of a word depends on the context in which it is used. ELMo and BERT are pre-trained on a large corpus of text and can be used to extract features from text data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo (Embeddings from Language Models)\n",
    "\n",
    "<img src=\"./images/elmo.png\" width=\"900\" height=\"300\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper: [Deep contextualized word representations](https://arxiv.org/abs/1802.05365)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT (Bidirectional Encoder Representations from Transformers)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). ELMo representations are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. This allows the model to learn richly contextualized representations of words by conditioning on both left and right context. BERT representations can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/bert-1.png\" height=\"400\" width=\"800\" /></center>\n",
    "\n",
    "\n",
    "BERT is trained on two tasks: (1) Masked Language Modeling (MLM) and (2) Next Sentence Prediction (NSP). The MLM task is a fill-in-the-blank task where the model is trained to predict the masked words in a sentence. The NSP task is a binary classification task where the model is trained to predict if the second sentence in a pair of sentences is the next sentence in a text. The model is trained on a large corpus of text, such as Wikipedia, and is pre-trained for 1 million steps. The model is then fine-tuned on a specific task, such as text classification, and is trained for 3,000 steps. The model is then used to extract features from text data.\n",
    "\n",
    "<center><img src=\"./images/bert.png\" height=\"400\" width=\"800\" /></center>\n",
    "\n",
    "\n",
    "code: https://github.com/google-research/bert <br />\n",
    "paper: [Delvin el al. (2018)](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Applications of Word Embeddings\n",
    "\n",
    "We can use word embeddings to solve a variety of NLP tasks, such as:\n",
    "\n",
    "* Text Classification\n",
    "* Text Summarization\n",
    "* Question Answering\n",
    "* Sentiment Analysis\n",
    "* Machine Translation\n",
    "* Text Generation\n",
    "* Text Similarity\n",
    "* Text Clustering\n",
    "* Recommendation Systems\n",
    "* Chatbots\n",
    "* Information Retrieval\n",
    "* and more..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing our understanding of Neural Networks - Gradient Descent\n",
    "\n",
    "\n",
    "<img src=\"./images/Neuron.drawio.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Autograd\n",
    "\n",
    "* Computes the gradients in the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with 5 elements\n",
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5.], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## require_grad=True\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## operations on the tensor\n",
    "\n",
    "# Computational Graph is created in the forward pass\n",
    "y = x**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Computes the gradients w.r.t. the parameters of the model\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  4.,  9., 16., 25.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y #grad_fn=<PowBackward0> is the gradient function (for backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 2*y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3., 10., 21., 36., 55.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.mean() #mean of all the elements in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dz/dx\n",
    "\n",
    "z.mean().backward() #scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.8000, 2.6000, 3.4000, 4.2000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad #gradient of z with respect to x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector jacobian product or chain rule\n",
    "\n",
    "<center><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e343f872b676a0e64646f27593d03c77c53cbaf3\" height=\"400\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  4.,  9., 16., 25.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32, requires_grad=True)\n",
    "y = x**2\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jacobian matrix\n",
    "j = torch.tensor([0.1, 1.0, 0.001, 0.0001, 0.00001], dtype=torch.float32)\n",
    "x.backward(j) #jacobian matrix - if not scalar value then we must pass a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0000e-01, 5.0000e+00, 7.0000e-03, 9.0000e-04, 1.1000e-04])\n"
     ]
    }
   ],
   "source": [
    "## dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  4.,  9., 16., 25.])\n"
     ]
    }
   ],
   "source": [
    "## to suspend tracking of gradients\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x**2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Other options to stop tracking gradients\n",
    "\n",
    "x.requires_grad_(False)\n",
    "x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "## Training example\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3): # gradients are accumulated\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    \n",
    "    # empty the gradients\n",
    "    # weights.grad.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "\n",
    "* Chain rule = product of partial derivatives\n",
    "\n",
    "x = input\n",
    "y = f(x)\n",
    "z = b(y)\n",
    "\n",
    "\n",
    "We can use the chain rule to compute the derivative of z with respect to y, and then the derivative of y with respect to x.\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch creates the computational graph automatically, and then uses the chain rule to compute the gradients of the loss with respect to the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### Calculating the gradients manually\n",
    "\n",
    "# 1. forward pass\n",
    "# 2. compute gradients\n",
    "# 3. Backward pass (d loss / d weights)\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass and compute the loss\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2 # linear regression loss\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## backward pass\n",
    "loss.backward()\n",
    "w.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Numpy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "[INFO]: epoch 1: w = 1.200, loss = 30.00000000\n",
      "[INFO]: epoch 3: w = 1.872, loss = 0.76800019\n",
      "[INFO]: epoch 5: w = 1.980, loss = 0.01966083\n",
      "[INFO]: epoch 7: w = 1.997, loss = 0.00050331\n",
      "[INFO]: epoch 9: w = 1.999, loss = 0.00001288\n",
      "[INFO]: epoch 11: w = 2.000, loss = 0.00000033\n",
      "[INFO]: epoch 13: w = 2.000, loss = 0.00000001\n",
      "[INFO]: epoch 15: w = 2.000, loss = 0.00000000\n",
      "[INFO]: epoch 17: w = 2.000, loss = 0.00000000\n",
      "[INFO]: epoch 19: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# linear regression\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = Mean Squared Error (MSE)\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "\n",
    "# gradient = d(loss)/d(w) = 1/N * 2x (xw - y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'[INFO]: epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "    \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A PyTorch Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "[INFO]: epoch 1: w = 0.300, loss = 30.00000000\n",
      "[INFO]: epoch 3: w = 0.772, loss = 15.66018772\n",
      "[INFO]: epoch 5: w = 1.113, loss = 8.17471695\n",
      "[INFO]: epoch 7: w = 1.359, loss = 4.26725292\n",
      "[INFO]: epoch 9: w = 1.537, loss = 2.22753215\n",
      "[INFO]: epoch 11: w = 1.665, loss = 1.16278565\n",
      "[INFO]: epoch 13: w = 1.758, loss = 0.60698116\n",
      "[INFO]: epoch 15: w = 1.825, loss = 0.31684780\n",
      "[INFO]: epoch 17: w = 1.874, loss = 0.16539653\n",
      "[INFO]: epoch 19: w = 1.909, loss = 0.08633806\n",
      "Prediction after training: f(5) = 9.612\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'[INFO]: epoch {epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "* Model Design (input, output size, forward pass)\n",
    "* Create loss and optimizer\n",
    "* Training Loop\n",
    "    * Forward pass - compute prediction\n",
    "    * Backward pass - gradients\n",
    "    * Update weights - optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = -3.665\n",
      "[INFO]: epoch 1: w = -0.411, loss = 53.89147186\n",
      "[INFO]: epoch 3: w = 0.204, loss = 26.07174301\n",
      "[INFO]: epoch 5: w = 0.631, loss = 12.67595291\n",
      "[INFO]: epoch 7: w = 0.928, loss = 6.22484207\n",
      "[INFO]: epoch 9: w = 1.135, loss = 3.11738729\n",
      "[INFO]: epoch 11: w = 1.279, loss = 1.61981273\n",
      "[INFO]: epoch 13: w = 1.380, loss = 0.89736181\n",
      "[INFO]: epoch 15: w = 1.450, loss = 0.54812449\n",
      "[INFO]: epoch 17: w = 1.500, loss = 0.37859368\n",
      "[INFO]: epoch 19: w = 1.535, loss = 0.29560304\n",
      "Prediction after training: f(5) = 8.867\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn #neural networks\n",
    "\n",
    "## linear regression\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape # 4, 1 (n_samples, n_features)\n",
    "\n",
    "test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# class LinearRegression(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super(LinearRegression, self).__init__() # super class constructor\n",
    "#         self.lin = nn.Linear(input_dim, output_dim) # define the linear layer\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.lin(x)\n",
    "    \n",
    "model = nn.Linear(input_size, output_size) # == LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(test).item():.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "loss = nn.MSELoss() # mean squared error loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # stochastic gradient descent\n",
    "\n",
    "# training\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    optimizer.step() # update weights\n",
    "    \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'[INFO]: epoch {epoch + 1}: w = {model.weight.item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(test).item():.3f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings as Input to a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.sparse.Embedding"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working with word embeddings in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "\n",
    "embeddings = nn.Embedding(10, 3) # 10 words, 3 dimensions\n",
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(10, 3, padding_idx=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = nn.Embedding(10, 3, padding_idx=0) # 10 words, 3 dimensions\n",
    "embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The cat is on the mat\",\n",
    "    \"The dog is on the couch\",\n",
    "    \"The bird is in the sky\",\n",
    "    \"The fish is in the water\",\n",
    "]\n",
    "\n",
    "labels = [0, 1, 2, 3]  # corresponding labels for the sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    tokens = [s.lower().split() for s in sentences]\n",
    "    return tokens\n",
    "\n",
    "tokens = tokenize(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(tokens):\n",
    "    vocab = set()\n",
    "    for s in tokens:\n",
    "        vocab.update(s)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = create_vocab(tokens)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bird',\n",
       " 'cat',\n",
       " 'couch',\n",
       " 'dog',\n",
       " 'fish',\n",
       " 'in',\n",
       " 'is',\n",
       " 'mat',\n",
       " 'on',\n",
       " 'sky',\n",
       " 'the',\n",
       " 'water'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our word to index mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "def encode_sentences(tokens, word_to_idx):\n",
    "    encoded_sentences = []\n",
    "    for s in tokens:\n",
    "        encoded_sentences.append([word_to_idx[word] for word in s])\n",
    "    return encoded_sentences\n",
    "\n",
    "encoded_sentences = encode_sentences(tokens, word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(encoded_sentences, seq_length):\n",
    "    padded_sentences = []\n",
    "    for s in encoded_sentences:\n",
    "        if len(s) < seq_length:\n",
    "            padded_sentences.append(s + [0] * (seq_length - len(s)))\n",
    "        else:\n",
    "            padded_sentences.append(s[:seq_length])\n",
    "    return padded_sentences\n",
    "\n",
    "seq_length = 6\n",
    "padded_sentences = pad_sequences(encoded_sentences, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, sentences, labels):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sentences[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "dataset = ToyDataset(padded_sentences, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "embedding_dim = 10\n",
    "hidden_dim = 32\n",
    "output_dim = 4\n",
    "\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4143, Accuracy: 0.2500\n",
      "Epoch 2, Loss: 1.4051, Accuracy: 0.2500\n",
      "Epoch 3, Loss: 1.3979, Accuracy: 0.2500\n",
      "Epoch 4, Loss: 1.3912, Accuracy: 0.2500\n",
      "Epoch 5, Loss: 1.3849, Accuracy: 0.2500\n",
      "Epoch 6, Loss: 1.3796, Accuracy: 0.2500\n",
      "Epoch 7, Loss: 1.3728, Accuracy: 0.2500\n",
      "Epoch 8, Loss: 1.3671, Accuracy: 0.2500\n",
      "Epoch 9, Loss: 1.3615, Accuracy: 0.2500\n",
      "Epoch 10, Loss: 1.3547, Accuracy: 0.2500\n",
      "Epoch 11, Loss: 1.3492, Accuracy: 0.2500\n",
      "Epoch 12, Loss: 1.3432, Accuracy: 0.2500\n",
      "Epoch 13, Loss: 1.3371, Accuracy: 0.2500\n",
      "Epoch 14, Loss: 1.3310, Accuracy: 0.2500\n",
      "Epoch 15, Loss: 1.3248, Accuracy: 0.2500\n",
      "Epoch 16, Loss: 1.3187, Accuracy: 0.2500\n",
      "Epoch 17, Loss: 1.3124, Accuracy: 0.2500\n",
      "Epoch 18, Loss: 1.3061, Accuracy: 0.2500\n",
      "Epoch 19, Loss: 1.2999, Accuracy: 0.2500\n",
      "Epoch 20, Loss: 1.2940, Accuracy: 0.2500\n",
      "Epoch 21, Loss: 1.2872, Accuracy: 0.2500\n",
      "Epoch 22, Loss: 1.2808, Accuracy: 0.2500\n",
      "Epoch 23, Loss: 1.2749, Accuracy: 0.5000\n",
      "Epoch 24, Loss: 1.2682, Accuracy: 0.5000\n",
      "Epoch 25, Loss: 1.2617, Accuracy: 0.5000\n",
      "Epoch 26, Loss: 1.2561, Accuracy: 0.5000\n",
      "Epoch 27, Loss: 1.2492, Accuracy: 0.5000\n",
      "Epoch 28, Loss: 1.2427, Accuracy: 0.5000\n",
      "Epoch 29, Loss: 1.2361, Accuracy: 0.5000\n",
      "Epoch 30, Loss: 1.2290, Accuracy: 0.5000\n",
      "Epoch 31, Loss: 1.2220, Accuracy: 0.5000\n",
      "Epoch 32, Loss: 1.2151, Accuracy: 0.7500\n",
      "Epoch 33, Loss: 1.2081, Accuracy: 0.7500\n",
      "Epoch 34, Loss: 1.2009, Accuracy: 1.0000\n",
      "Epoch 35, Loss: 1.1941, Accuracy: 1.0000\n",
      "Epoch 36, Loss: 1.1864, Accuracy: 1.0000\n",
      "Epoch 37, Loss: 1.1794, Accuracy: 1.0000\n",
      "Epoch 38, Loss: 1.1723, Accuracy: 1.0000\n",
      "Epoch 39, Loss: 1.1645, Accuracy: 1.0000\n",
      "Epoch 40, Loss: 1.1565, Accuracy: 1.0000\n",
      "Epoch 41, Loss: 1.1488, Accuracy: 1.0000\n",
      "Epoch 42, Loss: 1.1409, Accuracy: 1.0000\n",
      "Epoch 43, Loss: 1.1340, Accuracy: 1.0000\n",
      "Epoch 44, Loss: 1.1254, Accuracy: 1.0000\n",
      "Epoch 45, Loss: 1.1175, Accuracy: 1.0000\n",
      "Epoch 46, Loss: 1.1089, Accuracy: 1.0000\n",
      "Epoch 47, Loss: 1.1009, Accuracy: 1.0000\n",
      "Epoch 48, Loss: 1.0926, Accuracy: 1.0000\n",
      "Epoch 49, Loss: 1.0846, Accuracy: 1.0000\n",
      "Epoch 50, Loss: 1.0758, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        sentences, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sentences)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        epoch_accuracy += (predictions == labels).sum().item()\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "    epoch_accuracy /= len(dataset)\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions: tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"The cat is on the mat\",\n",
    "    \"The dog is on the couch\",\n",
    "]\n",
    "\n",
    "test_tokens = tokenize(test_sentences)\n",
    "encoded_test_sentences = encode_sentences(test_tokens, word_to_idx)\n",
    "padded_test_sentences = pad_sequences(encoded_test_sentences, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_sentences_tensor = torch.tensor(padded_test_sentences)\n",
    "    test_outputs = model(test_sentences_tensor)\n",
    "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
    "    print(f'Test predictions: {test_predictions}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_2_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
