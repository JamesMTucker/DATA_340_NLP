{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Text Normalization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Outline\n",
    "\n",
    "* What is text normalization?\n",
    "    * Tokenization\n",
    "    * Stemming\n",
    "    * Lemmatization\n",
    "    * Stopwords\n",
    "\n",
    "We have used these ideas in the past, but now we will go into more detail. More specifically, we will examine the differences between some of the leading libraries for text normalization.\n",
    "\n",
    "Since we have to represent our text in numbers, we want to get a good idea on what's happening to our corpus as we process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the same text to illustrate the differences among the libraries\n",
    "import pandas as pd\n",
    "\n",
    "text = \"\"\"\n",
    "Human infants have the remarkable ability to learn any human language. One proposed mechanism for this ability \n",
    "is distributional learning, where learners infer the underlying cluster structure from unlabeled input. Computational\n",
    "models of distributional learning have historically been principled but psychologically-implausible\n",
    "computational-level models, or ad hoc but psychologically plausible algorithmic-level models. Approximate rational\n",
    "models like particle filters can potentially bridge this divide, and allow principled, but psychologically plausible\n",
    "models of distributional learning to be specified and evaluated. As a proof of concept, I evaluate one such particle\n",
    "filter model, applied to learning English voicing categories from distributions of voice-onset times (VOTs). \n",
    "I find that this model learns well, but behaves somewhat differently from the standard, unconstrained Gibbs\n",
    "sampler implementation of the underlying rational model.\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Libraries\n",
    "\n",
    "* [NLTK](https://www.nltk.org/)\n",
    "* [spaCy](https://spacy.io/)\n",
    "* [TextBlob](https://textblob.readthedocs.io/en/dev/)\n",
    "* [Gensim](https://radimrehurek.com/gensim/)\n",
    "* [Stanford CoreNLP (Stanza)](https://stanfordnlp.github.io/stanza/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep tabs on our packages\n",
    "packages = ['nltk', 'spacy', 'textblob', 'gensim', 'stanza']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Example\n",
    "\n",
    "https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the text using the NLTK library\n",
    "\n",
    "# Import the NLTK library\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the text\n",
    "nltk_tokens = word_tokenize(text)\n",
    "\n",
    "## Total tokens in document\n",
    "len(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore the docs\n",
    "word_tokenize??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Example\n",
    "\n",
    "https://spacy.io/api/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the text using the spaCy library\n",
    "\n",
    "# Import the spaCy library\n",
    "import spacy\n",
    "\n",
    "# Create a spaCy object\n",
    "NLP = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenize the text\n",
    "spacy_tokens = [token.text for token in NLP(text)]\n",
    "len(spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the tokens\n",
    "spacy_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP.tokenizer??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Example\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/_modules/textblob/tokenizers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the text using the TextBlob library\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Tokenize the textblob\n",
    "len(blob.words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim Example\n",
    "\n",
    "https://tedboy.github.io/nlps/generated/generated/gensim.utils.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the gensim library\n",
    "import gensim\n",
    "\n",
    "## Tokenize the text using the gensim library\n",
    "gensim_tokens = list(gensim.utils.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gensim_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP Example | Now Stanza\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/installation_usage.html#getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the text using the Stanford CoreNLP library\n",
    "\n",
    "# Import the Stanford CoreNLP library\n",
    "import stanza\n",
    "\n",
    "# Pipeline for English\n",
    "stan_NLP = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "\n",
    "## Tokenize the text using the stanza library\n",
    "stan_tokens = [token.text for sent in stan_NLP(text).sentences for token in sent.tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stan_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_NLP??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "# Display the token counts\n",
    "display(Markdown(\"| NLTK | spaCy | TextBlob | Gensim | Stanza |\\n| --- | --- | --- | --- | --- |\\n| {} | {} | {} | {} | {} |\".format(len(nltk_tokens), len(spacy_tokens), len(blob.words), len(gensim_tokens), len(stan_tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the texts\n",
    "display(Markdown(\"| NLTK | spaCy | TextBlob | Gensim | Stanza |\\n| --- | --- | --- | --- | --- |\\n| {} | {} | {} | {} | {} |\".format(nltk_tokens, spacy_tokens, blob.words, gensim_tokens, stan_tokens)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Libraries\n",
    "\n",
    "Stemming is the computational process of reducing inflected (or sometimes derived) words to their word stem, base or root formâ€”generally a written word form."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Example\n",
    "\n",
    "https://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK Stemming\n",
    "\n",
    "## import the PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "## Create an instance of the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "## Stem the tokens\n",
    "nltk_stemmed_tokens = [stemmer.stem(token) for token in nltk_tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine the stemmed tokens\n",
    "print(\" \".join(nltk_stemmed_tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Example\n",
    "\n",
    "SpaCy does not have a built-in stemmer, but it does have a lemmatizer. See below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TextBlob Stemming\n",
    "\n",
    "## Stem the tokens\n",
    "blob = TextBlob(text)\n",
    "\n",
    "blob_stemmed_tokens = [word.stem() for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(blob_stemmed_tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genism Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gensim Stemming\n",
    "import gensim\n",
    "from gensim.parsing import stem_text\n",
    "\n",
    "# Stem the tokens\n",
    "gensim_stemmed_tokens = stem_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_stemmed_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP Example\n",
    "\n",
    "Stanza does not have a built-in stemmer, but it does have a lemmatizer. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the stemmed texts\n",
    "display(Markdown(\"| NLTK | TextBlob | Gensim |\\n| --- | --- | --- |\\n| {} | {} | {} |\".format(nltk_stemmed_tokens, blob_stemmed_tokens, gensim_stemmed_tokens)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization Libraries\n",
    "\n",
    "Lemmatization is the linguistic process of reducing the groups of inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "| Case | Masc. | Fem. | Neut. |\n",
    "|------|-------|------|-------|\n",
    "| nominative | der | die | das |\n",
    "| accusative | den | die | das |\n",
    "| dative | dem | der | dem |\n",
    "| genitive | des | der | des |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Example\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/stem/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "## NLTK Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Create an instance of the WordNetLemmatizer\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokens\n",
    "nltk_lemmas = [nltk_lemmatizer.lemmatize(token) for token in nltk_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(nltk_lemmas))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Example\n",
    "\n",
    "https://spacy.io/api/lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SpaCy Lemmatization\n",
    "import spacy\n",
    "\n",
    "## Create an instance of the spaCy library\n",
    "spacy_NLP = spacy.load('en_core_web_sm')\n",
    "\n",
    "## Lemmatize the tokens\n",
    "spacy_lemmas = [token.lemma_ for token in spacy_NLP(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(spacy_lemmas))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "## Create an instance of the TextBlob library\n",
    "blob = TextBlob(text)\n",
    "\n",
    "## Lemmatize the blob\n",
    "blob_lemmas = [word.lemmatize() for word in blob.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(blob_lemmas))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genism Example\n",
    "\n",
    "\n",
    "Gensim no longer hosts a lemmatizer. They used to port the code in from `Pattern` but it has since been removed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP Example\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/lemma.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "## Create an instance of the stanza library\n",
    "stanza_NLP = stanza.Pipeline(lang='en', processors='tokenize,lemma')\n",
    "\n",
    "## Create a document object\n",
    "doc = stanza_NLP(text)\n",
    "\n",
    "# Stanza lemmas\n",
    "stanza_lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "## Lemmatize the tokens\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the lemmatized texts\n",
    "display(Markdown(\"| NLTK | spaCy | TextBlob | Stanza |\\n| --- | --- | --- | --- |\\n| {} | {} | {} | {} |\".format(nltk_lemmas, spacy_lemmas, blob_lemmas, stanza_lemmas)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Libraries\n",
    "\n",
    "`Stopwords` are words that are so common that they are not useful for analysis. For example, the word `the` is a stopword. To nomralize our text with stopwords, we remove them from our corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Example\n",
    "\n",
    "https://www.nltk.org/search.html?q=stopwords&check_keywords=yes&area=default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK Stopwords\n",
    "\n",
    "## Import the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Example\n",
    "\n",
    "https://spacy.io/api/language#section-defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "## Create a list of SpaCy stopwords in English\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "spacy_stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Example\n",
    "\n",
    "Texblob relies on NLTK for stopwords."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genism Example\n",
    "\n",
    "https://radimrehurek.com/gensim/parsing/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "STOPWORDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP Example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d99a3f87c484a74ba405ca572f7f1b4059e93a8c4d7f8027bf5ae12e7919d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
