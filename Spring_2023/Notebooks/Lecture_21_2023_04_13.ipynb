{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 21: 2023-04-13 Transformers\n",
    "\n",
    "## Timeline overview\n",
    "\n",
    "* 2023-04-13: Lecture 21 - Introduction to Transformers\n",
    "* 2023-04-18: Lecture 22 - Attention is all you need\n",
    "* 2023-04-20: Lecture 23 - Applied NLP: Text Generation, Summarization, and Question Answering\n",
    "* 2023-04-25: Lecture 24 - Applied NLP (cont.): Multimodal Models, Production, Cloud Computing\n",
    "* 2023-04-27: Lecture 25 - Presentations (7 Presentations at 10 minutes each)\n",
    "* 2023-05-02: Lecture 26 - Presentations (7 Presentations at 10 minutes each)\n",
    "* 2023-05-04: Lecture 27 - Presentations (7 Presentations at 10 minutes each)\n",
    "\n",
    "## Lecture 21: Introduction to Transformers Overview\n",
    "\n",
    "* Our character RNN trained\n",
    "* Introduction to Transformers\n",
    "* HuggingFace Transformers library\n",
    "* Transformers for NLP\n",
    "* Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our character RNN trained\n",
    "\n",
    "* [Lecture 20](https://colab.research.google.com/drive/1Et8IO-BCBdSYkhkTcCbo624gqfJD9H7h#scrollTo=ACmdKJpkxaU4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Transformers\n",
    "\n",
    "### Milestones in Transformer Models\n",
    "\n",
    "* Vaswani, Ashish, et al. Attention Is All You Need. arXiv:1706.03762, arXiv, 5 Dec. 2017. arXiv.org, https://doi.org/10.48550/arXiv.1706.03762.\n",
    "\n",
    "### Some import models\n",
    "\n",
    "* June 2018: GPT (OpenAI)\n",
    "* October 2018: BERT (Google - summaries of sentences)\n",
    "* February 2019: GPT-2 (OpenAI - not immediately released due to ethical concerns)\n",
    "* October 2019: DistilBERT (Faster and better memory performance than BERT)\n",
    "* October 2019: BART and T5 (large pretrained models)\n",
    "* May 2020: GPT-3 (OpenAI - zero-shot learning)\n",
    "\n",
    "\n",
    "### Key ideas\n",
    "\n",
    "* Pretraining - Input is a very large corpus of text for weeks or months\n",
    "* Fine-tuning - Input is a specific task (e.g. sentiment analysis)\n",
    "* Encoder - Models that are good for understanding the input, like sentence classification or named entity recognition\n",
    "* Decoder - Models that are good for generating output, like text generation or summarization\n",
    "* Attention layers - Model attends to different relationships in different layers [BERT](https://huggingface.co/exbert/?model=bert-base-uncased&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=0&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true)\n",
    "\n",
    "<center><img src=\"https://lenngro.github.io/assets/images/2020-11-07-Attention-Is-All-You-Need/transformer-model-architecture.png\" height=\"600\" width=\"400\"></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Transformers library\n",
    "\n",
    "* [HuggingFace Transformers](https://huggingface.co/transformers/)\n",
    "* [Natural Language Processing Course](https://huggingface.co/course/chapter1/1)\n",
    "\n",
    "<center><img src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" height=\"200\" width=\"200\"></center>\n",
    "\n",
    "### Docs and Tutorials\n",
    "\n",
    "* [Docs](https://huggingface.co/transformers/)\n",
    "* [Tutorials](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "### Installation\n",
    "\n",
    "* `pip install transformers`\n",
    "* `pip install datasets`\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* [Datasets](https://huggingface.co/datasets/)\n",
    "  * Multimodal\n",
    "  * Computer Vision\n",
    "  * NLP\n",
    "  * Audio\n",
    "  * Tabular\n",
    "\n",
    "* NLP Datasets for various tasks\n",
    "  * Text Classification\n",
    "  * Token Classification\n",
    "  * Table Question Answering\n",
    "  * Question Answering\n",
    "  * Zero-Shot Classification\n",
    "  * Translation\n",
    "  * Summarization\n",
    "  * Conversational\n",
    "  * Text Generation\n",
    "  * Text2Text Generation\n",
    "  * Fill Mask\n",
    "  * Sentence similarity\n",
    "  * Table to text\n",
    "  * Multi-choice\n",
    "  * Text retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 28824 datasets available on the HuggingFace Hub\n",
      "The first 10 are: ['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews']\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Datasets https://github.com/huggingface/datasets\n",
    "# !pip install datasets\n",
    "\n",
    "from datasets import list_datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "all_ds = list_datasets()\n",
    "print(f'There are {len(all_ds)} datasets available on the HuggingFace Hub')\n",
    "print(f'The first 10 are: {all_ds[:10]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('We are very happy to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'We are very happy to show you the ðŸ¤— Transformers library.',\n",
       " 'labels': ['technology', 'business', 'sports', 'politics'],\n",
       " 'scores': [0.7958341836929321,\n",
       "  0.08859845995903015,\n",
       "  0.06836599111557007,\n",
       "  0.047201402485370636]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## zero-shot classification\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "classifier('We are very happy to show you the ðŸ¤— Transformers library.', candidate_labels=['politics', 'business', 'sports', 'technology'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/james/Projects/GitHub/DATA_340_NLP/Notebooks/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/media/james/Projects/GitHub/DATA_340_NLP/Notebooks/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Frodo and Sam were walking through the Shire when they came upon this new house, which had the ruins upon it. They went up to the porch, and fell down on the marble floor of it's roof like a statue made of gold\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text generation\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "generator('Frodo and Sam were walking through the Shire when')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 998/998 [00:00<00:00, 1.08MB/s]\n",
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.33G/1.33G [00:36<00:00, 36.2MB/s]\n",
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60.0/60.0 [00:00<00:00, 102kB/s]\n",
      "Downloading (â€¦)solve/main/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213k/213k [00:00<00:00, 3.31MB/s]\n",
      "/media/james/Projects/GitHub/DATA_340_NLP/Notebooks/venv/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9994654,\n",
       "  'word': 'Mary',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9840012,\n",
       "  'word': 'William and Mary',\n",
       "  'start': 32,\n",
       "  'end': 48},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.96413577,\n",
       "  'word': 'Natural Language Processing',\n",
       "  'start': 77,\n",
       "  'end': 104},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9948833,\n",
       "  'word': 'MIT',\n",
       "  'start': 108,\n",
       "  'end': 111}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# named entity recognition\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline('ner', grouped_entities=True)\n",
    "ner('Mary graduates this spring from William and Mary. She will continue to study Natural Language Processing at MIT.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9272308349609375, 'start': 108, 'end': 111, 'answer': 'MIT'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question answering\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline('question-answering')\n",
    "\n",
    "question_answerer(\n",
    "    question='Where does Mary study?',\n",
    "    context='Mary graduates this spring from William and Mary. She will continue to study Natural Language Processing at MIT.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the Transformers library provides state-of-the-art general-purpose architectures for natural language understanding (NLU) and natural language generation (NLG) with over 32+ pretrained models in 100+ languages .'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarization\n",
    "\n",
    "summarizer = pipeline('summarization', max_length=48, min_length=30, do_sample=False, model='t5-base')\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    The Transformers library provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...)\n",
    "    for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and\n",
    "    deep interoperability between TensorFlow 2.0 and PyTorch.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The importance of training data\n",
    "\n",
    "While the above uses are super easy, the real power of Transformers comes from the fact that they can be fine-tuned on a wide variety of tasks with just a few lines of code. This is made possible by the fact that they are pretrained on a large dataset (usually a few hundred million words) and then fine-tuned on a specific task. This is why Transformers are so powerful and why they are so widely used in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein as a solvent in its extraction.\\n\\nIt is not yet known whether the use of the compound for extraction in synthetic foods could significantly influence the safety, antioxidant, and detoxification properties.\\n\\nThis is in addition to the other reported problems with the product introduced in recent years, which have highlighted risks associated with the product and its use in food processing;\\n'},\n",
       " {'generated_text': 'SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein on carbon dioxide, its most important enzyme in creating COVID-1 and COVID-3. COVID-1 and COVID-3 are the primary COVID enzymes, and COVID-2 is the primary COVID enzyme of the respiratory system. COVID-4-1 and COVID-2 are a family of four enzymes that recognize and synthesize COVID'},\n",
       " {'generated_text': 'SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein to attack proteins within and outside of cells:\\n\\nHEX-1 (hEX), an exon that is thought to induce metabolic disturbances by disrupting the lipid (cell-type molecules) interspersed with the DNA (DNA) between the various cell nuclei and thus causing cellular senescence. The HEX protein can be found in mitochondria (cell nuclei'},\n",
       " {'generated_text': 'SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein (KGC) as a potent inhibitor of the endogenous cannabinoid system (reviewed in ). The kGC pathway has been implicated in tumor progression, proliferation/retectin clearance, cell death, and growth inhibition ( ). As of 2013, the KGC pathway and its Rab-1 receptor antagonist KGV1 were only previously available from human adipocytes. We now'},\n",
       " {'generated_text': 'SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein to carry out its action and to produce a high, sustained plasma glucose and insulin response. We therefore used a high-capacity (100% glucose buffer) human plasma insulin and glycoprotein protein in combination to measure how the human brain responds to these drugs. The rate at which the human brain was insulin-dependent and insulin-independent of its glucose, insulin-dependent (see'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model bias - GPT-2 was trained on novels and other story-like texts, so we will get really poor results in specialized domains\n",
    "generator('SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein', num_return_sequences=5, max_length=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical considerations & Subject Matter Experts\n",
    "\n",
    "The above is good example of how large language models kind 'ramble'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "# from transformers import BloomModel, AutoTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
    "           output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contextual embeddings\n",
    "\n",
    "def bert_text_preparation(text, tokenizer):\n",
    "  \"\"\"\n",
    "  Preprocesses text input in a way that BERT can interpret.\n",
    "  \"\"\"\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "  tokenized_text = tokenizer.tokenize(marked_text)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "  # convert inputs to tensors\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "  return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    \"\"\"\n",
    "    Obtains BERT embeddings for tokens, in context of the given sentence.\n",
    "    \"\"\"\n",
    "    # gradient calculation id disabled\n",
    "    with torch.no_grad():\n",
    "      # obtain hidden states\n",
    "      outputs = model(tokens_tensor, segments_tensor)\n",
    "      # print(outputs[0])\n",
    "      hidden_states = outputs[2]\n",
    "\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # where Y is the number of tokens in the sentence\n",
    "\n",
    "    # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # \"token\" is a [12 x 768] tensor\n",
    "\n",
    "        # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    return token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Advancing mHealth-supported Adoption and Sustainment of an Evidence-based Mental Health Intervention for Youth in a School-based Delivery Setting in Sierra Leone',\n",
    "             'Refining and Pilot Testing a Decision Support Intervention to Facilitate Adoption of Evidence-Based Programs to Improve Parent and Child Mental Health',\n",
    "             'Reusable, transparent, and reconfigurable N95-equivalent Respirator Masks: design, fabrication, and trials for enhanced adoption',\n",
    "             'Understanding the Adoption and Impact of New Risk Assessment Technologies in Prostate Cancer Care',\n",
    "             'Addressing adoption barriers to patient transportation services',\n",
    "             'The College Alcohol Intervention Matrix (College AIM): Adoption and Implementation Across College Campuses',\n",
    "             'Social Networks of Diffusion and Adoption: Investigating the Network Effects on implementation of evidence-based interventions for early intervention providers of children',\n",
    "             'HPV ECHO: Increasing the adoption of evidence-based communication strategies for HPV vaccination in rural primary care practices',\n",
    "             'Understanding disparities in the adoption and use of assistive technology by older Hispanics',\n",
    "             'Adoption and Implementation of an Evidence-based Safe Driving Program for High-Risk Teen Drivers',\n",
    "             'Motion Sequencing for All: pipelining, distribution and training to enable broad adoption of a next-generation platform for behavioral and neurobehavioral analysis',\n",
    "             \"The Implementation, Adoption, and Sustainability of Ho'ouna Pono\",\n",
    "             \"The Challenges and Benefits of Adopting Teens: A Comparative Study\",\n",
    "             \"Navigating the Unique Needs of Adolescent Adoption\",\n",
    "             \"The Impact of Timing on Adoption Outcomes: Examining Infant and Teen Adoption\",\n",
    "             \"Supporting the Transition to Adulthood in Adopted Teens\",\n",
    "             \"Exploring the Long-Term Effects of Adopting Teens versus Infants\",\n",
    "             \"Adopting Teens: A Systematic Review of the Literature\",\n",
    "             \"Addressing the Stereotypes and Realities of Adopting Teens\",\n",
    "             \"Comparing the Parenting Experiences of Adopting Infants and Teens\"\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
    "  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "\n",
    "  # make ordered dictionary to keep track of the position of each word\n",
    "  tokens = OrderedDict()\n",
    "\n",
    "  # loop over tokens in sensitive sentence\n",
    "  for token in tokenized_text[1:-1]:\n",
    "    # keep track of position of word and whether it occurs multiple times\n",
    "    if token in tokens:\n",
    "      tokens[token] += 1\n",
    "    else:\n",
    "      tokens[token] = 1\n",
    "\n",
    "    # compute the position of the current token\n",
    "    token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "    current_index = token_indices[tokens[token]-1]\n",
    "\n",
    "    # get the corresponding embedding\n",
    "    token_vec = list_token_embeddings[current_index]\n",
    "    \n",
    "    # save values\n",
    "    context_tokens.append(token)\n",
    "    context_embeddings.append(token_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advancing',\n",
       " 'mh',\n",
       " '##eal',\n",
       " '##th',\n",
       " '-',\n",
       " 'supported',\n",
       " 'adoption',\n",
       " 'and',\n",
       " 'sustain',\n",
       " '##ment',\n",
       " 'of',\n",
       " 'an',\n",
       " 'evidence',\n",
       " '-',\n",
       " 'based',\n",
       " 'mental',\n",
       " 'health',\n",
       " 'intervention',\n",
       " 'for',\n",
       " 'youth',\n",
       " 'in',\n",
       " 'a',\n",
       " 'school',\n",
       " '-',\n",
       " 'based',\n",
       " 'delivery',\n",
       " 'setting',\n",
       " 'in',\n",
       " 'sie',\n",
       " '##rr',\n",
       " '##a',\n",
       " 'leon',\n",
       " '##e',\n",
       " 'ref',\n",
       " '##ining',\n",
       " 'and',\n",
       " 'pilot',\n",
       " 'testing',\n",
       " 'a',\n",
       " 'decision',\n",
       " 'support',\n",
       " 'intervention',\n",
       " 'to',\n",
       " 'facilitate',\n",
       " 'adoption',\n",
       " 'of',\n",
       " 'evidence',\n",
       " '-',\n",
       " 'based',\n",
       " 'programs',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'parent',\n",
       " 'and',\n",
       " 'child',\n",
       " 'mental',\n",
       " 'health',\n",
       " 're',\n",
       " '##usa',\n",
       " '##ble',\n",
       " ',',\n",
       " 'transparent',\n",
       " ',',\n",
       " 'and',\n",
       " 'recon',\n",
       " '##fig',\n",
       " '##urable',\n",
       " 'n',\n",
       " '##95',\n",
       " '-',\n",
       " 'equivalent',\n",
       " 'respir',\n",
       " '##ator',\n",
       " 'masks',\n",
       " ':',\n",
       " 'design',\n",
       " ',',\n",
       " 'fabrication',\n",
       " ',',\n",
       " 'and',\n",
       " 'trials',\n",
       " 'for',\n",
       " 'enhanced',\n",
       " 'adoption',\n",
       " 'understanding',\n",
       " 'the',\n",
       " 'adoption',\n",
       " 'and',\n",
       " 'impact',\n",
       " 'of',\n",
       " 'new',\n",
       " 'risk',\n",
       " 'assessment',\n",
       " 'technologies',\n",
       " 'in',\n",
       " 'prostate',\n",
       " 'cancer',\n",
       " 'care',\n",
       " 'addressing',\n",
       " 'adoption',\n",
       " 'barriers',\n",
       " 'to',\n",
       " 'patient',\n",
       " 'transportation',\n",
       " 'services',\n",
       " 'the',\n",
       " 'college',\n",
       " 'alcohol',\n",
       " 'intervention',\n",
       " 'matrix',\n",
       " '(',\n",
       " 'college',\n",
       " 'aim',\n",
       " ')',\n",
       " ':',\n",
       " 'adoption',\n",
       " 'and',\n",
       " 'implementation',\n",
       " 'across',\n",
       " 'college',\n",
       " 'campus',\n",
       " '##es',\n",
       " 'social',\n",
       " 'networks',\n",
       " 'of',\n",
       " 'diffusion',\n",
       " 'and',\n",
       " 'adoption',\n",
       " ':',\n",
       " 'investigating',\n",
       " 'the',\n",
       " 'network',\n",
       " 'effects',\n",
       " 'on',\n",
       " 'implementation',\n",
       " 'of',\n",
       " 'evidence',\n",
       " '-',\n",
       " 'based',\n",
       " 'interventions',\n",
       " 'for',\n",
       " 'early',\n",
       " 'intervention',\n",
       " 'providers',\n",
       " 'of',\n",
       " 'children',\n",
       " 'hpv',\n",
       " 'echo',\n",
       " ':',\n",
       " 'increasing',\n",
       " 'the',\n",
       " 'adoption',\n",
       " 'of',\n",
       " 'evidence',\n",
       " '-',\n",
       " 'based',\n",
       " 'communication',\n",
       " 'strategies',\n",
       " 'for',\n",
       " 'hpv',\n",
       " 'vaccination',\n",
       " 'in',\n",
       " 'rural',\n",
       " 'primary',\n",
       " 'care',\n",
       " 'practices',\n",
       " 'understanding',\n",
       " 'disparities',\n",
       " 'in',\n",
       " 'the',\n",
       " 'adoption',\n",
       " 'and',\n",
       " 'use',\n",
       " 'of',\n",
       " 'assist',\n",
       " '##ive',\n",
       " 'technology',\n",
       " 'by',\n",
       " 'older',\n",
       " 'hispanics',\n",
       " 'adoption',\n",
       " 'and',\n",
       " 'implementation',\n",
       " 'of',\n",
       " 'an',\n",
       " 'evidence',\n",
       " '-',\n",
       " 'based',\n",
       " 'safe',\n",
       " 'driving',\n",
       " 'program',\n",
       " 'for',\n",
       " 'high',\n",
       " '-',\n",
       " 'risk',\n",
       " 'teen',\n",
       " 'drivers',\n",
       " 'motion',\n",
       " 'sequencing',\n",
       " 'for',\n",
       " 'all',\n",
       " ':',\n",
       " 'pip',\n",
       " '##elin',\n",
       " '##ing',\n",
       " ',',\n",
       " 'distribution',\n",
       " 'and',\n",
       " 'training',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'broad',\n",
       " 'adoption',\n",
       " 'of',\n",
       " 'a',\n",
       " 'next',\n",
       " '-',\n",
       " 'generation',\n",
       " 'platform',\n",
       " 'for',\n",
       " 'behavioral',\n",
       " 'and',\n",
       " 'neurobehavioral',\n",
       " 'analysis',\n",
       " 'the',\n",
       " 'implementation',\n",
       " ',',\n",
       " 'adoption',\n",
       " ',',\n",
       " 'and',\n",
       " 'sustainability',\n",
       " 'of',\n",
       " 'ho',\n",
       " \"'\",\n",
       " 'ou',\n",
       " '##na',\n",
       " 'pon',\n",
       " '##o',\n",
       " 'the',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'adopting',\n",
       " 'teen',\n",
       " '##s',\n",
       " ':',\n",
       " 'a',\n",
       " 'comparative',\n",
       " 'study',\n",
       " 'navig',\n",
       " '##ating',\n",
       " 'the',\n",
       " 'unique',\n",
       " 'needs',\n",
       " 'of',\n",
       " 'adolescent',\n",
       " 'adoption',\n",
       " 'the',\n",
       " 'impact',\n",
       " 'of',\n",
       " 'timing',\n",
       " 'on',\n",
       " 'adoption',\n",
       " 'outcomes',\n",
       " ':',\n",
       " 'examining',\n",
       " 'infant',\n",
       " 'and',\n",
       " 'teen',\n",
       " 'adoption',\n",
       " 'supporting',\n",
       " 'the',\n",
       " 'transition',\n",
       " 'to',\n",
       " 'adulthood',\n",
       " 'in',\n",
       " 'adopted',\n",
       " 'teen',\n",
       " '##s',\n",
       " 'exploring',\n",
       " 'the',\n",
       " 'long',\n",
       " '-',\n",
       " 'term',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'adopting',\n",
       " 'teen',\n",
       " '##s',\n",
       " 'versus',\n",
       " 'infants',\n",
       " 'adopting',\n",
       " 'teen',\n",
       " '##s',\n",
       " ':',\n",
       " 'a',\n",
       " 'systematic',\n",
       " 'review',\n",
       " 'of',\n",
       " 'the',\n",
       " 'literature',\n",
       " 'addressing',\n",
       " 'the',\n",
       " 'stereot',\n",
       " '##yp',\n",
       " '##es',\n",
       " 'and',\n",
       " 'real',\n",
       " '##ities',\n",
       " 'of',\n",
       " 'adopting',\n",
       " 'teen',\n",
       " '##s',\n",
       " 'comparing',\n",
       " 'the',\n",
       " 'parenting',\n",
       " 'experiences',\n",
       " 'of',\n",
       " 'adopting',\n",
       " 'infants',\n",
       " 'and',\n",
       " 'teen',\n",
       " '##s']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>Advancing mHealth-supported Adoption and Susta...</td>\n",
       "      <td>0.753539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>Refining and Pilot Testing a Decision Support ...</td>\n",
       "      <td>0.746505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>Understanding the Adoption and Impact of New R...</td>\n",
       "      <td>0.736815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>Addressing adoption barriers to patient transp...</td>\n",
       "      <td>0.766890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>The Implementation, Adoption, and Sustainabili...</td>\n",
       "      <td>0.906431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>The Challenges and Benefits of Adopting Teens:...</td>\n",
       "      <td>0.732473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>Navigating the Unique Needs of Adolescent Adop...</td>\n",
       "      <td>0.820002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>The Impact of Timing on Adoption Outcomes: Exa...</td>\n",
       "      <td>0.736598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence_1   \n",
       "30   Reusable, transparent, and reconfigurable N95-...  \\\n",
       "31   Reusable, transparent, and reconfigurable N95-...   \n",
       "32   Reusable, transparent, and reconfigurable N95-...   \n",
       "33   Reusable, transparent, and reconfigurable N95-...   \n",
       "34   Reusable, transparent, and reconfigurable N95-...   \n",
       "..                                                 ...   \n",
       "160  Motion Sequencing for All: pipelining, distrib...   \n",
       "161  Motion Sequencing for All: pipelining, distrib...   \n",
       "162  Motion Sequencing for All: pipelining, distrib...   \n",
       "163  Motion Sequencing for All: pipelining, distrib...   \n",
       "164  Motion Sequencing for All: pipelining, distrib...   \n",
       "\n",
       "                                            sentence_2  distance  \n",
       "30   Advancing mHealth-supported Adoption and Susta...  0.753539  \n",
       "31   Refining and Pilot Testing a Decision Support ...  0.746505  \n",
       "32   Reusable, transparent, and reconfigurable N95-...  1.000000  \n",
       "33   Understanding the Adoption and Impact of New R...  0.736815  \n",
       "34   Addressing adoption barriers to patient transp...  0.766890  \n",
       "..                                                 ...       ...  \n",
       "160  Motion Sequencing for All: pipelining, distrib...  1.000000  \n",
       "161  The Implementation, Adoption, and Sustainabili...  0.906431  \n",
       "162  The Challenges and Benefits of Adopting Teens:...  0.732473  \n",
       "163  Navigating the Unique Needs of Adolescent Adop...  0.820002  \n",
       "164  The Impact of Timing on Adoption Outcomes: Exa...  0.736598  \n",
       "\n",
       "[75 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# embeddings for the word 'record' \n",
    "token = 'adoption'\n",
    "indices = [i for i, t in enumerate(context_tokens) if t == token]\n",
    "\n",
    "token_embeddings = [context_embeddings[i] for i in indices]\n",
    "\n",
    "# # compare 'record' with different contexts\n",
    "list_of_distances = []\n",
    "for sentence_1, embed1 in zip(sentences, token_embeddings):\n",
    "  for sentence_2, embed2 in zip(sentences, token_embeddings):\n",
    "    cos_dist = 1 - cosine(embed1, embed2)\n",
    "    list_of_distances.append([sentence_1, sentence_2, cos_dist])\n",
    "\n",
    "distances_df = pd.DataFrame(list_of_distances, columns=['sentence_1', 'sentence_2', 'distance'])\n",
    "distances_df[distances_df.sentence_1.str.contains('adoption')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filepath = os.path.join('gdrive/My Drive/projections/')\n",
    "\n",
    "name = 'metadata.tsv'\n",
    "\n",
    "with open(os.path.join(filepath, name), 'w+') as file_metadata:\n",
    "  for i, token in enumerate(context_tokens):\n",
    "    file_metadata.write(token + '\\n')\n",
    "    \n",
    "import csv\n",
    "\n",
    "name = 'embeddings.tsv'\n",
    "\n",
    "with open(os.path.join(filepath, name), 'w+') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "    for embedding in context_embeddings:\n",
    "        writer.writerow(embedding.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
