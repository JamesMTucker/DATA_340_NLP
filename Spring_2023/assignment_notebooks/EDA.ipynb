{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Understanding our data is a crucial step in any data science project. In this assignment, you will perform an exploratory data analysis of corpus of documents. The corpus consists of 11,587 documents, each of which is a news article. The documents are stored in a folder called `data` in the root of the repository and the data is stored in the shared google drive `datasets`.\n",
    "\n",
    "In this assignment, you will analyze a corpus of news documents to answer the following questions:\n",
    "\n",
    "0. What is the nature of our data?\n",
    "    - 0a. What is the size of the corpus?\n",
    "    - 0b. Are there any duplicates in the corpus? If so, drop them.\n",
    "    - 0c. Are there any missing values in the corpus?\n",
    "    - 0d. How many unique documents are there in the corpus?\n",
    "1. What is the distribution of `token`s per document?\n",
    "    - 1a. What is the longest article?\n",
    "    - 1b. What is the shortest article?\n",
    "    - 1c. What is the 95th percentile of article lengths?\n",
    "2. How many different sources are there in the corpus?\n",
    "    - 2a. How many different sources are there in the dataset?\n",
    "    - 2b. What is the distribution of articles per source?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Corpus\n",
    "\n",
    "You are provided the following news corpus: `data/news_corpus.csv`. The corpus contains the following columns:\n",
    "\n",
    "- `index` int: The unique identifier of the document.\n",
    "- `source` str: The source of the document\n",
    "- `title` str: The title of the document\n",
    "- `text` str: The content of the article\n",
    "\n",
    "The data used in this notebook comes from the [`StoryGraph`](https://archive.org/details/storygraph?tab=about) project, created and maintained by Prof. Alexander Nwala.\n",
    "\n",
    "```BibTeX\n",
    "@MISC {nwala-cj20,\n",
    "    author = {Alexander Nwala and Michele C. Weigle and Michael L. Nelson},\n",
    "     title = {365 Dots in 2019: Quantifying Attention of News Sources},\n",
    "     year = {2020},\n",
    "      month = may,\n",
    "     howpublished = {Poster/demo accepted at the Computation + Journalism Symposium (symposium cancelled due to COVID-19)},\n",
    "     arxiv = {https://arxiv.org/abs/2003.09989},\n",
    "     pubdate = {202005}\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/news-2023-02-01.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. What is the nature of our data?\n",
    "\n",
    "Using your coding skills, answer the following questions. Please comment on your code and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0a. What is the size of the corpus?\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0b. Are there any duplicates in the corpus? If so, remove or drop them.\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0c. Are there any missing values in the corpus? If so, what data are missing?\n",
    "## Should the missing values be removed, explain?\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 What is the distribution of `token`s per document?\n",
    "\n",
    "Use the `spaCy` library to tokenize the text and analyze the distribution of token frequencies. You can use the `Counter` class from the `collections` library to count the number of times each token appears in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "## Load the spacy model: nlp\n",
    "NLP = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the tokens using Spacy\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the tokens\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## Plot the distribution of the number of tokens per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1a. What is the longest article?\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1b. What is the shortest article?\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1c. What is the 95th percentile of the number of tokens per document?\n",
    "## Hint: use np.percentile\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1d. What is the size of the vocabulary and the frequencies of each token in the corpus?\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "## Create list of stopwords from spacy\n",
    "stop_words = list(spacy.lang.en.stop_words.STOP_WORDS) + list(string.punctuation)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "## hint: use Counter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 How many different sources are there in the corpus?\n",
    "\n",
    "Please describe how many different sources exist in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2a. Plot how many different sources are there in the corpus?\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2b. Plot the distribution of articles per source?\n",
    "## hint: use seaborn boxplot\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d99a3f87c484a74ba405ca572f7f1b4059e93a8c4d7f8027bf5ae12e7919d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
