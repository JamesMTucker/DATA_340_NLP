{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with N-Gram Language Models\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/Fall_2023/notebooks/07_NGram_Models.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture outline:\n",
    "\n",
    "* Missed Lecture 9 Material\n",
    "* Assignment Updates and Clarifications\n",
    "* Project Updates, Clarifications, and Suggestions (Twitter scraper)\n",
    "* N-Gram Language Models\n",
    "  * Some background ideas and assumptions\n",
    "  * N-Gram Character Models\n",
    "  * N-Gram Word Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Language Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some background ideas and assumptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please turn in your _________ \n",
    "- [ ] notebook?\n",
    "- [ ] pdf?\n",
    "- [ ] html?\n",
    "- [ ] homework?\n",
    "- [ ] report?\n",
    "- [ ] slides?\n",
    "- [ ] poster?\n",
    "- [ ] video?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Models that assign probabilities to sequences of words are ... language models.\" (Jurafsky & Martin, p. 32)\n",
    "- Deep neural networks Language Models often assoctiate the training task of masked language modeling (MLM) with the task of predicting the next word in a sequence. Example: [BiomedNLP](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext?text=%5BMASK%5D+is+a+tumor+suppressor+gene)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some important concepts:\n",
    "\n",
    "- Smoothing\n",
    "- Backoff\n",
    "- Perplexity\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "\n",
    "$P(w|h)$ is the probability of a word $w$ given a history $h$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the probability of a sequence\n",
    "\n",
    "$$P(X_1, X_2, \\dots, X_n) = P(X_1)P(X_2|X_1)P(X_3|X_1, X_2) \\dots P(X_n|X_1, X_2, \\dots, X_{n-1})$$\n",
    "\n",
    "$$=\\sum_{k=1}^{n} P(X_k|X_1, X_2, \\dots, X_{k-1})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate the history by using the hidden markov assumption:\n",
    "\n",
    "$$P(w_n|w_{1:n-1}) \\approx P(w_n|w_{n-1})$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $N$ be the size of gram model. We can approximate the probability of a sequence by using the following equation:\n",
    "\n",
    "$$P(w_{1:n}) \\approx \\sum^{n}_{k=1} P(w_k|w_{k-1})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation\n",
    "\n",
    "$$P(w_n|w_{n-1}) = \\frac{C(w_{n-1}, w_n)}{\\sum_w C(w_{n-1}, w)}$$\n",
    "\n",
    "\"We can simplify this equation, since the sum of all bigram counts that start with a given word $w_{n-1}$ must be equal to the unigram count for that word $w_{n-1}$...\" (Jurafsky & Martin, p. 34)\n",
    "\n",
    "Explain why this is so.\n",
    "\n",
    "Thus, we can simplify the equation to:\n",
    "\n",
    "$$P(w_n|w_{n-1}) = \\frac{C(w_{n-1}, w_n)}{C(w_{n-1})}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Green Eggs and Ham\n",
    "\n",
    "What does this all mean? Let's code it up and seek for a better understanding.\n",
    "\n",
    "\n",
    "Let's use the following corpus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
    "\n",
    "sentences = corpus.split(\".\")\n",
    "sentences = [f\"<s> {s.strip()} </s>\" for s in sentences if s.strip() != \"\"]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create bigram tokens for each sentence\n",
    "tokens = [s.split() for s in sentences]\n",
    "bigrams = [bigram for sentence in tokens for bigram in zip(sentence[:-1], sentence[1:])]\n",
    "\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# flatten the list of tokens\n",
    "tokens = [token for sentence in tokens for token in sentence]\n",
    "\n",
    "# create a frequency distribution\n",
    "unigram_freq = {k: v for k, v in sorted(Counter(tokens).items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "## Generate probability table using the relative frequency\n",
    "freq = nltk.FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the probability of each bigram\n",
    "probs = {k: v / unigram_freq[k[0]] for k, v in freq.items()}\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a heatmap of the probabilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Generate a matrix of the probabilities with the bigrams as the index\n",
    "df = pd.DataFrame.from_dict(probs, orient=\"index\", columns=[\"Probability\"])\n",
    "df[['token_1', 'token_2']] = pd.DataFrame(df.index.tolist(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the table to get the heatmap\n",
    "df.pivot(\"token_1\", \"token_2\", \"Probability\").replace(np.nan, f'{0:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a heatmap of the probabilities\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate heatmap of the bigram probabilities\n",
    "sns.heatmap(df.pivot(\"token_1\", \"token_2\", \"Probability\"), annot=True, fmt=\".2f\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: A Character Bigram Language Model\n",
    "\n",
    "In this example, we will create a character bigram language model. We will use the _Lord of the Rings_ as our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Lord of the rings text\n",
    "\n",
    "# use google to load the data from drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "datasets_dir = \"/content/drive/My Drive/DATA_340_3_NLP/Datasets/\"\n",
    "\n",
    "# get the txt files\n",
    "filenames = [os.path.join(datasets_dir, f) for f in os.listdir(datasets_dir) if f.endswith(\".txt\") and 'LOTR' in f]\n",
    "\n",
    "# read the files\n",
    "corpus = []\n",
    "\n",
    "# read \n",
    "for f in filenames:\n",
    "    with open(f, 'r', encoding='ISO-8859-1') as file:\n",
    "        corpus.append(file.read())\n",
    "        \n",
    "# remove whitespace from the corpus\n",
    "corpus = \" \".join(corpus)\n",
    "corpus = \" \".join(corpus.split())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import unidecode\n",
    "except ModuleNotFoundError:\n",
    "  !pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "text = unidecode(corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a bigram dictionary\n",
    "bigrams = {}\n",
    "\n",
    "# Since we need to mark the start and end of the sentence, we add a special character\n",
    "for w in words:\n",
    "    chs = ['_'] + list(w) + ['_']\n",
    "    for ch1, ch2 in zip(chs[:-1], chs[1:]):\n",
    "        bigrams[(ch1, ch2)] = bigrams.get((ch1, ch2), 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the bigrams\n",
    "sorted(bigrams.items(), key=lambda kv: kv[1], reverse=True)[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Encoder and Decoder for Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of characters from our data\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "# Create a encoding dictionary\n",
    "stoi = {c:i+1 for i, c in enumerate(chars)}\n",
    "stoi['_'] = 0\n",
    "\n",
    "# Create a decoding dictionary\n",
    "itos = {i:c for c, i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the int to string dictionary\n",
    "itos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create a bigram frequency count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the Torch library to create our bigram matrix\n",
    "import torch\n",
    "\n",
    "# Since we our input is len(chars) x len(chars), we create a matrix of zeros\n",
    "N = torch.zeros((len(chars)+1, len(chars)+1), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['_'] + list(w) + ['_']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now see the counts of each bigram in our tensor\n",
    "N[0, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualize the bigram count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's visualize the bigram matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(36,36))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(61):\n",
    "    for j in range(61):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha='center', va='bottom', color='gray', fontsize=8)\n",
    "        plt.text(j, i, N[i, j].item(), ha='center', va='top', color='gray', fontsize=8)\n",
    "plt.axis('off');\n",
    "plt.savefig('bigram_matrix.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Convert the bigram count matrix to a probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample from the probability distribution\n",
    "p = N[0].float()\n",
    "\n",
    "## normalize\n",
    "p = p / p.sum()\n",
    "\n",
    "## Sum to 1\n",
    "p.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Use our model to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a probability matrix for all the bigrams\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "## Our paragraph to write\n",
    "paragraph = []\n",
    "\n",
    "for i in range(1000):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p = N[ix].float()\n",
    "        p = p / p.sum()\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    paragraph.append(''.join(out))\n",
    "    \n",
    "paragraph = ''.join(paragraph)\n",
    "paragraph = paragraph.replace('_', ' ')\n",
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Did our model learn anything?\n",
    "\n",
    "## Tolkein's style\n",
    "for w in words[:10]:\n",
    "    chs = ['_'] + list(w) + ['_']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = N[ix1, ix2].item() / N[ix1].sum().item()\n",
    "        print(f\"{ch1} -> {ch2} : {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our model's style\n",
    "\n",
    "words = paragraph.split(\" \")\n",
    "\n",
    "for w in words[:10]:\n",
    "    chs = ['_'] + list(w) + ['_']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = N[ix1, ix2].item() / N[ix1].sum().item()\n",
    "        print(f\"{ch1} -> {ch2} : {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 9: Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maximum Likelihood Estimation\n",
    "import numpy as np\n",
    "\n",
    "words = paragraph.split(\" \")\n",
    "\n",
    "## log(a*b) = log(a) + log(b)\n",
    "\n",
    "for w in words[:10]:\n",
    "    chs = ['_'] + list(w) + ['_']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = N[ix1, ix2].item() / N[ix1].sum().item()\n",
    "        logprob = np.log(prob)\n",
    "        print(f\"{ch1} -> {ch2} : {prob:.4f} {logprob:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to a maximize the likelihood of the generated text with respect to the model parameters. (statistical inference)\n",
    "\n",
    "[Log's are monotonic](https://www.wolframalpha.com/input?i=log%28x%29+from+0+to+1), so we can maximize the log-likelihood instead.\n",
    "\n",
    "The smaller the log-likelihood, the better the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = paragraph.split(\" \")\n",
    "log_likelihood = 0\n",
    "n = 0\n",
    "\n",
    "\n",
    "## log(a*b) = log(a) + log(b)\n",
    "\n",
    "# for w in ['james']: # we can test any word we want\n",
    "for w in ['jamez']: # a problem with this model is that it doesn't know how to spell\n",
    "# for w in words[:10]:\n",
    "    chs = ['_'] + list(w) + ['_']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = N[ix1, ix2].item() / N[ix1].sum().item()\n",
    "        logprob = np.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f\"{ch1} -> {ch2} : {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = paragraph.split(\" \")\n",
    "log_likelihood = 0\n",
    "n = 0\n",
    "\n",
    "\n",
    "## log(a*b) = log(a) + log(b)\n",
    "\n",
    "# for w in ['james']: # we can test any word we want\n",
    "for w in ['jamez']: # a problem with this model is that it doesn't know how to spell\n",
    "# for w in words[:10]:\n",
    "    chs = ['_'] + list(w) + ['_']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = N[ix1, ix2].item() / N[ix1].sum().item()\n",
    "        logprob = np.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f\"{ch1} -> {ch2} : {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n=}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = paragraph.split(\" \")\n",
    "log_likelihood = 0\n",
    "n = 0\n",
    "\n",
    "\n",
    "## log(a*b) = log(a) + log(b)\n",
    "\n",
    "# for w in ['james']: # we can test any word we want\n",
    "for w in ['jamez']: # a problem with this model is that it doesn't know how to spell\n",
    "# for w in words[:10]:\n",
    "    chs = ['_'] + list(w) + ['_']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = N[ix1, ix2].item() / N[ix1].sum().item() + 1 # add one for smoothing\n",
    "        logprob = np.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f\"{ch1} -> {ch2} : {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n=}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2345)\n",
    "\n",
    "## Our paragraph to write\n",
    "paragraph = []\n",
    "\n",
    "for i in range(1000):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p = N[ix].float()\n",
    "        p = p / p.sum()\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    paragraph.append(''.join(out))\n",
    "    \n",
    "paragraph = ''.join(paragraph)\n",
    "paragraph = paragraph.replace('_', ' ')\n",
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d99a3f87c484a74ba405ca572f7f1b4059e93a8c4d7f8027bf5ae12e7919d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
