{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Text Normalization\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Text Normalization](#Text-Normalization)\n",
    "2. [Tokenization](#Tokenization)\n",
    "3. [Stemming](#Stemming)\n",
    "4. [Lemmatization](#Lemmatization)\n",
    "5. [Stop Words](#Stop-Words)\n",
    "6. [Vectorization](#Vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "\n",
    "Text normalization is the process of transforming text into a single canonical form that it might not have had before. This can involve changing the case of the text, removing punctuation, expanding contractions, and so on. The goal is to transform text into a more standard form that might be easier for other NLP tasks. More importantly, our goal is to think about strategies to represent text in numerical form, which is the only way that machine learning algorithms can process text.\n",
    "\n",
    "### Some general stdlib python libraries for text normalization\n",
    "\n",
    "* `string` (punctuation)\n",
    "* `re` (regular expressions)\n",
    "* `lower()`, `upper()`, `capitalize()`, `title()` # string methods\n",
    "* `split()`, `join()` # more helpful string methods\n",
    "* `replace()` # more helpful string methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pip installable libraries for text normalization\n",
    "\n",
    "* `nltk` (Natural Language Toolkit)\n",
    "* `spacy` (Industrial Strength NLP)\n",
    "* `textblob` (Simplified Text Processing)\n",
    "* `gensim` (Topic Modeling for Humans)\n",
    "* `stanza` (Python wrapper for Stanford NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample text for understading text normalization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Human infants have the remarkable ability to learn any human language. One proposed mechanism for this ability \n",
    "is distributional learning, where learners infer the underlying cluster structure from unlabeled input. Computational\n",
    "models of distributional learning have historically been principled but psychologically-implausible\n",
    "computational-level models, or ad hoc but psychologically plausible algorithmic-level models. Approximate rational\n",
    "models like particle filters can potentially bridge this divide, and allow principled, but psychologically plausible\n",
    "models of distributional learning to be specified and evaluated. As a proof of concept, I evaluate one such particle\n",
    "filter model, applied to learning English voicing categories from distributions of voice-onset times (VOTs). \n",
    "I find that this model learns well, but behaves somewhat differently from the standard, unconstrained Gibbs\n",
    "sampler implementation of the underlying rational model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization: breaking text into smaller units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "NLTK is a slightly outdated platform for building Python programs to work with human language data. Although somewhat outdated, it provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic analysis. NLTK is a great tool for learning NLP, but it is not the best tool for building production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_tokens = word_tokenize(text)\n",
    "\n",
    "print(nltk_tokens, len(nltk_tokens), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK tokenization docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "\n",
    "SpaCy is a modern Python library for industrial-strength natural language processing. It is designed to be fast and accurate, and it includes built-in capabilities for visualizing and analyzing NLP data. SpaCy is a great choice for building real-world NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize our sample text using spacy\n",
    "\n",
    "import spacy\n",
    "\n",
    "NLP = spacy.load('en_core_web_sm')\n",
    "\n",
    "spacy_tokens = [token.text for token in NLP(text)]\n",
    "\n",
    "print(spacy_tokens, len(spacy_tokens), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.tokens.Token??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob\n",
    "\n",
    "TextBlob is a simplified text processing library that sits on top of NLTK. It provides an easy-to-use interface to NLTK along with some text processing capabilities of its own. TextBlob is a great choice for getting started with NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text using the textblob library\n",
    "from textblob import TextBlob\n",
    "\n",
    "blob_tokens = TextBlob(text).words\n",
    "\n",
    "print(blob_tokens, len(blob_tokens), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = [x for x in dir(TextBlob) if not x.startswith('_')]\n",
    "funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim\n",
    "\n",
    "Gensim is a Python library for topic modeling, document indexing, and similarity retrieval with large corpora. It is designed to be fast and memory efficient, and it includes capabilities for hierarchical document clustering. Gensim is a great choice for building production NLP systems.\n",
    "\n",
    "https://tedboy.github.io/nlps/generated/generated/gensim.utils.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# returns a generator\n",
    "gensim_tokens = list(gensim.utils.tokenize(text))\n",
    "\n",
    "# get the token count\n",
    "# gensim_tokens_len = len(list(gensim_tokens))\n",
    "\n",
    "print(list(gensim_tokens), len(gensim_tokens), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.utils.tokenize??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CoreNLP\n",
    "\n",
    "Stanford CoreNLP is a suite of production-ready natural language analysis tools. It provides a set of human language technology tools that can be used to analyze text. It provides support for tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. It is written in Java, but it provides a Python wrapper that can be used to access its capabilities.\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/installation_usage.html#getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "# Instantiate our stanza pipeline\n",
    "stan_NLP = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "\n",
    "stan_tokens = [token.text for sent in stan_NLP(text).sentences for token in sent.tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stan_tokens, len(stan_tokens), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_NLP??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "# Display the token counts\n",
    "display(Markdown(\"| NLTK | spaCy | TextBlob | Gensim | Stanza |\\n| --- | --- | --- | --- | --- |\\n| {} | {} | {} | {} | {} |\"\\\n",
    "    .format(len(nltk_tokens),\n",
    "            len(spacy_tokens),\n",
    "            len(blob_tokens),\n",
    "            len(gensim_tokens),\n",
    "            len(stan_tokens)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the texts\n",
    "display(Markdown(\"| NLTK | spaCy | TextBlob | Gensim | Stanza |\\n| --- | --- | --- | --- | --- |\\n| {} | {} | {} | {} | {} |\"\\\n",
    "    .format(nltk_tokens,\n",
    "            spacy_tokens,\n",
    "            blob_tokens,\n",
    "            gensim_tokens,\n",
    "            stan_tokens))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming: (attempting to) reduce words to their root form\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base, or root form. For example, the stem of the word `waiting` is `wait`. A stemming algorithm reduces the words `waiting`, `waits`, and `waited` to the same stem. This is important in building NLP systems because it helps us ensure that we are processing all forms of a word using the same representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "https://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK Stemming\n",
    "\n",
    "## import the PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "## Create an instance of the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "## Stem the tokens\n",
    "nltk_stemmed_tokens = [stemmer.stem(token) for token in nltk_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stemmed = \" \".join(nltk_stemmed_tokens)\n",
    "\n",
    "# Display the stemmed text\n",
    "print(nltk_stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "\n",
    "https://spacy.io/api/lemmatizer\n",
    "\n",
    "See below for SpaCy lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/quickstart.html#stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TextBlob Stemming\n",
    "\n",
    "## Stem the tokens\n",
    "blob = TextBlob(text)\n",
    "\n",
    "blob_stemmed_tokens = [word.stem() for word in blob.words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim\n",
    "\n",
    "https://tedboy.github.io/nlps/generated/generated/gensim.parsing.porter.PorterStemmer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "gensim_stemmed_tokens = [PorterStemmer().stem(token) for token in gensim_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_stemmed_tokens = \" \".join(gensim_stemmed_tokens)\n",
    "gensim_stemmed_tokens[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza\n",
    "\n",
    "Stanza does not have a built-in stemmer, but it does have a lemmatizer. See below for Stanza lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of stemming algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the stemmed texts\n",
    "display(Markdown(\"| NLTK | TextBlob | Gensim |\\n| --- | --- | --- |\\n| {} | {} | {} |\"\\\n",
    "    .format(nltk_stemmed_tokens,\n",
    "            blob_stemmed_tokens,\n",
    "            gensim_stemmed_tokens))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization: reducing words to their dictionary form\n",
    "\n",
    "Lemmatization is the linguistic process of reducing the groups of inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "| Case | Masc. | Fem. | Neut. |\n",
    "|------|-------|------|-------|\n",
    "| nominative | der | die | das |\n",
    "| accusative | den | die | das |\n",
    "| dative | dem | der | dem |\n",
    "| genitive | des | der | des |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/stem/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "## NLTK Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Create an instance of the WordNetLemmatizer\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the tokens\n",
    "nltk_lemmas = [nltk_lemmatizer.lemmatize(token) for token in nltk_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(nltk_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "\n",
    "https://spacy.io/api/lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SpaCy Lemmatization\n",
    "import spacy\n",
    "\n",
    "## Create an instance of the spaCy library\n",
    "spacy_NLP = spacy.load('en_core_web_sm')\n",
    "\n",
    "## Lemmatize the tokens\n",
    "spacy_lemmas = [token.lemma_ for token in spacy_NLP(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(spacy_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "## Create an instance of the TextBlob library\n",
    "blob = TextBlob(text)\n",
    "\n",
    "## Lemmatize the blob\n",
    "blob_lemmas = [word.lemmatize() for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(blob_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim\n",
    "\n",
    "Gensim no longer has a lemmatizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/lemma.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "## Create an instance of the stanza library\n",
    "stanza_NLP = stanza.Pipeline(lang='en', processors='tokenize,lemma')\n",
    "\n",
    "## Create a document object\n",
    "doc = stanza_NLP(text)\n",
    "\n",
    "# Stanza lemmas\n",
    "stanza_lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "## Lemmatize the tokens\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the lemmatized texts\n",
    "display(Markdown(\"| NLTK | spaCy | TextBlob | Stanza |\\n| --- | --- | --- | --- |\\n| {} | {} | {} | {} |\"\\\n",
    "    .format(nltk_lemmas,\n",
    "            spacy_lemmas,\n",
    "            blob_lemmas,\n",
    "            stanza_lemmas))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword lists\n",
    "\n",
    "`Stopwords` are words that are so common that they are not useful for analysis. For example, the word `the` is a stopword. To nomralize our text with stopwords, we remove them from our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "https://www.nltk.org/book/ch02.html#stopwords_index_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK Stopwords\n",
    "\n",
    "## Import the stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "\n",
    "https://spacy.io/api/language#section-defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "## Create a list of SpaCy stopwords in English\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "spacy_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob\n",
    "\n",
    "TextBlob relies on NLTK for stopword lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim\n",
    "\n",
    "https://radimrehurek.com/gensim/parsing/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your own stopword list using Zipf's Law or other statistical methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization: representing text as numbers\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "Bag of words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "* A vocabulary of known words.\n",
    "* A measure of the presence of known words.\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "TF-IDF stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.\n",
    "\n",
    "### GloVe\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "### FastText\n",
    "\n",
    "FastText is an algorithm that generates vectors based on subword information. It is an extension of the word2vec model, which learns vectors for subwords by treating each subword as an atomic entity. FastText represents each word as an n-gram of characters. Word vectors are then generated by computing the mean vector of each word. Subword information allows us to obtain vectors for words that did not appear in our training corpus.\n",
    "\n",
    "### Elmo\n",
    "\n",
    "Elmo is a deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment, and sentiment analysis.\n",
    "\n",
    "### GPT\n",
    "\n",
    "GPT vectors are based on the GPT model, which is a large-scale transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT is a direct scale-up of the GPT-2 model, which is trained on 40GB of internet text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing text\n",
    "\n",
    "There are numerous ways to vectorize text. We will begin with some relatively naive methods and advance to more sophisticated methods in the coming weeks. To begin, let's code out a vanilla implementation of a bag of words vectorizer using `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus\n",
    "\n",
    "When we vectorize text, we are concerned with the corpus, understood as a collection of documents, and the vocabulary used in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['NLP class is the most awesome class in the Data Science program',\n",
    "          'I think you are wrong and the best data science class is Graphing Algorithms',\n",
    "          'No, you both are wrong, it is Machine Learning']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "How shall we define vocabulary?\n",
    "\n",
    "* White space?\n",
    "* Tokens?\n",
    "* Lemmas?\n",
    "* Stems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vocab = set([word for word in ' '.join(corpus).split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_vocab, len(token_vocab), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's go with lemmas\n",
    "import spacy\n",
    "\n",
    "NLP = spacy.load('en_core_web_sm')\n",
    "\n",
    "vocabulary = [token.lemma_.lower() for token in NLP(' '.join(corpus))]\n",
    "\n",
    "print(vocabulary, len(vocabulary), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce down to unique words\n",
    "vocabulary = list(set(vocabulary))\n",
    "\n",
    "print(vocabulary, len(vocabulary), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizating our corpus\n",
    "\n",
    "How can we vectorize our corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a function to vectorize our text\n",
    "\n",
    "def create_sparse_vector(sentence, vocabulary):\n",
    "    # initialize the vector as the same length as the vocabulary\n",
    "    vector = np.zeros(len(vocabulary), dtype=int)\n",
    "    \n",
    "    # tokenize the sentence\n",
    "    sentence_lemmas = [token.lemma_.lower() for token in NLP(sentence)]\n",
    "    \n",
    "    for i, lemma in enumerate(sentence_lemmas):\n",
    "        if lemma.lower() in vocabulary:\n",
    "            index = vocabulary.index(lemma)\n",
    "            vector[index] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the function to vectorize our corpus\n",
    "\n",
    "vectors = [create_sparse_vector(sentence, vocabulary) for sentence in corpus]\n",
    "print(vectors, len(vectors), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look up some words in our vocabulary\n",
    "\n",
    "vocabulary[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `sklearn` CountVectorizer\n",
    "\n",
    "Now we have coded out our own vectorizer, we can have a better understanding of what `sklearn` is doing under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# let's vectorize our sentences\n",
    "vectorized = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# let's look at the vocabulary\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the vectorized sentences\n",
    "print(vectorized.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
