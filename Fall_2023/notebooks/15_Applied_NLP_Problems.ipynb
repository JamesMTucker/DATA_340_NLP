{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Applied NLP Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Overview\n",
    "\n",
    "* Named Entity Recognition (NER) using SpaCy and Transformers\n",
    "* Text summarization using Transformers\n",
    "* Text generation using Transformers\n",
    "* Analyzing `Fake news` using Transformers and ChatGPT\n",
    "* Semantic role labeling using Transformers and ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) using SpaCy and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER) is the task of identifying named entities in text and classifying them into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text} is type {ent.label_} - index_location: {ent.start_char}:{ent.end_char}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### more complex data\n",
    "\n",
    "# https://pubmed.ncbi.nlm.nih.gov/37071411/\n",
    "text = \"\"\"\n",
    "Most patients were initially certified for a 1:1 (∆9-tetrahydrocannabinol:cannabidiol) tincture.\n",
    "Eight-seven percent of patients (n = 60) were noted to exhibit an improvement in any PD symptom after starting MC.\n",
    "Symptoms with the highest incidence of improvement included cramping/dystonia, pain, spasticity, lack of appetite, dyskinesia, and tremor.\n",
    "After starting MC, 56% of opioid users (n = 14) were able to decrease or discontinue opioid use with an average daily morphine milligram equivalent change from 31 at baseline to 22 at the last follow-up visit.\n",
    "The MC was well-tolerated with no severe AEs reported and low rate of MC discontinuation due to AEs (n = 4).\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text} is type {ent.label_} - index_location: {ent.start_char}:{ent.end_char}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using SciSpacy\n",
    "\n",
    "import spacy\n",
    "import scispacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_scibert\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text} is type {ent.label_} - index_location: {ent.start_char}:{ent.end_char}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Entity: patients is type ENTITY - index_location: 6:14\n",
    "Entity: ∆9-tetrahydrocannabinol:cannabidiol) tincture is type ENTITY - index_location: 51:96\n",
    "Entity: patients is type ENTITY - index_location: 121:129\n",
    "Entity: improvement is type ENTITY - index_location: 164:175\n",
    "Entity: PD is type ENTITY - index_location: 183:185\n",
    "Entity: symptom is type ENTITY - index_location: 186:193\n",
    "Entity: MC is type ENTITY - index_location: 209:211\n",
    "Entity: Symptoms is type ENTITY - index_location: 213:221\n",
    "Entity: incidence is type ENTITY - index_location: 239:248\n",
    "Entity: improvement is type ENTITY - index_location: 252:263\n",
    "Entity: cramping/dystonia is type ENTITY - index_location: 273:290\n",
    "Entity: pain is type ENTITY - index_location: 292:296\n",
    "Entity: spasticity is type ENTITY - index_location: 298:308\n",
    "Entity: lack of appetite is type ENTITY - index_location: 310:326\n",
    "Entity: dyskinesia is type ENTITY - index_location: 328:338\n",
    "Entity: tremor is type ENTITY - index_location: 344:350\n",
    "Entity: MC is type ENTITY - index_location: 367:369\n",
    "Entity: opioid is type ENTITY - index_location: 378:384\n",
    "Entity: users is type ENTITY - index_location: 385:390\n",
    "Entity: decrease is type ENTITY - index_location: 413:421\n",
    "Entity: discontinue is type ENTITY - index_location: 425:436\n",
    "Entity: opioid is type ENTITY - index_location: 437:443\n",
    "Entity: daily is type ENTITY - index_location: 464:469\n",
    "Entity: morphine is type ENTITY - index_location: 470:478\n",
    "Entity: milligram is type ENTITY - index_location: 479:488\n",
    "...\n",
    "Entity: low rate is type ENTITY - index_location: 620:628\n",
    "Entity: MC is type ENTITY - index_location: 632:634\n",
    "Entity: discontinuation is type ENTITY - index_location: 635:650\n",
    "Entity: AEs is type ENTITY - index_location: 658:661\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/james/Projects/GitHub/DATA_340_NLP/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-28 12:26:39.570832: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-28 12:26:39.637248: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-28 12:26:39.895912: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-28 12:26:39.895935: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-28 12:26:39.897840: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-28 12:26:40.042582: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-28 12:26:40.791894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading model.safetensors: 100%|██████████| 433M/433M [00:06<00:00, 69.6MB/s] \n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/media/james/Projects/GitHub/DATA_340_NLP/venv/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'MISC',\n",
       "  'score': 0.9794679,\n",
       "  'word': 'PD',\n",
       "  'start': 183,\n",
       "  'end': 185},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.75518674,\n",
       "  'word': 'MC',\n",
       "  'start': 209,\n",
       "  'end': 211},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.7457224,\n",
       "  'word': 'MC',\n",
       "  'start': 367,\n",
       "  'end': 369},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.671571,\n",
       "  'word': 'MC',\n",
       "  'start': 566,\n",
       "  'end': 568},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.52196264,\n",
       "  'word': 'A',\n",
       "  'start': 603,\n",
       "  'end': 604},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.6522742,\n",
       "  'word': 'MC',\n",
       "  'start': 632,\n",
       "  'end': 634}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline('ner', model='dslim/bert-base-NER', tokenizer='dslim/bert-base-NER', grouped_entities=True)\n",
    "ner(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 266M/266M [00:03<00:00, 66.6MB/s] \n",
      "/media/james/Projects/GitHub/DATA_340_NLP/venv/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'Lab_value',\n",
       "  'score': 0.4000865,\n",
       "  'word': '1',\n",
       "  'start': 46,\n",
       "  'end': 47},\n",
       " {'entity_group': 'Lab_value',\n",
       "  'score': 0.9972366,\n",
       "  'word': 'eight - seven percent',\n",
       "  'start': 98,\n",
       "  'end': 117},\n",
       " {'entity_group': 'Disease_disorder',\n",
       "  'score': 0.99834895,\n",
       "  'word': 'pd',\n",
       "  'start': 183,\n",
       "  'end': 185},\n",
       " {'entity_group': 'Sign_symptom',\n",
       "  'score': 0.9996455,\n",
       "  'word': 'cr',\n",
       "  'start': 273,\n",
       "  'end': 275},\n",
       " {'entity_group': 'Sign_symptom',\n",
       "  'score': 0.93797743,\n",
       "  'word': '##amp',\n",
       "  'start': 275,\n",
       "  'end': 278},\n",
       " {'entity_group': 'Sign_symptom',\n",
       "  'score': 0.7478892,\n",
       "  'word': 'd',\n",
       "  'start': 282,\n",
       "  'end': 283},\n",
       " {'entity_group': 'Sign_symptom',\n",
       "  'score': 0.985441,\n",
       "  'word': 'spa',\n",
       "  'start': 298,\n",
       "  'end': 301},\n",
       " {'entity_group': 'Sign_symptom',\n",
       "  'score': 0.92435396,\n",
       "  'word': 'dyskines',\n",
       "  'start': 328,\n",
       "  'end': 336},\n",
       " {'entity_group': 'Lab_value',\n",
       "  'score': 0.7490449,\n",
       "  'word': '56 %',\n",
       "  'start': 371,\n",
       "  'end': 374},\n",
       " {'entity_group': 'Disease_disorder',\n",
       "  'score': 0.9088097,\n",
       "  'word': 'op',\n",
       "  'start': 378,\n",
       "  'end': 380},\n",
       " {'entity_group': 'Medication',\n",
       "  'score': 0.8767595,\n",
       "  'word': 'op',\n",
       "  'start': 437,\n",
       "  'end': 439},\n",
       " {'entity_group': 'Medication',\n",
       "  'score': 0.77150154,\n",
       "  'word': '##phine',\n",
       "  'start': 473,\n",
       "  'end': 478},\n",
       " {'entity_group': 'Dosage',\n",
       "  'score': 0.4974289,\n",
       "  'word': '##ram',\n",
       "  'start': 485,\n",
       "  'end': 488},\n",
       " {'entity_group': 'Lab_value',\n",
       "  'score': 0.9992072,\n",
       "  'word': '31',\n",
       "  'start': 512,\n",
       "  'end': 514},\n",
       " {'entity_group': 'Disease_disorder',\n",
       "  'score': 0.99966466,\n",
       "  'word': 'mc',\n",
       "  'start': 566,\n",
       "  'end': 568},\n",
       " {'entity_group': 'Severity',\n",
       "  'score': 0.9941514,\n",
       "  'word': 'severe',\n",
       "  'start': 596,\n",
       "  'end': 602},\n",
       " {'entity_group': 'Disease_disorder',\n",
       "  'score': 0.9994223,\n",
       "  'word': 'ae',\n",
       "  'start': 603,\n",
       "  'end': 605},\n",
       " {'entity_group': 'Lab_value',\n",
       "  'score': 0.9885456,\n",
       "  'word': 'low',\n",
       "  'start': 620,\n",
       "  'end': 623},\n",
       " {'entity_group': 'Disease_disorder',\n",
       "  'score': 0.9996766,\n",
       "  'word': 'mc',\n",
       "  'start': 632,\n",
       "  'end': 634},\n",
       " {'entity_group': 'Disease_disorder',\n",
       "  'score': 0.9996525,\n",
       "  'word': 'ae',\n",
       "  'start': 658,\n",
       "  'end': 660}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/d4data/biomedical-ner-all\n",
    "ner = pipeline('ner', model='d4data/biomedical-ner-all', tokenizer='d4data/biomedical-ner-all', grouped_entities=True)\n",
    "ner(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your own pipeline\n",
    "\n",
    "* Extract semantic triples from the text then perform NER on the extracted triples\n",
    "* Use Stanford CoreNLP to extract semantic triples from the text then perform NER on the extracted triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 12:28:53 WARNING: Directory /home/james/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n",
      "2023-11-28 12:28:53 INFO: Writing properties to tmp file: corenlp_server-e2195cccde2e4d65.props\n",
      "2023-11-28 12:28:53 INFO: Starting server with command: java -Xmx16G -cp /home/james/stanza_corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9020 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-e2195cccde2e4d65.props -annotators tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,openie -preload -outputFormat serialized\n",
      "[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---\n",
      "[main] INFO CoreNLP - Server default properties:\n",
      "\t\t\t(Note: unspecified annotator properties are English defaults)\n",
      "\t\t\tannotators = tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,openie\n",
      "\t\t\tinputFormat = text\n",
      "\t\t\toutputFormat = serialized\n",
      "\t\t\tprettyPrint = false\n",
      "\t\t\tthreads = 5\n",
      "[main] INFO CoreNLP - Threads: 5\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words-distsim.tagger ... done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.2 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.2 sec].\n",
      "[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.\n",
      "[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580705 unique entries out of 581864 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4867 unique entries out of 4867 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585572 unique entries from 2 files\n",
      "[main] INFO edu.stanford.nlp.pipeline.NERCombinerAnnotator - numeric classifiers: true; SUTime: true [no docDate]; fine grained: true\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.2 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... Time elapsed: 0.6 sec\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 20000 vectors, elapsed Time: 0.462 sec\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [1.1 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref\n",
      "[main] INFO edu.stanford.nlp.coref.statistical.SimpleLinearClassifier - Loading coref model edu/stanford/nlp/models/coref/statistical/ranking_model.ser.gz ... done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.CorefMentionAnnotator - Using mention detector type: dependency\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.093 seconds]\n",
      "[main] ERROR CoreNLP - Could not pre-load annotators in server; encountered exception:\n",
      "[main] ERROR CoreNLP - java.lang.IllegalArgumentException: annotator \"openie\" requires annotation \"PolarityAnnotation\". The usual requirements for this annotator are: tokenize,pos,lemma,depparse,natlog\n",
      "  edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:292)\n",
      "  edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:194)\n",
      "  edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:190)\n",
      "  edu.stanford.nlp.pipeline.StanfordCoreNLPServer.launchServer(StanfordCoreNLPServer.java:1622)\n",
      "  edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1644)\n",
      "[main] INFO CoreNLP - Starting server...\n",
      "[main] INFO CoreNLP - StanfordCoreNLPServer listening at /[0:0:0:0:0:0:0:0]:9020\n",
      "[pool-1-thread-3] INFO CoreNLP - [/127.0.0.1:47866] API call w/annotators tokenize,pos,lemma,ner,parse,depparse,natlog,coref,openie\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Most patients were initially certified for a 1:1 (∆9-tetrahydrocannabinol:cannabidiol) tincture. Eight-seven percent of patients (n = 60) were noted to exhibit an improvement in any PD symptom after starting MC. Symptoms with the highest incidence of improvement included cramping/dystonia, pain, spasticity, lack of appetite, dyskinesia, and tremor. After starting MC, 56% of opioid users (n = 14) were able to decrease or discontinue opioid use with an average daily morphine milligram equivalent change from 31 at baseline to 22 at the last follow-up visit. The MC was well-tolerated with no severe AEs reported and low rate of MC discontinuation due to AEs (n = 4). \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['patients', 'were', 'initially certified'],\n",
       " ['patients', 'were certified for', '1:1'],\n",
       " ['Most patients', 'were certified for', '1:1'],\n",
       " ['Most patients', 'were initially certified for', '1:1'],\n",
       " ['patients', 'were initially certified for', '1:1'],\n",
       " ['patients', 'were', 'certified'],\n",
       " ['Most patients', 'were', 'certified'],\n",
       " ['Most patients', 'were', 'initially certified'],\n",
       " ['improvement', 'is in', 'PD symptom'],\n",
       " ['Eight seven percent', 'were', 'noted'],\n",
       " ['Eight seven percent', 'exhibit', 'improvement in PD symptom'],\n",
       " ['dystonia', 'lack of', 'appetite'],\n",
       " ['Symptoms', 'is with', 'highest incidence of improvement'],\n",
       " ['Symptoms', 'included', 'cramping dystonia'],\n",
       " ['Symptoms', 'included', 'lack'],\n",
       " ['Symptoms', 'included', 'lack of appetite']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "from stanza.server import CoreNLPClient\n",
    "stanza.install_corenlp()\n",
    "\n",
    "## extract triples from the text\n",
    "triples = []\n",
    "\n",
    "# define the properties\n",
    "config = {\n",
    "    \"annotators\": \"tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,openie\",\n",
    "    \"openie.max_entailments_per_clause\": \"100\",\n",
    "    \"openie.threads\": \"4\",\n",
    "    \"memory\": \"16G\",\n",
    "    \"endpoint\": \"http://localhost:9020\",\n",
    "}\n",
    "\n",
    "client = CoreNLPClient(annotators=config['annotators'], memory=config['memory'], endpoint=config['endpoint'])\n",
    "\n",
    "document = client.annotate(text)\n",
    "for i, sentence in enumerate(document.sentence):\n",
    "    for triple in sentence.openieTriple:\n",
    "        triples.append([triple.subject, triple.relation, triple.object])\n",
    "        \n",
    "triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "Most patients were initially certified for a 1:1 (∆9-tetrahydrocannabinol:cannabidiol) tincture.\n",
       "Eight-seven percent of patients (n = 60) were noted to exhibit an improvement in any PD symptom after starting MC.\n",
       "Symptoms with the highest incidence of improvement included cramping/dystonia, pain, spasticity, lack of appetite, dyskinesia, and tremor.\n",
       "After starting MC, 56% of opioid users (n = 14) were able to decrease or discontinue opioid use with an average daily morphine milligram equivalent change from 31 at baseline to 22 at the last follow-up visit.\n",
       "The MC was well-tolerated with no severe AEs reported and low rate of MC discontinuation due to AEs (n = 4).\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: 1:1 is type DATE - index_location: 28:31\n",
      "Entity: 1:1 is type DATE - index_location: 33:36\n",
      "Entity: 1:1 is type DATE - index_location: 43:46\n",
      "Entity: 1:1 is type DATE - index_location: 38:41\n",
      "Entity: Eight seven percent is type PERCENT - index_location: 0:19\n",
      "Entity: Eight is type CARDINAL - index_location: 0:5\n",
      "Entity: seven percent is type PERCENT - index_location: 6:19\n"
     ]
    }
   ],
   "source": [
    "## Analyze the triples for NER\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for i, triple in enumerate(triples):\n",
    "    doc = \" \".join(triple)\n",
    "    doc = nlp(doc)\n",
    "    for ent in doc.ents:\n",
    "        print(f'Entity: {ent.text} is type {ent.label_} - index_location: {ent.start_char}:{ent.end_char}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/media/james/Projects/GitHub/DATA_340_NLP/venv/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: PD is type MISC - index_location: 18:20\n",
      "Entity: PD is type MISC - index_location: 43:45\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner', model='dslim/bert-base-NER', tokenizer='dslim/bert-base-NER', grouped_entities=True)\n",
    "\n",
    "for i, triple in enumerate(triples):\n",
    "    doc = \" \".join(triple)\n",
    "    doc = ner(doc)\n",
    "    for ent in doc:\n",
    "        print(f'Entity: {ent[\"word\"]} is type {ent[\"entity_group\"]} - index_location: {ent[\"start\"]}:{ent[\"end\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization using Transformers\n",
    "\n",
    "There are two types of text summarization:\n",
    "\n",
    "* Extractive summarization: Extracting a subset of the original text to form the summary\n",
    "* Abstractive summarization: Generating new text to form the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive summarization\n",
    "\n",
    "There are several extractive summarization techniques:\n",
    "\n",
    "* LexRank - LexRank is a graph-based algorithm that uses the PageRank algorithm to rank sentences based on their similarity to other sentences in the text.\n",
    "* SentRank - SentRank is a graph-based algorithm that uses the PageRank algorithm to rank sentences based on their similarity to other sentences in the text.\n",
    "* Luhn - Uses TF-IDF to rank sentences based on their similarity to other sentences in the text.\n",
    "* SumBasic - Utilize the frequency of words in the text to rank sentences. (abstract-like)\n",
    "* KL-Sum - Kullback-Leibler divergence is used to rank sentences based on their similarity to other sentences in the text.\n",
    "* LSA - Latent semantic analysis or indexing uses singular value decomposition to compute matrices for analyzing relationships between sets of observations.\n",
    "* K-Means - K-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 1.12G/1.12G [00:15<00:00, 70.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import BloomTokenizerFast\n",
    "from transformers import BloomForCausalLM\n",
    "\n",
    "MODEL = BloomForCausalLM.from_pretrained('bigscience/bloom-560m')\n",
    "TOKENIZER = BloomTokenizerFast.from_pretrained('bigscience/bloom-560m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarization of our text using the Bloom model\n",
    "\n",
    "def summarize_text(text: str, tokenizer=TOKENIZER, min_output=40, max_output=100, max_length=80, model=MODEL):\n",
    "    \"\"\"Take a string of text and generate a summary\"\"\"\n",
    "    tokens_input = tokenizer.encode(\"summarize: \" + text, return_tensors='pt', max_length=max_length, truncation=True)\n",
    "    ids = model.generate(tokens_input, min_length=min_output, max_length=max_output)\n",
    "    summary = tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "summarize: \n",
       "Most patients were initially certified for a 1:1 (∆9-tetrahydrocannabinol:cannabidiol) tincture.\n",
       "Eight-seven percent of patients (n = 60) were noted to exhibit an improvement in any PD symptom after starting MC.\n",
       "Symptoms with the highest incidence of improvement included cramping/dystonia, pain, spasticity, and fatigue. The most common side effects were nausea, vomiting, and diarrhea."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary = summarize_text(text)\n",
    "display(HTML(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News\n",
    "\n",
    "What is fake news?\n",
    "\n",
    "* Fake news is a type of yellow journalism or propaganda that consists of deliberate misinformation or hoaxes spread via traditional print and broadcast news media or online social media.\n",
    "* Fake news can be published to intentionally or circumstantially damage the reputation of a person or entity, or make money through advertising revenue.\n",
    "* But ... fake news is not always false. The label can be used to discredit news that is critical of a person or organization, or to draw attention away from critical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake News Detection processing\n",
    "\n",
    "* Is it a news article?\n",
    "* Is there consensus on the truthfulness of the article?\n",
    "* If yes, return the consensus\n",
    "* If no, continue\n",
    "  * What is challenged in the article?\n",
    "    * Sentiment analysis - can shed light on the overall tone of the article (positive, negative, neutral) - heatmap of the article by paragraph or section\n",
    "    * Named entity recognition - can we identify the entities in the article (people, places, organizations, etc.)\n",
    "    * Can we perform semantic role labeling on the article?\n",
    "    * Are there references to other sources?\n",
    "\n",
    "\n",
    "\n",
    "adapted from Rothman, D. _Transformers for Natural Language Processing_. O'Reilly Media, Inc., 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using vectors to analyze sentence or document similarity\n",
    "\n",
    "#### Dot product\n",
    "\n",
    "The dot product or inner product of two vectors is defined as:\n",
    "\n",
    "$$ \\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i b_i $$\n",
    "\n",
    "\n",
    "With our vectors are defined as:\n",
    "\n",
    "$$ |v| = \\sqrt{\\sum^{N}_{i=1} v^2_i}$$\n",
    "\n",
    "* The longer the vector, the larger the magnitude\n",
    "* More frequent words will have larger magnitude\n",
    "* Raw dot product is not normalized - how can we use it to measure similarity?\n",
    "\n",
    "\n",
    "#### Normalized dot product\n",
    "\n",
    "$$ \\vec{a} \\cdot \\vec{b} = \\frac{\\sum_{i=1}^{n} a_i b_i}{\\sqrt{\\sum^{N}_{i=1} a^2_i} \\sqrt{\\sum^{N}_{i=1} b^2_i}} $$\n",
    "\n",
    "#### Cosine similarity\n",
    "\n",
    "With the cosine similarity, we can measure the angle between two vectors. The cosine similarity is defined as:\n",
    "\n",
    "$$ \\text{cosine(a, b)  = } \\frac{a\\cdot b}{|a||b|} = \\frac{\\sum_{i=1}^{n} a_i b_i}{\\sqrt{\\sum^{N}_{i=1} a^2_i} \\sqrt{\\sum^{N}_{i=1} b^2_i}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity of words\n",
    "\n",
    "https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin\n",
    "\n",
    "### Cosine similarity of sentences and documents\n",
    "\n",
    "How can we use the word vectors to measure the similarity between sentences or documents?\n",
    "\n",
    "* Average the word vectors in the sentence or document\n",
    "* Calculate the cosine similarity between the two sentences or documents\n",
    "* Train a classifier to predict the similarity between sentences or documents\n",
    "* Train a sentence embedding model to generate sentence or document vectors\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdm_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdbow_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdm_concat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdm_tag_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdv_mapfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshrink_windows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Serialize/deserialize objects from disk, by equipping them with the `save()` / `load()` methods.\n",
      "\n",
      "Warnings\n",
      "--------\n",
      "This uses pickle internally (among other techniques), so objects must not contain unpicklable attributes\n",
      "such as lambda functions etc.\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbow_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm_concat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdm_tag_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdv_mapfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshrink_windows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Class for training, using and evaluating neural networks described in\u001b[0m\n",
      "\u001b[0;34m        `Distributed Representations of Sentences and Documents <http://arxiv.org/abs/1405.4053v2>`_.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        documents : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\u001b[0m\n",
      "\u001b[0;34m            Input corpus, can be simply a list of elements, but for larger corpora,consider an iterable that streams\u001b[0m\n",
      "\u001b[0;34m            the documents directly from disk/network. If you don't supply `documents` (or `corpus_file`), the model is\u001b[0m\n",
      "\u001b[0;34m            left uninitialized -- use if you plan to initialize it in some other way.\u001b[0m\n",
      "\u001b[0;34m        corpus_file : str, optional\u001b[0m\n",
      "\u001b[0;34m            Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\u001b[0m\n",
      "\u001b[0;34m            You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\u001b[0m\n",
      "\u001b[0;34m            `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\u001b[0m\n",
      "\u001b[0;34m            Documents' tags are assigned automatically and are equal to line number, as in\u001b[0m\n",
      "\u001b[0;34m            :class:`~gensim.models.doc2vec.TaggedLineDocument`.\u001b[0m\n",
      "\u001b[0;34m        dm : {1,0}, optional\u001b[0m\n",
      "\u001b[0;34m            Defines the training algorithm. If `dm=1`, 'distributed memory' (PV-DM) is used.\u001b[0m\n",
      "\u001b[0;34m            Otherwise, `distributed bag of words` (PV-DBOW) is employed.\u001b[0m\n",
      "\u001b[0;34m        vector_size : int, optional\u001b[0m\n",
      "\u001b[0;34m            Dimensionality of the feature vectors.\u001b[0m\n",
      "\u001b[0;34m        window : int, optional\u001b[0m\n",
      "\u001b[0;34m            The maximum distance between the current and predicted word within a sentence.\u001b[0m\n",
      "\u001b[0;34m        alpha : float, optional\u001b[0m\n",
      "\u001b[0;34m            The initial learning rate.\u001b[0m\n",
      "\u001b[0;34m        min_alpha : float, optional\u001b[0m\n",
      "\u001b[0;34m            Learning rate will linearly drop to `min_alpha` as training progresses.\u001b[0m\n",
      "\u001b[0;34m        seed : int, optional\u001b[0m\n",
      "\u001b[0;34m            Seed for the random number generator. Initial vectors for each word are seeded with a hash of\u001b[0m\n",
      "\u001b[0;34m            the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\u001b[0m\n",
      "\u001b[0;34m            you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\u001b[0m\n",
      "\u001b[0;34m            from OS thread scheduling.\u001b[0m\n",
      "\u001b[0;34m            In Python 3, reproducibility between interpreter launches also requires use of the `PYTHONHASHSEED`\u001b[0m\n",
      "\u001b[0;34m            environment variable to control hash randomization.\u001b[0m\n",
      "\u001b[0;34m        min_count : int, optional\u001b[0m\n",
      "\u001b[0;34m            Ignores all words with total frequency lower than this.\u001b[0m\n",
      "\u001b[0;34m        max_vocab_size : int, optional\u001b[0m\n",
      "\u001b[0;34m            Limits the RAM during vocabulary building; if there are more unique\u001b[0m\n",
      "\u001b[0;34m            words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\u001b[0m\n",
      "\u001b[0;34m            Set to `None` for no limit.\u001b[0m\n",
      "\u001b[0;34m        sample : float, optional\u001b[0m\n",
      "\u001b[0;34m            The threshold for configuring which higher-frequency words are randomly downsampled,\u001b[0m\n",
      "\u001b[0;34m            useful range is (0, 1e-5).\u001b[0m\n",
      "\u001b[0;34m        workers : int, optional\u001b[0m\n",
      "\u001b[0;34m            Use these many worker threads to train the model (=faster training with multicore machines).\u001b[0m\n",
      "\u001b[0;34m        epochs : int, optional\u001b[0m\n",
      "\u001b[0;34m            Number of iterations (epochs) over the corpus. Defaults to 10 for Doc2Vec.\u001b[0m\n",
      "\u001b[0;34m        hs : {1,0}, optional\u001b[0m\n",
      "\u001b[0;34m            If 1, hierarchical softmax will be used for model training.\u001b[0m\n",
      "\u001b[0;34m            If set to 0, and `negative` is non-zero, negative sampling will be used.\u001b[0m\n",
      "\u001b[0;34m        negative : int, optional\u001b[0m\n",
      "\u001b[0;34m            If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\u001b[0m\n",
      "\u001b[0;34m            should be drawn (usually between 5-20).\u001b[0m\n",
      "\u001b[0;34m            If set to 0, no negative sampling is used.\u001b[0m\n",
      "\u001b[0;34m        ns_exponent : float, optional\u001b[0m\n",
      "\u001b[0;34m            The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\u001b[0m\n",
      "\u001b[0;34m            to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\u001b[0m\n",
      "\u001b[0;34m            than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\u001b[0m\n",
      "\u001b[0;34m            More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\u001b[0m\n",
      "\u001b[0;34m            other values may perform better for recommendation applications.\u001b[0m\n",
      "\u001b[0;34m        dm_mean : {1,0}, optional\u001b[0m\n",
      "\u001b[0;34m            If 0, use the sum of the context word vectors. If 1, use the mean.\u001b[0m\n",
      "\u001b[0;34m            Only applies when `dm` is used in non-concatenative mode.\u001b[0m\n",
      "\u001b[0;34m        dm_concat : {1,0}, optional\u001b[0m\n",
      "\u001b[0;34m            If 1, use concatenation of context vectors rather than sum/average;\u001b[0m\n",
      "\u001b[0;34m            Note concatenation results in a much-larger model, as the input\u001b[0m\n",
      "\u001b[0;34m            is no longer the size of one (sampled or arithmetically combined) word vector, but the\u001b[0m\n",
      "\u001b[0;34m            size of the tag(s) and all words in the context strung together.\u001b[0m\n",
      "\u001b[0;34m        dm_tag_count : int, optional\u001b[0m\n",
      "\u001b[0;34m            Expected constant number of document tags per document, when using\u001b[0m\n",
      "\u001b[0;34m            dm_concat mode.\u001b[0m\n",
      "\u001b[0;34m        dbow_words : {1,0}, optional\u001b[0m\n",
      "\u001b[0;34m            If set to 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW\u001b[0m\n",
      "\u001b[0;34m            doc-vector training; If 0, only trains doc-vectors (faster).\u001b[0m\n",
      "\u001b[0;34m        trim_rule : function, optional\u001b[0m\n",
      "\u001b[0;34m            Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\u001b[0m\n",
      "\u001b[0;34m            be trimmed away, or handled using the default (discard if word count < min_count).\u001b[0m\n",
      "\u001b[0;34m            Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\u001b[0m\n",
      "\u001b[0;34m            or a callable that accepts parameters (word, count, min_count) and returns either\u001b[0m\n",
      "\u001b[0;34m            :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\u001b[0m\n",
      "\u001b[0;34m            The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\u001b[0m\n",
      "\u001b[0;34m            of the model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            The input parameters are of the following types:\u001b[0m\n",
      "\u001b[0;34m                * `word` (str) - the word we are examining\u001b[0m\n",
      "\u001b[0;34m                * `count` (int) - the word's frequency count in the corpus\u001b[0m\n",
      "\u001b[0;34m                * `min_count` (int) - the minimum count threshold.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\u001b[0m\n",
      "\u001b[0;34m            List of callbacks that need to be executed/run at specific stages during training.\u001b[0m\n",
      "\u001b[0;34m        shrink_windows : bool, optional\u001b[0m\n",
      "\u001b[0;34m            New in 4.1. Experimental.\u001b[0m\n",
      "\u001b[0;34m            If True, the effective window size is uniformly sampled from  [1, `window`]\u001b[0m\n",
      "\u001b[0;34m            for each target word during training, to match the original word2vec algorithm's\u001b[0m\n",
      "\u001b[0;34m            approximate weighting of context words by distance. Otherwise, the effective\u001b[0m\n",
      "\u001b[0;34m            window size is always fixed to `window` words to either side.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Some important internal attributes are the following:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Attributes\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\u001b[0m\n",
      "\u001b[0;34m            This object essentially contains the mapping between words and embeddings. After training, it can be used\u001b[0m\n",
      "\u001b[0;34m            directly to query those embeddings in various ways. See the module level docstring for examples.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        dv : :class:`~gensim.models.keyedvectors.KeyedVectors`\u001b[0m\n",
      "\u001b[0;34m            This object contains the paragraph vectors learned from the training data. There will be one such vector\u001b[0m\n",
      "\u001b[0;34m            for each unique document tag supplied during training. They may be individually accessed using the tag\u001b[0m\n",
      "\u001b[0;34m            as an indexed-access key. For example, if one of the training documents used a tag of 'doc003':\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            .. sourcecode:: pycon\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m                >>> model.dv['doc003']\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcorpus_iterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mdm_mean\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbow_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdm_mean\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbow_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbow_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm_concat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm_tag_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm_tag_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mdm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdm_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdm_tag_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using concatenative %d-dimensional layer1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdv\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdv_mapfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# EXPERIMENTAL lockf feature; create minimal no-op lockf arrays (1 element of 1.0)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# advanced users should directly resize/adjust as desired after any vocab growth\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_lockf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnull_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm_concat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mshrink_windows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshrink_windows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Indicates whether 'distributed memory' (PV-DM) will be used, else 'distributed bag of words'\u001b[0m\n",
      "\u001b[0;34m        (PV-DBOW) is used.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msg\u001b[0m  \u001b[0;31m# opposite of SG\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mdbow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Indicates whether 'distributed bag of words' (PV-DBOW) will be used, else 'distributed memory'\u001b[0m\n",
      "\u001b[0;34m        (PV-DM) is used.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msg\u001b[0m  \u001b[0;31m# same as SG\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The `docvecs` property has been renamed `dv`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The `docvecs` property has been renamed `dv`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mdocvecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_clear_post_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Resets the current word vectors. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# to not use an identical rnd stream as words, deterministically change seed (w/ 1000th prime)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m7919\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mreset_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Copy shareable data structures from another (possibly pre-trained) model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        This specifically causes some structures to be shared, so is limited to\u001b[0m\n",
      "\u001b[0;34m        structures (like those rleated to the known word/tag vocabularies) that\u001b[0m\n",
      "\u001b[0;34m        won't change during training or thereafter. Beware vocabulary edits/updates\u001b[0m\n",
      "\u001b[0;34m        to either model afterwards: the partial sharing and out-of-band modification\u001b[0m\n",
      "\u001b[0;34m        may leave the other model in a broken state.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        other_model : :class:`~gensim.models.doc2vec.Doc2Vec`\u001b[0m\n",
      "\u001b[0;34m            Other model whose internal data structures will be copied over to the current object.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandos\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcum_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcum_table\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpandos\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_do_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcython_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_private_mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_doctags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_private_mem\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdoctag_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdoctags_lockf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_lockf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstart_doctag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_doctags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_tally\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2v_train_epoch_dbow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_doctag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcython_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbow_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_tally\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2v_train_epoch_dm_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_doctag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcython_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_tally\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2v_train_epoch_dm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_doctag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcython_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_tally\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_do_train_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Train model using `job` data.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        job : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`\u001b[0m\n",
      "\u001b[0;34m            The corpus chunk to be used for training this batch.\u001b[0m\n",
      "\u001b[0;34m        alpha : float\u001b[0m\n",
      "\u001b[0;34m            Learning rate to be used for training this batch.\u001b[0m\n",
      "\u001b[0;34m        inits : (np.ndarray, np.ndarray)\u001b[0m\n",
      "\u001b[0;34m            Each worker threads private work memory.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        (int, int)\u001b[0m\n",
      "\u001b[0;34m             2-tuple (effective word count after ignoring unknown words and sentence length trimming, total word count).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtally\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdoctag_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdoctag_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdoctags_lockf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_lockf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtally\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_document_dbow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbow_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtally\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_document_dm_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtally\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_document_dm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mtally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_word_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Update the model's neural weights.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\u001b[0m\n",
      "\u001b[0;34m        progress-percentage logging, either `total_examples` (count of documents) or `total_words` (count of\u001b[0m\n",
      "\u001b[0;34m        raw words in documents) **MUST** be provided. If `documents` is the same corpus\u001b[0m\n",
      "\u001b[0;34m        that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\u001b[0m\n",
      "\u001b[0;34m        you can simply use `total_examples=self.corpus_count`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        To avoid common mistakes around the model's ability to do multiple training passes itself, an\u001b[0m\n",
      "\u001b[0;34m        explicit `epochs` argument **MUST** be provided. In the common and recommended case\u001b[0m\n",
      "\u001b[0;34m        where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once,\u001b[0m\n",
      "\u001b[0;34m        you can set `epochs=self.iter`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        corpus_iterable : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\u001b[0m\n",
      "\u001b[0;34m            Can be simply a list of elements, but for larger corpora,consider an iterable that streams\u001b[0m\n",
      "\u001b[0;34m            the documents directly from disk/network. If you don't supply `documents` (or `corpus_file`), the model is\u001b[0m\n",
      "\u001b[0;34m            left uninitialized -- use if you plan to initialize it in some other way.\u001b[0m\n",
      "\u001b[0;34m        corpus_file : str, optional\u001b[0m\n",
      "\u001b[0;34m            Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\u001b[0m\n",
      "\u001b[0;34m            You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\u001b[0m\n",
      "\u001b[0;34m            `corpus_file` arguments need to be passed (not both of them). Documents' tags are assigned automatically\u001b[0m\n",
      "\u001b[0;34m            and are equal to line number, as in :class:`~gensim.models.doc2vec.TaggedLineDocument`.\u001b[0m\n",
      "\u001b[0;34m        total_examples : int, optional\u001b[0m\n",
      "\u001b[0;34m            Count of documents.\u001b[0m\n",
      "\u001b[0;34m        total_words : int, optional\u001b[0m\n",
      "\u001b[0;34m            Count of raw words in documents.\u001b[0m\n",
      "\u001b[0;34m        epochs : int, optional\u001b[0m\n",
      "\u001b[0;34m            Number of iterations (epochs) over the corpus.\u001b[0m\n",
      "\u001b[0;34m        start_alpha : float, optional\u001b[0m\n",
      "\u001b[0;34m            Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\u001b[0m\n",
      "\u001b[0;34m            for this one call to `train`.\u001b[0m\n",
      "\u001b[0;34m            Use only if making multiple calls to `train`, when you want to manage the alpha learning-rate yourself\u001b[0m\n",
      "\u001b[0;34m            (not recommended).\u001b[0m\n",
      "\u001b[0;34m        end_alpha : float, optional\u001b[0m\n",
      "\u001b[0;34m            Final learning rate. Drops linearly from `start_alpha`.\u001b[0m\n",
      "\u001b[0;34m            If supplied, this replaces the final `min_alpha` from the constructor, for this one call to\u001b[0m\n",
      "\u001b[0;34m            :meth:`~gensim.models.doc2vec.Doc2Vec.train`.\u001b[0m\n",
      "\u001b[0;34m            Use only if making multiple calls to :meth:`~gensim.models.doc2vec.Doc2Vec.train`, when you want to manage\u001b[0m\n",
      "\u001b[0;34m            the alpha learning-rate yourself (not recommended).\u001b[0m\n",
      "\u001b[0;34m        word_count : int, optional\u001b[0m\n",
      "\u001b[0;34m            Count of words already trained. Set this to 0 for the usual\u001b[0m\n",
      "\u001b[0;34m            case of training on all words in documents.\u001b[0m\n",
      "\u001b[0;34m        queue_factor : int, optional\u001b[0m\n",
      "\u001b[0;34m            Multiplier for size of queue (number of workers * queue_factor).\u001b[0m\n",
      "\u001b[0;34m        report_delay : float, optional\u001b[0m\n",
      "\u001b[0;34m            Seconds to wait before reporting progress.\u001b[0m\n",
      "\u001b[0;34m        callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\u001b[0m\n",
      "\u001b[0;34m            List of callbacks that need to be executed/run at specific stages during training.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Either one of corpus_file or corpus_iterable value must be provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both corpus_file and corpus_iterable must not be provided at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parameter corpus_file must be a valid path to a file, got %r instead\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"corpus_iterable must be an iterable of TaggedDocument, got %r instead\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Calculate offsets for each worker along with initial doctags (doctag ~ document/line number in a file)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0moffsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_doctags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_offsets_and_start_doctags_for_corpusfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'offsets'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_doctags'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_doctags\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_get_offsets_and_start_doctags_for_corpusfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get offset and initial document tag in a corpus_file for each worker.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Firstly, approximate offsets are calculated based on number of workers and corpus_file size.\u001b[0m\n",
      "\u001b[0;34m        Secondly, for each approximate offset we find the maximum offset which points to the beginning of line and\u001b[0m\n",
      "\u001b[0;34m        less than approximate offset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        corpus_file : str\u001b[0m\n",
      "\u001b[0;34m            Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\u001b[0m\n",
      "\u001b[0;34m        workers : int\u001b[0m\n",
      "\u001b[0;34m            Number of workers.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        list of int, list of int\u001b[0m\n",
      "\u001b[0;34m            Lists with offsets and document tags with length = number of workers.\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcorpus_file_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mapprox_offsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstart_doctags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcurr_offset_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mprev_filepos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mcurr_offset_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapprox_offsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mcurr_filepos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_filepos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mwhile\u001b[0m \u001b[0mcurr_offset_idx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapprox_offsets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mapprox_offsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_offset_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcurr_filepos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0moffsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_filepos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mstart_doctags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mcurr_offset_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mprev_filepos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_filepos\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_doctags\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_raw_word_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get the number of words in a given job.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        job : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`\u001b[0m\n",
      "\u001b[0;34m            Corpus chunk.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        int\u001b[0m\n",
      "\u001b[0;34m            Number of raw words in the corpus chunk.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mestimated_lookup_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get estimated memory for tag lookup, 0 if using pure int tags.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        int\u001b[0m\n",
      "\u001b[0;34m            The estimated RAM required to look up a tag in bytes.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m140\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Infer a vector for given post-bulk training document.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Notes\u001b[0m\n",
      "\u001b[0;34m        -----\u001b[0m\n",
      "\u001b[0;34m        Subsequent calls to this function may infer different representations for the same document.\u001b[0m\n",
      "\u001b[0;34m        For a more stable representation, increase the number of epochs to assert a stricter convergence.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        doc_words : list of str\u001b[0m\n",
      "\u001b[0;34m            A document for which the vector representation will be inferred.\u001b[0m\n",
      "\u001b[0;34m        alpha : float, optional\u001b[0m\n",
      "\u001b[0;34m            The initial learning rate. If unspecified, value from model initialization will be reused.\u001b[0m\n",
      "\u001b[0;34m        min_alpha : float, optional\u001b[0m\n",
      "\u001b[0;34m            Learning rate will linearly drop to `min_alpha` over all inference epochs. If unspecified,\u001b[0m\n",
      "\u001b[0;34m            value from model initialization will be reused.\u001b[0m\n",
      "\u001b[0;34m        epochs : int, optional\u001b[0m\n",
      "\u001b[0;34m            Number of times to train the new document. Larger values take more time, but may improve\u001b[0m\n",
      "\u001b[0;34m            quality and run-to-run stability of inferred vectors. If unspecified, the `epochs` value\u001b[0m\n",
      "\u001b[0;34m            from model initialization will be reused.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        np.ndarray\u001b[0m\n",
      "\u001b[0;34m            The inferred paragraph vector for the new document.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a common mistake; fail with a nicer error\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parameter doc_words of infer_vector() must be a list of strings (not a single string).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_alpha\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdoctag_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseudorandom_weak_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdoctag_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdoctags_lockf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdoctag_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mneu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_aligned\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0malpha_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtrain_document_dbow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mlearn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtrain_document_dm_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mlearn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtrain_document_dm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneu1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mlearn_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoctags_lockf\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0malpha\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha_delta\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get the vector representation of (possibly multi-term) tag.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        tag : {str, int, list of str, list of int}\u001b[0m\n",
      "\u001b[0;34m            The tag (or tags) to be looked up in the model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        np.ndarray\u001b[0m\n",
      "\u001b[0;34m            The vector representations of each tag as a matrix (will be 1D if `tag` was a single tag)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Abbreviated name reflecting major configuration parameters.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        str\u001b[0m\n",
      "\u001b[0;34m            Human readable representation of the models internal state.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbow_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dbow+w'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# also training words\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dbow'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# PV-DBOW (skip-gram-style)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# PV-DM...\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdm_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dm/c'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ...with concatenative context layer\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dm/m'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dm/s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'd%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# dimensions\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# negative samples\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msg\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbow_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# window size, when relevant\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mc%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's%g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m'%s<%s>'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoctag_vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'*dt_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Store the input-hidden weight matrix in the same format used by the original C word2vec-tool.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        fname : str\u001b[0m\n",
      "\u001b[0;34m            The file path used to save the vectors in.\u001b[0m\n",
      "\u001b[0;34m        doctag_vec : bool, optional\u001b[0m\n",
      "\u001b[0;34m            Indicates whether to store document vectors.\u001b[0m\n",
      "\u001b[0;34m        word_vec : bool, optional\u001b[0m\n",
      "\u001b[0;34m            Indicates whether to store word vectors.\u001b[0m\n",
      "\u001b[0;34m        prefix : str, optional\u001b[0m\n",
      "\u001b[0;34m            Uniquely identifies doctags from word vocab, and avoids collision in case of repeated string in doctag\u001b[0m\n",
      "\u001b[0;34m            and word vocab.\u001b[0m\n",
      "\u001b[0;34m        fvocab : str, optional\u001b[0m\n",
      "\u001b[0;34m            Optional file path used to save the vocabulary.\u001b[0m\n",
      "\u001b[0;34m        binary : bool, optional\u001b[0m\n",
      "\u001b[0;34m            If True, the data will be saved in binary word2vec format, otherwise - will be saved in plain text.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtotal_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# save word vectors\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mdoctag_vec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mtotal_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# save document vectors\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mdoctag_vec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwrite_header\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# simply appending to existing file\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mwrite_header\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mwrite_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwrite_header\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0msort_attr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'doc_count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"init_sims() is now obsoleted and will be completely removed in future versions. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m        Precompute L2-normalized vectors. Obsoleted.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        If you need a single unit-normalized vector for some key, call\u001b[0m\n",
      "\u001b[0;34m        :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\u001b[0m\n",
      "\u001b[0;34m        ``doc2vec_model.dv.get_vector(key, norm=True)``.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        To refresh norms after you performed some atypical out-of-band vector tampering,\u001b[0m\n",
      "\u001b[0;34m        call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        replace : bool\u001b[0m\n",
      "\u001b[0;34m            If True, forget the original trained vectors and only keep the normalized ones.\u001b[0m\n",
      "\u001b[0;34m            You lose information if you do this.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Load a previously saved :class:`~gensim.models.doc2vec.Doc2Vec` model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        fname : str\u001b[0m\n",
      "\u001b[0;34m            Path to the saved file.\u001b[0m\n",
      "\u001b[0;34m        *args : object\u001b[0m\n",
      "\u001b[0;34m            Additional arguments, see `~gensim.models.word2vec.Word2Vec.load`.\u001b[0m\n",
      "\u001b[0;34m        **kwargs : object\u001b[0m\n",
      "\u001b[0;34m            Additional arguments, see `~gensim.models.word2vec.Word2Vec.load`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        See Also\u001b[0m\n",
      "\u001b[0;34m        --------\u001b[0m\n",
      "\u001b[0;34m        :meth:`~gensim.models.doc2vec.Doc2Vec.save`\u001b[0m\n",
      "\u001b[0;34m            Save :class:`~gensim.models.doc2vec.Doc2Vec` model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        :class:`~gensim.models.doc2vec.Doc2Vec`\u001b[0m\n",
      "\u001b[0;34m            Loaded model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrethrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Model load error. Was model saved using code from an older Gensim version? \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Try loading older model using gensim-3.8.3, then re-saving, to restore \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"compatibility with current code.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Estimate required memory for a model using current settings.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        vocab_size : int, optional\u001b[0m\n",
      "\u001b[0;34m            Number of raw words in the vocabulary.\u001b[0m\n",
      "\u001b[0;34m        report : dict of (str, int), optional\u001b[0m\n",
      "\u001b[0;34m            A dictionary from string representations of the **specific** model's memory consuming members\u001b[0m\n",
      "\u001b[0;34m            to their size in bytes.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        dict of (str, int), optional\u001b[0m\n",
      "\u001b[0;34m            A dictionary from string representations of the model's memory consuming members to their size in bytes.\u001b[0m\n",
      "\u001b[0;34m            Includes members from the base classes as well as weights and tag lookup memory estimation specific to the\u001b[0m\n",
      "\u001b[0;34m            class.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doctag_lookup'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimated_lookup_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doctag_syn0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Build vocabulary from a sequence of documents (can be a once-only generator stream).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        documents : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\u001b[0m\n",
      "\u001b[0;34m            Can be simply a list of :class:`~gensim.models.doc2vec.TaggedDocument` elements, but for larger corpora,\u001b[0m\n",
      "\u001b[0;34m            consider an iterable that streams the documents directly from disk/network.\u001b[0m\n",
      "\u001b[0;34m            See :class:`~gensim.models.doc2vec.TaggedBrownCorpus` or :class:`~gensim.models.doc2vec.TaggedLineDocument`\u001b[0m\n",
      "\u001b[0;34m        corpus_file : str, optional\u001b[0m\n",
      "\u001b[0;34m            Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\u001b[0m\n",
      "\u001b[0;34m            You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\u001b[0m\n",
      "\u001b[0;34m            `corpus_file` arguments need to be passed (not both of them). Documents' tags are assigned automatically\u001b[0m\n",
      "\u001b[0;34m            and are equal to a line number, as in :class:`~gensim.models.doc2vec.TaggedLineDocument`.\u001b[0m\n",
      "\u001b[0;34m        update : bool\u001b[0m\n",
      "\u001b[0;34m            If true, the new words in `documents` will be added to model's vocab.\u001b[0m\n",
      "\u001b[0;34m        progress_per : int\u001b[0m\n",
      "\u001b[0;34m            Indicates how many words to process before showing/updating the progress.\u001b[0m\n",
      "\u001b[0;34m        keep_raw_vocab : bool\u001b[0m\n",
      "\u001b[0;34m            If not true, delete the raw vocabulary after the scaling is done and free up RAM.\u001b[0m\n",
      "\u001b[0;34m        trim_rule : function, optional\u001b[0m\n",
      "\u001b[0;34m            Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\u001b[0m\n",
      "\u001b[0;34m            be trimmed away, or handled using the default (discard if word count < min_count).\u001b[0m\n",
      "\u001b[0;34m            Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\u001b[0m\n",
      "\u001b[0;34m            or a callable that accepts parameters (word, count, min_count) and returns either\u001b[0m\n",
      "\u001b[0;34m            :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\u001b[0m\n",
      "\u001b[0;34m            The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\u001b[0m\n",
      "\u001b[0;34m            of the model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            The input parameters are of the following types:\u001b[0m\n",
      "\u001b[0;34m                * `word` (str) - the word we are examining\u001b[0m\n",
      "\u001b[0;34m                * `count` (int) - the word's frequency count in the corpus\u001b[0m\n",
      "\u001b[0;34m                * `min_count` (int) - the minimum count threshold.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        **kwargs\u001b[0m\n",
      "\u001b[0;34m            Additional key word arguments passed to the internal vocabulary construction.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreport_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vocab_from_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Build vocabulary from a dictionary of word frequencies.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Build model vocabulary from a passed dictionary that contains a (word -> word count) mapping.\u001b[0m\n",
      "\u001b[0;34m        Words must be of type unicode strings.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        word_freq : dict of (str, int)\u001b[0m\n",
      "\u001b[0;34m            Word <-> count mapping.\u001b[0m\n",
      "\u001b[0;34m        keep_raw_vocab : bool, optional\u001b[0m\n",
      "\u001b[0;34m            If not true, delete the raw vocabulary after the scaling is done and free up RAM.\u001b[0m\n",
      "\u001b[0;34m        corpus_count : int, optional\u001b[0m\n",
      "\u001b[0;34m            Even if no corpus is provided, this argument can set corpus_count explicitly.\u001b[0m\n",
      "\u001b[0;34m        trim_rule : function, optional\u001b[0m\n",
      "\u001b[0;34m            Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\u001b[0m\n",
      "\u001b[0;34m            be trimmed away, or handled using the default (discard if word count < min_count).\u001b[0m\n",
      "\u001b[0;34m            Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\u001b[0m\n",
      "\u001b[0;34m            or a callable that accepts parameters (word, count, min_count) and returns either\u001b[0m\n",
      "\u001b[0;34m            :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\u001b[0m\n",
      "\u001b[0;34m            The rule, if given, is only used to prune vocabulary during\u001b[0m\n",
      "\u001b[0;34m            :meth:`~gensim.models.doc2vec.Doc2Vec.build_vocab` and is not stored as part of the model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            The input parameters are of the following types:\u001b[0m\n",
      "\u001b[0;34m                * `word` (str) - the word we are examining\u001b[0m\n",
      "\u001b[0;34m                * `count` (int) - the word's frequency count in the corpus\u001b[0m\n",
      "\u001b[0;34m                * `min_count` (int) - the minimum count threshold.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        update : bool, optional\u001b[0m\n",
      "\u001b[0;34m            If true, the new provided words in `word_freq` dict will be added to model's vocab.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"processing provided word frequencies\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Instead of scanning text, this will assign provided word frequencies dictionary(word_freq)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# to be directly the raw vocab.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mraw_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_freq\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"collected %i different raw words, with total frequency of %i\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Since no documents are provided, this is to control the corpus_count\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_vocab\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreport_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdocument_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtotal_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmin_reduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minterval_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.00001\u001b[0m  \u001b[0;31m# guard against next sample being identical\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minterval_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mchecked_string_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmax_rawint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# highest raw int tag seen (-1 for none)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdoctags_lookup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdoctags_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;34m\"Each 'words' should be a list of words (usually unicode strings). \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;34m\"First 'words' here is instead plain %s.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mchecked_string_types\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mdocument_no\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprogress_per\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0minterval_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_words\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minterval_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minterval_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"PROGRESS: at example #%i, processed %i words (%i words/s), %i word types, %i tags\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdocument_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoctags_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0minterval_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0minterval_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdocument_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# Note a document tag during initial corpus scan, for structure sizing.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mmax_rawint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoctags_lookup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mdoctags_lookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mdoctags_lookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdocument_length\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mdoctags_lookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoctag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoctags_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mdoctags_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_vocab_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_vocab_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_reduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mmin_reduce\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_no\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoctags_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"More unique tags (%i) than documents (%i).\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoctags_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmax_rawint\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Highest int doctag (%i) larger than count of documents (%i). This means \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"at least %i excess, unused slots (%i bytes) will be allocated for vectors.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mmax_rawint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rawint\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m(\u001b[0m\u001b[0mmax_rawint\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmax_rawint\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# adjust indexes/list to account for range of pure-int keyed doctags\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoctags_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdoctags_lookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoctags_lookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_rawint\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdoctags_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rawint\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdoctags_list\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoctags_list\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoctags_lookup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_vecattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_vecattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doc_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Create the model's vocabulary: a mapping from unique words in the corpus to their frequency count.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        documents : iterable of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\u001b[0m\n",
      "\u001b[0;34m            The tagged documents used to create the vocabulary. Their tags can be either str tokens or ints (faster).\u001b[0m\n",
      "\u001b[0;34m        corpus_file : str, optional\u001b[0m\n",
      "\u001b[0;34m            Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\u001b[0m\n",
      "\u001b[0;34m            You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\u001b[0m\n",
      "\u001b[0;34m            `corpus_file` arguments need to be passed (not both of them).\u001b[0m\n",
      "\u001b[0;34m        progress_per : int\u001b[0m\n",
      "\u001b[0;34m            Progress will be logged every `progress_per` documents.\u001b[0m\n",
      "\u001b[0;34m        trim_rule : function, optional\u001b[0m\n",
      "\u001b[0;34m            Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\u001b[0m\n",
      "\u001b[0;34m            be trimmed away, or handled using the default (discard if word count < min_count).\u001b[0m\n",
      "\u001b[0;34m            Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\u001b[0m\n",
      "\u001b[0;34m            or a callable that accepts parameters (word, count, min_count) and returns either\u001b[0m\n",
      "\u001b[0;34m            :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\u001b[0m\n",
      "\u001b[0;34m            The rule, if given, is only used to prune vocabulary during\u001b[0m\n",
      "\u001b[0;34m            :meth:`~gensim.models.doc2vec.Doc2Vec.build_vocab` and is not stored as part of the model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m            The input parameters are of the following types:\u001b[0m\n",
      "\u001b[0;34m                * `word` (str) - the word we are examining\u001b[0m\n",
      "\u001b[0;34m                * `count` (int) - the word's frequency count in the corpus\u001b[0m\n",
      "\u001b[0;34m                * `min_count` (int) - the minimum count threshold.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        (int, int)\u001b[0m\n",
      "\u001b[0;34m            Tuple of `(total words in the corpus, number of documents)`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collecting all words and their counts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mcorpus_iterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaggedLineDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"collected %i word types and %i unique tags from a corpus of %i examples and %i words\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msimilarity_unseen_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_words1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_words2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Compute cosine similarity between two post-bulk out of training documents.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        model : :class:`~gensim.models.doc2vec.Doc2Vec`\u001b[0m\n",
      "\u001b[0;34m            An instance of a trained `Doc2Vec` model.\u001b[0m\n",
      "\u001b[0;34m        doc_words1 : list of str\u001b[0m\n",
      "\u001b[0;34m            Input document.\u001b[0m\n",
      "\u001b[0;34m        doc_words2 : list of str\u001b[0m\n",
      "\u001b[0;34m            Input document.\u001b[0m\n",
      "\u001b[0;34m        alpha : float, optional\u001b[0m\n",
      "\u001b[0;34m            The initial learning rate.\u001b[0m\n",
      "\u001b[0;34m        min_alpha : float, optional\u001b[0m\n",
      "\u001b[0;34m            Learning rate will linearly drop to `min_alpha` as training progresses.\u001b[0m\n",
      "\u001b[0;34m        epochs : int, optional\u001b[0m\n",
      "\u001b[0;34m            Number of epoch to train the new document.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        float\u001b[0m\n",
      "\u001b[0;34m            The cosine similarity between `doc_words1` and `doc_words2`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_words1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_words2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           /media/james/Projects/GitHub/DATA_340_NLP/venv/lib/python3.10/site-packages/gensim/models/doc2vec.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "Doc2Vec??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07645398, -0.05182466, -0.08459883, -0.09623718,  0.06788807],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector([\"system\", \"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politicususa</td>\n",
       "      <td>Prosecutors Pay Attention: Stormy Daniels Than...</td>\n",
       "      <td>Manhattan prosecutors are likely to notice tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politicususa</td>\n",
       "      <td>Investigators Push For Access To Trump Staff C...</td>\n",
       "      <td>Print\\nInvestigators looking into Donald Trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politicususa</td>\n",
       "      <td>The End Is Near For George Santos As He Steps ...</td>\n",
       "      <td>The AP reported:\\nRepublican Rep. George Santo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politicususa</td>\n",
       "      <td>Rachel Maddow Cuts Trump To The Bone With Stor...</td>\n",
       "      <td>Rachel Maddow showed how Trump committed a cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vox</td>\n",
       "      <td>Alec Baldwin has been formally charged with in...</td>\n",
       "      <td>Candles are placed in front of a photo of cine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         source                                              title  \\\n",
       "0  politicususa  Prosecutors Pay Attention: Stormy Daniels Than...   \n",
       "1  politicususa  Investigators Push For Access To Trump Staff C...   \n",
       "2  politicususa  The End Is Near For George Santos As He Steps ...   \n",
       "3  politicususa  Rachel Maddow Cuts Trump To The Bone With Stor...   \n",
       "4           vox  Alec Baldwin has been formally charged with in...   \n",
       "\n",
       "                                                text  \n",
       "0  Manhattan prosecutors are likely to notice tha...  \n",
       "1  Print\\nInvestigators looking into Donald Trump...  \n",
       "2  The AP reported:\\nRepublican Rep. George Santo...  \n",
       "3  Rachel Maddow showed how Trump committed a cri...  \n",
       "4  Candles are placed in front of a photo of cine...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "df = pd.read_csv('../datasets/news-2023-02-01.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df['title'].tolist()\n",
    "titles = [title for title in titles if type(title) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([title.split() for title in titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Attention:': 0,\n",
       " 'Confirming': 1,\n",
       " 'Daniels': 2,\n",
       " 'For': 3,\n",
       " 'Her': 4,\n",
       " 'Pay': 5,\n",
       " 'Prosecutors': 6,\n",
       " 'Publicly': 7,\n",
       " 'Stormy': 8,\n",
       " 'Story': 9,\n",
       " 'Thanks': 10,\n",
       " 'Trump': 11,\n",
       " 'Access': 12,\n",
       " 'Computers': 13,\n",
       " 'Investigators': 14,\n",
       " 'Push': 15,\n",
       " 'Staff': 16,\n",
       " 'To': 17,\n",
       " 'As': 18,\n",
       " 'Committees': 19,\n",
       " 'Down': 20,\n",
       " 'End': 21,\n",
       " 'From': 22,\n",
       " 'George': 23,\n",
       " 'He': 24,\n",
       " 'Is': 25,\n",
       " 'Near': 26,\n",
       " 'Santos': 27,\n",
       " 'Steps': 28,\n",
       " 'The': 29,\n",
       " 'Analysis': 30,\n",
       " 'Bone': 31,\n",
       " 'Cuts': 32,\n",
       " 'Hush': 33,\n",
       " 'Maddow': 34,\n",
       " 'Money': 35,\n",
       " 'Rachel': 36,\n",
       " 'With': 37,\n",
       " '-': 38,\n",
       " 'Alec': 39,\n",
       " 'Baldwin': 40,\n",
       " 'Vox': 41,\n",
       " 'been': 42,\n",
       " 'charged': 43,\n",
       " 'formally': 44,\n",
       " 'has': 45,\n",
       " 'involuntary': 46,\n",
       " 'manslaughter': 47,\n",
       " 'with': 48,\n",
       " 'Google': 49,\n",
       " 'What': 50,\n",
       " 'and': 51,\n",
       " 'at': 52,\n",
       " 'companies': 53,\n",
       " 'for': 54,\n",
       " 'industries': 55,\n",
       " 'layoffs': 56,\n",
       " 'mean': 57,\n",
       " 'other': 58,\n",
       " 'tech': 59,\n",
       " 'Did': 60,\n",
       " 'Representative-elect': 61,\n",
       " 'Republican': 62,\n",
       " 'about': 63,\n",
       " 'his': 64,\n",
       " 'lie': 65,\n",
       " 'life': 66,\n",
       " 'story?': 67,\n",
       " '17': 68,\n",
       " '2023:': 69,\n",
       " 'Sundance': 70,\n",
       " 'movies': 71,\n",
       " 'out': 72,\n",
       " 'this': 73,\n",
       " 'to': 74,\n",
       " 'watch': 75,\n",
       " 'year': 76,\n",
       " 'Civil': 77,\n",
       " 'Deposition': 78,\n",
       " 'Fraud': 79,\n",
       " 'Give': 80,\n",
       " 'HuffPost': 81,\n",
       " 'In': 82,\n",
       " 'Investigation': 83,\n",
       " 'Latest': 84,\n",
       " 'New': 85,\n",
       " 'News': 86,\n",
       " 'Watch': 87,\n",
       " 'York': 88,\n",
       " '|': 89,\n",
       " 'DeSantis': 90,\n",
       " 'Defund': 91,\n",
       " 'Diversity': 92,\n",
       " 'Florida': 93,\n",
       " 'Plans': 94,\n",
       " 'Programs': 95,\n",
       " 'Ron': 96,\n",
       " 'Universities': 97,\n",
       " '13-Year-Old': 98,\n",
       " 'Boy': 99,\n",
       " 'Charged': 100,\n",
       " 'DC': 101,\n",
       " 'Employee': 102,\n",
       " 'Government': 103,\n",
       " 'Murder': 104,\n",
       " 'Of': 105,\n",
       " 'Biden': 106,\n",
       " 'Bungling': 107,\n",
       " 'Constitution': 108,\n",
       " 'Court': 109,\n",
       " 'On': 110,\n",
       " 'Pick': 111,\n",
       " 'Questions': 112,\n",
       " 'Republicans': 113,\n",
       " 'Rip': 114,\n",
       " 'Ahead': 115,\n",
       " \"Nichols'\": 116,\n",
       " 'Tyre': 117,\n",
       " 'a': 118,\n",
       " 'bodycam': 119,\n",
       " 'broken': 120,\n",
       " 'funeral,': 121,\n",
       " 'is': 122,\n",
       " 'of': 123,\n",
       " 'police': 124,\n",
       " 'promise': 125,\n",
       " 'video': 126,\n",
       " \"'spree'\": 127,\n",
       " 'another': 128,\n",
       " 'doing': 129,\n",
       " 'enough': 130,\n",
       " 'executions': 131,\n",
       " 'isn’t': 132,\n",
       " 'stop': 133,\n",
       " 'House': 134,\n",
       " 'Under': 135,\n",
       " 'aside': 136,\n",
       " 'committees': 137,\n",
       " 'from': 138,\n",
       " 'pressure,': 139,\n",
       " 'steps': 140,\n",
       " 'Arizona': 141,\n",
       " 'Kari': 142,\n",
       " 'Lake': 143,\n",
       " 'chief': 144,\n",
       " 'elections': 145,\n",
       " 'into': 146,\n",
       " 'investigation': 147,\n",
       " 'seeks': 148,\n",
       " 'tweet': 149,\n",
       " 'Budget': 150,\n",
       " 'Demands': 151,\n",
       " 'Details': 152,\n",
       " 'McCarthy': 153,\n",
       " 'Times': 154,\n",
       " 'on': 155,\n",
       " 'Arms': 156,\n",
       " 'Comply': 157,\n",
       " 'Fails': 158,\n",
       " 'Nuclear': 159,\n",
       " 'Russia': 160,\n",
       " 'START,': 161,\n",
       " 'Says': 162,\n",
       " 'Treaty': 163,\n",
       " 'U.S.': 164,\n",
       " '101': 165,\n",
       " 'Attack': 166,\n",
       " 'Dead': 167,\n",
       " 'Grapples': 168,\n",
       " 'Left': 169,\n",
       " 'Pakistan': 170,\n",
       " 'Terrorist': 171,\n",
       " 'That': 172,\n",
       " 'Aside': 173,\n",
       " 'Step': 174,\n",
       " 'Temporarily': 175,\n",
       " 'Will': 176,\n",
       " 'Post': 177,\n",
       " 'Washington': 178,\n",
       " 'funding': 179,\n",
       " 'heading': 180,\n",
       " 'major': 181,\n",
       " 'project': 182,\n",
       " 'rail': 183,\n",
       " 'tout': 184,\n",
       " 'tunnel': 185,\n",
       " 'Congress,': 186,\n",
       " 'Police': 187,\n",
       " 'are': 188,\n",
       " 'back': 189,\n",
       " 'but': 190,\n",
       " 'deal': 191,\n",
       " 'hope': 192,\n",
       " 'in': 193,\n",
       " 'little': 194,\n",
       " 'reform': 195,\n",
       " 'talks': 196,\n",
       " 'Democrats?': 197,\n",
       " 'Kagan': 198,\n",
       " 'Sotomayor': 199,\n",
       " 'Supreme': 200,\n",
       " 'open': 201,\n",
       " 'or': 202,\n",
       " 'seat': 203,\n",
       " 'the': 204,\n",
       " 'Santos’s': 205,\n",
       " 'by': 206,\n",
       " 'constituents': 207,\n",
       " 'him': 208,\n",
       " 'historic': 209,\n",
       " 'margin': 210,\n",
       " 'resign': 211,\n",
       " 'want': 212,\n",
       " 'Business': 213,\n",
       " 'CNN': 214,\n",
       " 'Covid': 215,\n",
       " 'Pfizer': 216,\n",
       " 'boom': 217,\n",
       " 'over': 218,\n",
       " 'sales': 219,\n",
       " 'Bill': 220,\n",
       " 'Gates': 221,\n",
       " \"Here's\": 222,\n",
       " 'Video:': 223,\n",
       " \"didn't\": 224,\n",
       " 'during': 225,\n",
       " 'pandemic': 226,\n",
       " 'protocols': 227,\n",
       " 'thinks': 228,\n",
       " 'what': 229,\n",
       " 'work': 230,\n",
       " 'Adult': 231,\n",
       " 'dramatically': 232,\n",
       " 'dropped': 233,\n",
       " 'drug': 234,\n",
       " 'pandemic,': 235,\n",
       " 'rose': 236,\n",
       " 'says': 237,\n",
       " 'study': 238,\n",
       " 'use': 239,\n",
       " 'youth,': 240,\n",
       " 'Amtrak': 241,\n",
       " \"Biden's\": 242,\n",
       " 'Politics': 243,\n",
       " 'coming': 244,\n",
       " 'improvements': 245,\n",
       " 'infrastructure': 246,\n",
       " 'law': 247,\n",
       " 'thanks': 248,\n",
       " 'train': 249,\n",
       " '(Republican)': 250,\n",
       " 'Crashes': 251,\n",
       " 'Liz': 252,\n",
       " 'POLITICO': 253,\n",
       " 'Party': 254,\n",
       " 'Truss': 255,\n",
       " '2024': 256,\n",
       " 'Senate': 257,\n",
       " 'Trump’s': 258,\n",
       " 'bid': 259,\n",
       " 'cobbles': 260,\n",
       " 'new': 261,\n",
       " 'slow-rolling': 262,\n",
       " 'support': 263,\n",
       " 'together': 264,\n",
       " 'I': 265,\n",
       " 'Trump:': 266,\n",
       " 'got': 267,\n",
       " 'reelected': 268,\n",
       " 'snaps': 269,\n",
       " 'changes': 270,\n",
       " 'programs': 271,\n",
       " 'proposed': 272,\n",
       " 'targets': 273,\n",
       " 'university': 274,\n",
       " '‘ideological’': 275,\n",
       " 'ABC': 276,\n",
       " 'Coast': 277,\n",
       " 'Guard': 278,\n",
       " 'Man': 279,\n",
       " 'Puerto': 280,\n",
       " 'Rico:': 281,\n",
       " 'after': 282,\n",
       " 'apparently': 283,\n",
       " 'cliff': 284,\n",
       " 'dies': 285,\n",
       " 'falling': 286,\n",
       " 'off': 287,\n",
       " 'City': 288,\n",
       " 'touts': 289,\n",
       " 'Manhunt': 290,\n",
       " 'Oregon': 291,\n",
       " 'accused': 292,\n",
       " 'man': 293,\n",
       " 'torturing': 294,\n",
       " 'underway': 295,\n",
       " 'woman:': 296,\n",
       " 'Minnesota': 297,\n",
       " 'abortion': 298,\n",
       " 'bill': 299,\n",
       " 'broad': 300,\n",
       " 'governor': 301,\n",
       " 'rights': 302,\n",
       " 'signs': 303,\n",
       " 'HHS': 304,\n",
       " 'Hill': 305,\n",
       " 'Russian-backed': 306,\n",
       " 'US': 307,\n",
       " 'actively': 308,\n",
       " 'care': 309,\n",
       " 'hackers': 310,\n",
       " 'health': 311,\n",
       " 'sector,': 312,\n",
       " 'targeting': 313,\n",
       " 'warns': 314,\n",
       " 'DNC': 315,\n",
       " 'PAC': 316,\n",
       " 'Sanders': 317,\n",
       " 'banning': 318,\n",
       " 'primary': 319,\n",
       " 'spending': 320,\n",
       " 'super': 321,\n",
       " 'urges': 322,\n",
       " 'vote': 323,\n",
       " 'Maryland': 324,\n",
       " 'eye': 325,\n",
       " 'four-day': 326,\n",
       " 'lawmakers': 327,\n",
       " 'workweek': 328,\n",
       " '–': 329,\n",
       " '8': 330,\n",
       " 'ex-lawmakers': 331,\n",
       " 'make': 332,\n",
       " 'private': 333,\n",
       " 'quick': 334,\n",
       " 'sector': 335,\n",
       " 'shift': 336,\n",
       " 'Campaign': 337,\n",
       " 'First': 338,\n",
       " 'His': 339,\n",
       " 'Identity': 340,\n",
       " 'Public': 341,\n",
       " 'RealClearPolitics': 342,\n",
       " 'Stop': 343,\n",
       " 'Struggled': 344,\n",
       " 'Convergence': 345,\n",
       " 'Hunter': 346,\n",
       " 'Joe': 347,\n",
       " 'Scandal': 348,\n",
       " 'Be': 349,\n",
       " 'Economic': 350,\n",
       " 'Good': 351,\n",
       " 'May': 352,\n",
       " 'Not': 353,\n",
       " 'When': 354,\n",
       " '2023': 355,\n",
       " 'Being': 356,\n",
       " 'Book': 357,\n",
       " 'Cease': 358,\n",
       " 'Comic': 359,\n",
       " 'Davos': 360,\n",
       " 'How': 361,\n",
       " 'RealClearMarkets': 362,\n",
       " 'Villain': 363,\n",
       " 'Wrapped,': 364,\n",
       " 'Fox': 365,\n",
       " 'approval': 366,\n",
       " 'below': 367,\n",
       " 'far': 368,\n",
       " 'most': 369,\n",
       " 'presidents': 370,\n",
       " 'ratings': 371,\n",
       " 'recent': 372,\n",
       " 'remain': 373,\n",
       " 'underwater,': 374,\n",
       " 'well': 375,\n",
       " 'COVID': 376,\n",
       " 'White': 377,\n",
       " 'emergency': 378,\n",
       " 'end': 379,\n",
       " 'ignores': 380,\n",
       " 'objections,': 381,\n",
       " 'votes': 382,\n",
       " 'Capitol': 383,\n",
       " 'Christmas': 384,\n",
       " 'Virginia': 385,\n",
       " 'West': 386,\n",
       " 'forest': 387,\n",
       " 'national': 388,\n",
       " 'provide': 389,\n",
       " 'tree': 390,\n",
       " 'Black': 391,\n",
       " 'California': 392,\n",
       " 'History': 393,\n",
       " 'Lives': 394,\n",
       " 'Matter': 395,\n",
       " 'district': 396,\n",
       " 'flag': 397,\n",
       " 'month': 398,\n",
       " 'raise': 399,\n",
       " 'school': 400,\n",
       " 'Bemoans': 401,\n",
       " 'Caller': 402,\n",
       " 'Daily': 403,\n",
       " 'Empire’': 404,\n",
       " 'Learn': 405,\n",
       " 'Maps': 406,\n",
       " 'Parse': 407,\n",
       " 'Politico': 408,\n",
       " 'Songhai': 409,\n",
       " 'Students': 410,\n",
       " 'Won’t': 411,\n",
       " 'Writer': 412,\n",
       " '‘How': 413,\n",
       " 'ATF': 414,\n",
       " 'After': 415,\n",
       " 'Effect': 416,\n",
       " 'Lawsuits': 417,\n",
       " 'Multiple': 418,\n",
       " 'Pistol': 419,\n",
       " 'Rule': 420,\n",
       " 'Slapped': 421,\n",
       " 'Takes': 422,\n",
       " 'And': 423,\n",
       " 'Are': 424,\n",
       " 'HBO’s': 425,\n",
       " 'Hit': 426,\n",
       " 'Last': 427,\n",
       " 'Newest': 428,\n",
       " 'Ratings': 429,\n",
       " 'Smash': 430,\n",
       " 'Us,’': 431,\n",
       " 'WOW': 432,\n",
       " '‘The': 433,\n",
       " '$100': 434,\n",
       " 'Annual': 435,\n",
       " 'Billion': 436,\n",
       " 'Earnings': 437,\n",
       " 'Pfizer’s': 438,\n",
       " 'Treatments': 439,\n",
       " \"'Zombie\": 440,\n",
       " 'Announces': 441,\n",
       " 'CRT': 442,\n",
       " 'Destroy': 443,\n",
       " 'More': 444,\n",
       " 'No': 445,\n",
       " 'Proposal': 446,\n",
       " \"Studies,'\": 447,\n",
       " 'Bashing': 448,\n",
       " 'Bernie': 449,\n",
       " 'But': 450,\n",
       " 'Capitalism,': 451,\n",
       " 'Cash': 452,\n",
       " 'Event': 453,\n",
       " \"He'll\": 454,\n",
       " 'Hosting': 455,\n",
       " 'Millionaire': 456,\n",
       " 'Sure': 457,\n",
       " 'Bible': 458,\n",
       " 'Biggest': 459,\n",
       " 'Bowl-Bound': 460,\n",
       " 'Career': 461,\n",
       " 'Cites': 462,\n",
       " 'Game': 463,\n",
       " 'Hurts': 464,\n",
       " 'Jalen': 465,\n",
       " 'Prepares': 466,\n",
       " 'QB': 467,\n",
       " 'Super': 468,\n",
       " 'Verse': 469,\n",
       " 'as': 470,\n",
       " 'Another': 471,\n",
       " 'Combusts': 472,\n",
       " 'Company': 473,\n",
       " 'Critical': 474,\n",
       " 'Has': 475,\n",
       " 'Lessons': 476,\n",
       " \"Musk's\": 477,\n",
       " 'Solomon:': 478,\n",
       " 'Some': 479,\n",
       " 'Spontaneously': 480,\n",
       " 'Tesla': 481,\n",
       " '11': 482,\n",
       " '42': 483,\n",
       " 'Barrier': 484,\n",
       " 'Border': 485,\n",
       " 'Title': 486,\n",
       " '77K': 487,\n",
       " 'Aliens': 488,\n",
       " 'Democrats': 489,\n",
       " \"Driver's\": 490,\n",
       " 'Illegal': 491,\n",
       " 'Licenses': 492,\n",
       " 'Move': 493,\n",
       " 'Chicago': 494,\n",
       " 'Effort': 495,\n",
       " 'Foreign': 496,\n",
       " 'H-1B': 497,\n",
       " 'Industry': 498,\n",
       " 'Jobs': 499,\n",
       " 'Laid': 500,\n",
       " 'Launches': 501,\n",
       " 'Off': 502,\n",
       " 'Score': 503,\n",
       " 'Tech': 504,\n",
       " 'Visa': 505,\n",
       " 'Workers': 506,\n",
       " '9': 507,\n",
       " 'Late': 508,\n",
       " 'March': 509,\n",
       " 'Month': 510,\n",
       " 'One': 511,\n",
       " 'Over': 512,\n",
       " 'Release': 513,\n",
       " 'Set': 514,\n",
       " '—': 515,\n",
       " '$5': 516,\n",
       " 'Debt': 517,\n",
       " 'Enough': 518,\n",
       " 'Forgiveness': 519,\n",
       " 'Francisco': 520,\n",
       " 'Longtime': 521,\n",
       " 'Million': 522,\n",
       " 'Official': 523,\n",
       " 'Payment': 524,\n",
       " 'Reparations': 525,\n",
       " 'Residents': 526,\n",
       " 'San': 527,\n",
       " 'Total': 528,\n",
       " \"Assistant's\": 529,\n",
       " 'BACK': 530,\n",
       " 'FOR': 531,\n",
       " 'FaceTime': 532,\n",
       " 'HAVE': 533,\n",
       " 'Him': 534,\n",
       " 'If': 535,\n",
       " 'MAKE': 536,\n",
       " 'Refused': 537,\n",
       " 'Sex': 538,\n",
       " 'She': 539,\n",
       " 'TO': 540,\n",
       " 'Threatened': 541,\n",
       " 'UP': 542,\n",
       " 'WORK:': 543,\n",
       " 'Withhold': 544,\n",
       " 'YOU': 545,\n",
       " '(VIDEO)': 546,\n",
       " 'Center': 547,\n",
       " 'Confronts': 548,\n",
       " 'FBI': 549,\n",
       " 'Leak': 550,\n",
       " 'Media': 551,\n",
       " 'November': 552,\n",
       " 'Penn': 553,\n",
       " 'Reveals': 554,\n",
       " 'Searched': 555,\n",
       " 'Spox': 556,\n",
       " '*Key*': 557,\n",
       " 'Audio': 558,\n",
       " 'Bob': 559,\n",
       " 'Concealed': 560,\n",
       " 'Deceptively': 561,\n",
       " 'Deleted': 562,\n",
       " 'Here': 563,\n",
       " 'Interviews': 564,\n",
       " 'Lawsuit:': 565,\n",
       " 'Long': 566,\n",
       " 'Months': 567,\n",
       " 'Parts': 568,\n",
       " 'President': 569,\n",
       " 'Redactions': 570,\n",
       " 'Woodward': 571,\n",
       " '5th': 572,\n",
       " 'Devastates': 573,\n",
       " 'Matters': 574,\n",
       " 'Nicolle': 575,\n",
       " 'Shows': 576,\n",
       " 'Taking': 577,\n",
       " 'Wallace': 578,\n",
       " 'Why': 579,\n",
       " 'Donald': 580,\n",
       " 'be': 581,\n",
       " 'case': 582,\n",
       " 'could': 583,\n",
       " 'criminally': 584,\n",
       " 'hush': 585,\n",
       " 'money': 586,\n",
       " 'Committee': 587,\n",
       " 'Deniers': 588,\n",
       " 'Election': 589,\n",
       " 'Ethics': 590,\n",
       " 'Stacked': 591,\n",
       " \"'special'\": 592,\n",
       " 'A': 593,\n",
       " 'Georgia': 594,\n",
       " 'grand': 595,\n",
       " 'jury': 596,\n",
       " 'like': 597,\n",
       " \"wouldn't\": 598,\n",
       " 'Fight,': 599,\n",
       " 'Limit': 600,\n",
       " 'Say': 601,\n",
       " 'Spending': 602,\n",
       " 'They': 603,\n",
       " 'Want': 604,\n",
       " '81': 605,\n",
       " 'Ann': 606,\n",
       " 'Korologos,': 607,\n",
       " 'McLaughlin': 608,\n",
       " 'Reagan-era': 609,\n",
       " 'labor': 610,\n",
       " 'secretary,': 611,\n",
       " 'These': 612,\n",
       " 'benefits': 613,\n",
       " 'disappear': 614,\n",
       " 'emergencies': 615,\n",
       " 'ends': 616,\n",
       " 'public': 617,\n",
       " 'when': 618,\n",
       " 'will': 619,\n",
       " 'DeSantis,': 620,\n",
       " 'battle': 621,\n",
       " 'big': 622,\n",
       " 'machine': 623,\n",
       " 'prepares': 624,\n",
       " 'rivals': 625,\n",
       " 'Earth': 626,\n",
       " 'Rare,': 627,\n",
       " 'Wednesday': 628,\n",
       " 'comet': 629,\n",
       " 'green': 630,\n",
       " 'pass': 631,\n",
       " 'Affairs': 632,\n",
       " 'Omar': 633,\n",
       " 'Spartz': 634,\n",
       " 'committee': 635,\n",
       " 'denying': 636,\n",
       " 'she': 637,\n",
       " 'Every': 638,\n",
       " 'Fell': 639,\n",
       " 'Outlet': 640,\n",
       " 'Scam': 641,\n",
       " 'Virtually': 642,\n",
       " 'Day': 643,\n",
       " 'Gov.': 644,\n",
       " 'Maine': 645,\n",
       " 'Mills': 646,\n",
       " \"Valentine's\": 647,\n",
       " 'address': 648,\n",
       " 'budget': 649,\n",
       " 'give': 650,\n",
       " 'All': 651,\n",
       " 'Assume': 652,\n",
       " 'CONLIN:': 653,\n",
       " 'Politicians': 654,\n",
       " 'Self-Serving': 655,\n",
       " 'Alarm:': 656,\n",
       " 'Catastrophic': 657,\n",
       " 'Corn': 658,\n",
       " 'Crop': 659,\n",
       " 'Ever': 660,\n",
       " 'Faced': 661,\n",
       " 'Farmers': 662,\n",
       " 'Headed': 663,\n",
       " 'Most': 664,\n",
       " 'Problem': 665,\n",
       " 'Sound': 666,\n",
       " \"'Vacant\": 667,\n",
       " \"Apartments'\": 668,\n",
       " 'Demand': 669,\n",
       " 'Free': 670,\n",
       " 'Hotel,': 671,\n",
       " 'Leaving': 672,\n",
       " 'Luxury': 673,\n",
       " 'Migrants': 674,\n",
       " 'NYC': 675,\n",
       " 'Stays': 676,\n",
       " '\"His': 677,\n",
       " 'Country\"': 678,\n",
       " 'Endorses': 679,\n",
       " 'GOP': 680,\n",
       " 'Incredibly': 681,\n",
       " 'J.D.': 682,\n",
       " 'Policies': 683,\n",
       " 'Primary': 684,\n",
       " 'Senator': 685,\n",
       " 'This': 686,\n",
       " 'Vance': 687,\n",
       " 'Were': 688,\n",
       " 'Care': 689,\n",
       " 'Health': 690,\n",
       " 'earnings': 691,\n",
       " 'expected': 692,\n",
       " 'record': 693,\n",
       " 'Blinken': 694,\n",
       " 'Examiner': 695,\n",
       " 'Israeli': 696,\n",
       " 'democracy': 697,\n",
       " 'dismisses': 698,\n",
       " 'minister’s': 699,\n",
       " 'rebuke': 700,\n",
       " '‘lecture’': 701,\n",
       " '$1.5M': 702,\n",
       " 'Woman': 703,\n",
       " 'chicken': 704,\n",
       " 'impoverished': 705,\n",
       " 'stealing': 706,\n",
       " 'wings': 707,\n",
       " '$800': 708,\n",
       " '15': 709,\n",
       " 'Carolina': 710,\n",
       " 'Deadline': 711,\n",
       " 'South': 712,\n",
       " 'Stimulus': 713,\n",
       " 'checks': 714,\n",
       " 'claim': 715,\n",
       " 'days': 716,\n",
       " 'up': 717,\n",
       " 'update:': 718,\n",
       " 'worth': 719,\n",
       " 'COVID-19': 720,\n",
       " 'approves': 721,\n",
       " 'immediately': 722,\n",
       " 'amid': 723,\n",
       " 'finances': 724,\n",
       " 'resigns': 725,\n",
       " 'scrutiny': 726,\n",
       " 'treasurer': 727,\n",
       " 'black': 728,\n",
       " 'enough,': 729,\n",
       " 'million': 730,\n",
       " 'not': 731,\n",
       " 'official': 732,\n",
       " 'reparations': 733,\n",
       " 'residents': 734,\n",
       " '$2.2': 735,\n",
       " 'ANOTHER': 736,\n",
       " 'Aid': 737,\n",
       " 'Including': 738,\n",
       " 'Long-Range': 739,\n",
       " 'Military': 740,\n",
       " 'Missiles:': 741,\n",
       " 'Reuters': 742,\n",
       " 'Send': 743,\n",
       " 'Ukraine': 744,\n",
       " 'Al': 745,\n",
       " 'Attend': 746,\n",
       " 'Eulogy': 747,\n",
       " 'Funeral': 748,\n",
       " 'Harris,': 749,\n",
       " 'Host': 750,\n",
       " 'Kamala': 751,\n",
       " 'MSNBC': 752,\n",
       " 'Nichols': 753,\n",
       " 'Officials': 754,\n",
       " 'Senior': 755,\n",
       " 'Sharpton': 756,\n",
       " 'Wednesday;': 757,\n",
       " 'Elizabeth': 758,\n",
       " 'Harris': 759,\n",
       " 'Warren-Kamala': 760,\n",
       " 'feud': 761,\n",
       " 'made': 762,\n",
       " 'president': 763,\n",
       " 'vice': 764,\n",
       " 'calendar': 765,\n",
       " 'presidential': 766,\n",
       " 'rearranging': 767,\n",
       " \"'bear's\": 768,\n",
       " 'Mars': 769,\n",
       " 'NASA': 770,\n",
       " 'captures': 771,\n",
       " \"face'\": 772,\n",
       " 'photo': 773,\n",
       " 'surface': 774,\n",
       " \"'challenge'\": 775,\n",
       " 'Mexico': 776,\n",
       " 'media': 777,\n",
       " 'social': 778,\n",
       " 'students': 779,\n",
       " 'treated': 780,\n",
       " 'viral': 781,\n",
       " \"'Makes\": 782,\n",
       " 'Bobby': 783,\n",
       " \"Hull's\": 784,\n",
       " 'critics': 785,\n",
       " 'death:': 786,\n",
       " 'grandson': 787,\n",
       " \"legend's\": 788,\n",
       " 'me': 789,\n",
       " \"puke'\": 790,\n",
       " 'responds': 791,\n",
       " '1,000': 792,\n",
       " \"World's\": 793,\n",
       " 'YouTuber': 794,\n",
       " 'enrages': 795,\n",
       " 'internet': 796,\n",
       " 'paying': 797,\n",
       " \"people's\": 798,\n",
       " \"porn'\": 799,\n",
       " 'surgeries:': 800,\n",
       " 'top': 801,\n",
       " '‘Charity': 802,\n",
       " 'Dem': 803,\n",
       " 'Focused': 804,\n",
       " 'Mothers': 805,\n",
       " 'Proves': 806,\n",
       " 'Rep': 807,\n",
       " '‘Demonizing’': 808,\n",
       " '‘Pejorative’': 809,\n",
       " '‘Welfare’': 810,\n",
       " 'Death,': 811,\n",
       " 'Fake': 812,\n",
       " 'German': 813,\n",
       " 'Killed': 814,\n",
       " 'Lookalike': 815,\n",
       " 'Own': 816,\n",
       " 'Allegedly': 817,\n",
       " 'Arrests': 818,\n",
       " 'Building': 819,\n",
       " 'Carrying': 820,\n",
       " 'Impersonating': 821,\n",
       " 'Knives': 822,\n",
       " 'Officer,': 823,\n",
       " 'Secret': 824,\n",
       " 'Service': 825,\n",
       " 'Attending': 826,\n",
       " 'Banning': 827,\n",
       " 'Camp': 828,\n",
       " 'Kids': 829,\n",
       " 'Lawmaker': 830,\n",
       " 'Proposes': 831,\n",
       " 'base,': 832,\n",
       " 'battles': 833,\n",
       " 'closures': 834,\n",
       " 'covid': 835,\n",
       " 'resonate': 836,\n",
       " 'still': 837,\n",
       " 'vaccines': 838,\n",
       " \"'reenergize\": 839,\n",
       " 'DeMeco': 840,\n",
       " 'J.J.': 841,\n",
       " 'Ryans': 842,\n",
       " 'Texans': 843,\n",
       " 'Watt': 844,\n",
       " 'base': 845,\n",
       " 'coach': 846,\n",
       " 'fan': 847,\n",
       " 'head': 848,\n",
       " 'hiring': 849,\n",
       " 'next': 850,\n",
       " \"reignite'\": 851,\n",
       " 'Chamber': 852,\n",
       " 'Commerce': 853,\n",
       " 'EXCLUSIVE:': 854,\n",
       " 'Environmental': 855,\n",
       " 'Lawsuit': 856,\n",
       " 'Pledges': 857,\n",
       " '‘Outrageous’': 858,\n",
       " 'Group': 859,\n",
       " 'animals': 860,\n",
       " 'behind': 861,\n",
       " 'left': 862,\n",
       " 'save': 863,\n",
       " 'war': 864,\n",
       " 'works': 865,\n",
       " 'Hillicon': 866,\n",
       " 'PayPal': 867,\n",
       " 'Valley': 868,\n",
       " 'Workday,': 869,\n",
       " 'follow': 870,\n",
       " 'layoff': 871,\n",
       " 'trend': 872,\n",
       " '&': 873,\n",
       " 'Defense': 874,\n",
       " 'National': 875,\n",
       " 'Security': 876,\n",
       " 'arms': 877,\n",
       " 'nuclear': 878,\n",
       " 'treaty': 879,\n",
       " 'violating': 880,\n",
       " 'McCarthy,': 881,\n",
       " 'Moderate': 882,\n",
       " 'debt': 883,\n",
       " 'help': 884,\n",
       " 'limit': 885,\n",
       " 'offer': 886,\n",
       " 'Fed': 887,\n",
       " 'down': 888,\n",
       " 'drives': 889,\n",
       " 'growth': 890,\n",
       " 'wage': 891,\n",
       " 'Brand': 892,\n",
       " 'Builds': 893,\n",
       " 'Education': 894,\n",
       " 'Establishment,': 895,\n",
       " 'Haley': 896,\n",
       " 'race': 897,\n",
       " 'run': 898,\n",
       " 'signal': 899,\n",
       " 'slowly': 900,\n",
       " 'starts': 901,\n",
       " 'video,': 902,\n",
       " 'JESSE': 903,\n",
       " 'WATTERS:': 904,\n",
       " 'dumping': 905,\n",
       " 'Illinois': 906,\n",
       " 'appeal': 907,\n",
       " 'control': 908,\n",
       " 'effect': 909,\n",
       " 'gun': 910,\n",
       " 'law,': 911,\n",
       " 'leaving': 912,\n",
       " 'loses': 913,\n",
       " 'order': 914,\n",
       " 'restraining': 915,\n",
       " 'Adams': 916,\n",
       " 'Eric': 917,\n",
       " 'Mayor': 918,\n",
       " 'crisis': 919,\n",
       " 'migrant': 920,\n",
       " 'more': 921,\n",
       " 'pleads': 922,\n",
       " 'Allen': 923,\n",
       " 'Beach': 924,\n",
       " \"Bills'\": 925,\n",
       " 'Bowl': 926,\n",
       " 'Josh': 927,\n",
       " 'Pebble': 928,\n",
       " 'Pro': 929,\n",
       " 'golf': 930,\n",
       " 'play': 931,\n",
       " 'skipping': 932,\n",
       " 'tournament': 933,\n",
       " '1990s': 934,\n",
       " 'Manson': 935,\n",
       " 'Marilyn': 936,\n",
       " 'assaulting': 937,\n",
       " 'lawsuit': 938,\n",
       " 'minor': 939,\n",
       " 'sexually': 940,\n",
       " 'plans': 941,\n",
       " 'remove': 942,\n",
       " 'soon': 943,\n",
       " 'Dems': 944,\n",
       " 'Medicare,': 945,\n",
       " 'Social': 946,\n",
       " 'anti-socialist': 947,\n",
       " 'hit': 948,\n",
       " 'press': 949,\n",
       " 'whether': 950,\n",
       " \"'Invisible'\": 951,\n",
       " 'Attempts': 952,\n",
       " 'Customers': 953,\n",
       " 'Developing': 954,\n",
       " 'High-Tech': 955,\n",
       " \"Lowe's\": 956,\n",
       " 'Rampant': 957,\n",
       " 'System': 958,\n",
       " \"That's\": 959,\n",
       " 'Theft': 960,\n",
       " 'Thwart': 961,\n",
       " \"'Horrific\": 962,\n",
       " 'Driving': 963,\n",
       " \"Experience':\": 964,\n",
       " 'Falls': 965,\n",
       " 'Family': 966,\n",
       " 'Highway': 967,\n",
       " \"Owner's\": 968,\n",
       " 'Steering': 969,\n",
       " 'Wheel': 970,\n",
       " 'While': 971,\n",
       " 'Needs': 972,\n",
       " 'Officially': 973,\n",
       " 'Prosecution': 974,\n",
       " 'Prove': 975,\n",
       " \"'The\": 976,\n",
       " 'American': 977,\n",
       " \"Better'\": 978,\n",
       " 'Calls': 979,\n",
       " 'Cognitive': 980,\n",
       " 'Deserve': 981,\n",
       " 'Doctor': 982,\n",
       " 'People': 983,\n",
       " 'Prominent': 984,\n",
       " 'Test:': 985,\n",
       " 'Undergo': 986,\n",
       " 'Behind': 987,\n",
       " 'Congressional': 988,\n",
       " 'Hearings': 989,\n",
       " 'Kashmir': 990,\n",
       " 'Taliban': 991,\n",
       " 'Turn': 992,\n",
       " 'Up': 993,\n",
       " 'Weapons': 994,\n",
       " 'Gateway': 995,\n",
       " 'Page': 996,\n",
       " 'Pundit': 997,\n",
       " 'found': 998,\n",
       " \"'Serving\": 999,\n",
       " ...}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the reviews into Gensim bag-of-words vectors\n",
    "corpus = [dictionary.doc2bow(title.split()) for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Gensim TF-IDF model on the corpus\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the corpus into Gensim TF-IDF vectors\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m           TransformedCorpus\n",
      "\u001b[0;31mString form:\u001b[0m    <gensim.interfaces.TransformedCorpus object at 0x7f34899b1660>\n",
      "\u001b[0;31mLength:\u001b[0m         11586\n",
      "\u001b[0;31mFile:\u001b[0m           /media/james/Projects/GitHub/DATA_340_NLP/venv/lib/python3.10/site-packages/gensim/interfaces.py\n",
      "\u001b[0;31mDocstring:\u001b[0m      Interface for corpora that are the result of an online (streamed) transformation.\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Parameters\n",
      "----------\n",
      "obj : object\n",
      "    A transformation :class:`~gensim.interfaces.TransformationABC` object that will be applied\n",
      "    to each document from `corpus` during iteration.\n",
      "corpus : iterable of list of (int, number)\n",
      "    Corpus in bag-of-words format.\n",
      "chunksize : int, optional\n",
      "    If provided, a slightly more effective processing will be performed by grouping documents from `corpus`."
     ]
    }
   ],
   "source": [
    "tfidf_corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.32468534328551457), (1, 0.32468534328551457), (2, 0.2584169554144531), (3, 0.19024770138233393), (4, 0.30048513863439946), (5, 0.3179550964042962), (6, 0.2984219385078802), (7, 0.32468534328551457), (8, 0.2584169554144531), (9, 0.32468534328551457), (10, 0.32468534328551457), (11, 0.15067146529062592)]\n",
      "[(3, 0.24495403190564266), (11, 0.19399752348070728), (12, 0.38852224809611624), (13, 0.4227104186686381), (14, 0.4227104186686381), (15, 0.4136144820378332), (16, 0.39775189221153373), (17, 0.25448230244861914)]\n",
      "[(3, 0.19956880919341843), (18, 0.3069028857577644), (19, 0.3363941626852941), (20, 0.2826278275460694), (21, 0.33077778414752956), (22, 0.22836338466496256), (23, 0.23864755918795416), (24, 0.2820621988488852), (25, 0.27035192728438345), (26, 0.32970348315347797), (27, 0.26751629670278293), (28, 0.337569976449488), (29, 0.08691868993437478)]\n",
      "[(2, 0.2539793881751479), (8, 0.2539793881751479), (11, 0.14808411665013346), (17, 0.19425395894264366), (29, 0.08143617957551065), (30, 0.34218613455722996), (31, 0.34218613455722996), (32, 0.29055378695236683), (33, 0.34218613455722996), (34, 0.34218613455722996), (35, 0.29055378695236683), (36, 0.34218613455722996), (37, 0.22448568687277906)]\n",
      "[(38, 0.08659324886156236), (39, 0.31797901348037716), (40, 0.34368979242236875), (41, 0.2083112139627468), (42, 0.3496762111011332), (43, 0.29184910200293585), (44, 0.35322143909980586), (45, 0.33814723443500233), (46, 0.3462914567231912), (47, 0.3462914567231912), (48, 0.20852071075794726)]\n",
      "[(38, 0.07402307489327965), (41, 0.1780720413542432), (49, 0.2916493723272464), (50, 0.21764937757264552), (51, 0.1415951181083158), (52, 0.20223104594493363), (53, 0.3149834172310251), (54, 0.1405424849516107), (55, 0.3149834172310251), (56, 0.3149834172310251), (57, 0.3149834172310251), (58, 0.5080228774918935), (59, 0.3077923073517219)]\n",
      "[(23, 0.21653246531500112), (27, 0.24272598234022386), (38, 0.07843466234797426), (41, 0.18868468322034812), (60, 0.32541907604918063), (61, 0.3414329412613849), (62, 0.32686048707381316), (63, 0.2821253951405844), (64, 0.3101600237978764), (65, 0.3414329412613849), (66, 0.3379129930535574), (67, 0.3414329412613849)]\n",
      "[(38, 0.07907004678573486), (41, 0.19021318232741535), (54, 0.1501248208957635), (68, 0.3529288911331183), (69, 0.32523864826626536), (70, 0.3529288911331183), (71, 0.3529288911331183), (72, 0.3138301001970962), (73, 0.3186669913566733), (74, 0.10596753700779697), (75, 0.34604468199899313), (76, 0.3529288911331183)]\n",
      "[(11, 0.16064008365296692), (77, 0.382462674544001), (78, 0.382462674544001), (79, 0.36216514252410714), (80, 0.316450942182073), (81, 0.21082996227985085), (82, 0.2495936981282891), (83, 0.3413086787856043), (84, 0.21061946428612824), (85, 0.17302076075403913), (86, 0.12263077070113014), (87, 0.31113071434368345), (88, 0.18805668191289474), (89, 0.07479299921952001)]\n",
      "[(17, 0.1905041741961437), (81, 0.19059938981861124), (82, 0.2256434808951682), (84, 0.19040909054270322), (86, 0.11086351207328012), (89, 0.0676161009554323), (90, 0.2285631915253434), (91, 0.34576277303374314), (92, 0.3226798203463732), (93, 0.3220295515443004), (94, 0.34576277303374314), (95, 0.34576277303374314), (96, 0.29167235497115035), (97, 0.34576277303374314)]\n"
     ]
    }
   ],
   "source": [
    "# examine the first 10 documents\n",
    "for doc in tfidf_corpus[:10]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Gensim LSI model on the TF-IDF vectors\n",
    "lsi_model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m           LsiModel\n",
      "\u001b[0;31mString form:\u001b[0m    LsiModel<num_terms=3950, num_topics=25, decay=1.0, chunksize=20000>\n",
      "\u001b[0;31mFile:\u001b[0m           /media/james/Projects/GitHub/DATA_340_NLP/venv/lib/python3.10/site-packages/gensim/models/lsimodel.py\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mLsiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformationABC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasemodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseTopicModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Model for `Latent Semantic Indexing\u001b[0m\n",
      "\u001b[0;34m    <https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing>`_.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The decomposition algorithm is described in `\"Fast and Faster: A Comparison of Two Streamed\u001b[0m\n",
      "\u001b[0;34m    Matrix Decomposition Algorithms\" <https://arxiv.org/pdf/1102.5597.pdf>`_.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Notes\u001b[0m\n",
      "\u001b[0;34m    -----\u001b[0m\n",
      "\u001b[0;34m    * :attr:`gensim.models.lsimodel.LsiModel.projection.u` - left singular vectors,\u001b[0m\n",
      "\u001b[0;34m    * :attr:`gensim.models.lsimodel.LsiModel.projection.s` - singular values,\u001b[0m\n",
      "\u001b[0;34m    * ``model[training_corpus]`` - right singular vectors (can be reconstructed if needed).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    See Also\u001b[0m\n",
      "\u001b[0;34m    --------\u001b[0m\n",
      "\u001b[0;34m    `FAQ about LSI matrices\u001b[0m\n",
      "\u001b[0;34m    <https://github.com/RaRe-Technologies/gensim/wiki/Recipes-&-FAQ#q4-how-do-you-output-the-u-s-vt-matrices-of-lsi>`_.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Examples\u001b[0m\n",
      "\u001b[0;34m    --------\u001b[0m\n",
      "\u001b[0;34m    .. sourcecode:: pycon\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        >>> from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\u001b[0m\n",
      "\u001b[0;34m        >>> from gensim.models import LsiModel\u001b[0m\n",
      "\u001b[0;34m        >>>\u001b[0m\n",
      "\u001b[0;34m        >>> model = LsiModel(common_corpus[:3], id2word=common_dictionary)  # train model\u001b[0m\n",
      "\u001b[0;34m        >>> vector = model[common_corpus[4]]  # apply model to BoW document\u001b[0m\n",
      "\u001b[0;34m        >>> model.add_documents(common_corpus[4:])  # update model with new documents\u001b[0m\n",
      "\u001b[0;34m        >>> tmp_fname = get_tmpfile(\"lsi.model\")\u001b[0m\n",
      "\u001b[0;34m        >>> model.save(tmp_fname)  # save model\u001b[0m\n",
      "\u001b[0;34m        >>> loaded_model = LsiModel.load(tmp_fname)  # load model\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monepass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP2_EXTRA_ITERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mextra_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP2_EXTRA_DIMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Build an LSI model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\u001b[0m\n",
      "\u001b[0;34m            Stream of document vectors or a sparse matrix of shape (`num_documents`, `num_terms`).\u001b[0m\n",
      "\u001b[0;34m        num_topics : int, optional\u001b[0m\n",
      "\u001b[0;34m            Number of requested factors (latent dimensions)\u001b[0m\n",
      "\u001b[0;34m        id2word : dict of {int: str}, optional\u001b[0m\n",
      "\u001b[0;34m            ID to word mapping, optional.\u001b[0m\n",
      "\u001b[0;34m        chunksize :  int, optional\u001b[0m\n",
      "\u001b[0;34m            Number of documents to be used in each training chunk.\u001b[0m\n",
      "\u001b[0;34m        decay : float, optional\u001b[0m\n",
      "\u001b[0;34m            Weight of existing observations relatively to new ones.\u001b[0m\n",
      "\u001b[0;34m        distributed : bool, optional\u001b[0m\n",
      "\u001b[0;34m            If True - distributed mode (parallel execution on several machines) will be used.\u001b[0m\n",
      "\u001b[0;34m        onepass : bool, optional\u001b[0m\n",
      "\u001b[0;34m            Whether the one-pass algorithm should be used for training.\u001b[0m\n",
      "\u001b[0;34m            Pass `False` to force a multi-pass stochastic algorithm.\u001b[0m\n",
      "\u001b[0;34m        power_iters: int, optional\u001b[0m\n",
      "\u001b[0;34m            Number of power iteration steps to be used.\u001b[0m\n",
      "\u001b[0;34m            Increasing the number of power iterations improves accuracy, but lowers performance\u001b[0m\n",
      "\u001b[0;34m        extra_samples : int, optional\u001b[0m\n",
      "\u001b[0;34m            Extra samples to be used besides the rank `k`. Can improve accuracy.\u001b[0m\n",
      "\u001b[0;34m        dtype : type, optional\u001b[0m\n",
      "\u001b[0;34m            Enforces a type for elements of the decomposed matrix.\u001b[0m\n",
      "\u001b[0;34m        random_seed: {None, int}, optional\u001b[0m\n",
      "\u001b[0;34m            Random seed used to initialize the pseudo-random number generator,\u001b[0m\n",
      "\u001b[0;34m            a local instance of numpy.random.RandomState instance.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0monepass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"forcing the one-pass algorithm for distributed LSA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0monepass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monepass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monepass\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_iters\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m'at least one of corpus/id2word must be specified, to establish input space dimensionality'\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no word id mapping provided; initializing from corpus, assuming identity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_from_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProjection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mextra_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using serial LSI version on this node\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0monepass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"distributed stochastic LSA not implemented yet; \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m\"run either distributed one-pass, or serial randomized.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mimport\u001b[0m \u001b[0mPyro4\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdispatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyro4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PYRONAME:gensim.lsi_dispatcher'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"looking for dispatcher at %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pyroUri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mpower_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monepass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0monepass\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetworkers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using distributed version with %i workers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# distributed version was specifically requested, so this is an error state\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"failed to initialize distributed LSI (%s)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"failed to initialize distributed LSI (%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"trained {self} in {time.time() - start:.2f}s\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Update model with new `corpus`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        corpus : {iterable of list of (int, float), scipy.sparse.csc}\u001b[0m\n",
      "\u001b[0;34m            Stream of document vectors or sparse matrix of shape (`num_terms`, num_documents).\u001b[0m\n",
      "\u001b[0;34m        chunksize : int, optional\u001b[0m\n",
      "\u001b[0;34m            Number of documents to be used in each training chunk, will use `self.chunksize` if not specified.\u001b[0m\n",
      "\u001b[0;34m        decay : float, optional\u001b[0m\n",
      "\u001b[0;34m            Weight of existing observations relatively to new ones,  will use `self.decay` if not specified.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Notes\u001b[0m\n",
      "\u001b[0;34m        -----\u001b[0m\n",
      "\u001b[0;34m        Training proceeds in chunks of `chunksize` documents at a time. The size of `chunksize` is a tradeoff\u001b[0m\n",
      "\u001b[0;34m        between increased speed (bigger `chunksize`) vs. lower memory footprint (smaller `chunksize`).\u001b[0m\n",
      "\u001b[0;34m        If the distributed mode is on, each chunk is sent to a different worker/computer.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"updating model with new documents\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# get computation parameters; if not specified, use the ones from constructor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mchunksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mdecay\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LsiModel.add_documents() called but no documents provided, is this intended?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monepass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# we are allowed multiple passes over the input => use a faster, randomized two-pass algo\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProjection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mupdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstochastic_svd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mnum_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mextra_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# the one-pass algo\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mdoc_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'initializing %s workers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0mchunk_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preparing a new chunk of documents\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mnnz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;31m# construct the job as a sparse matrix, to minimize memory overhead\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;31m# definitely avoid materializing it as a dense matrix!\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"converting corpus to csc format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus2csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mnum_nnz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mdel\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mdoc_no\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;31m# distributed version: add this job to the job queue, so workers can work on it\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creating job #%i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;31m# put job into queue; this will eventually block, because the queue has a small finite size\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputjob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;32mdel\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dispatched documents up to #%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;31m# serial version, there is only one \"worker\" (myself) => process the job directly\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProjection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                            \u001b[0mpower_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;32mdel\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0;32mdel\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"processed documents up to #%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;31m# wait for all workers to finish (distributed version only)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reached the end of input; now waiting for all remaining jobs to finish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdoc_no\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"must be in serial mode to receive jobs\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProjection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mpower_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"processed sparse job of %i documents\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get a human readable representation of model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        str\u001b[0m\n",
      "\u001b[0;34m            A human readable string of the current objects parameters.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"%s<num_terms=%s, num_topics=%s, decay=%s, chunksize=%s>\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get the latent representation for `bow`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        bow : {list of (int, int), iterable of list of (int, int)}\u001b[0m\n",
      "\u001b[0;34m            Document or corpus in BoW representation.\u001b[0m\n",
      "\u001b[0;34m        scaled : bool, optional\u001b[0m\n",
      "\u001b[0;34m            If True - topics will be scaled by the inverse of singular values.\u001b[0m\n",
      "\u001b[0;34m        chunksize :  int, optional\u001b[0m\n",
      "\u001b[0;34m            Number of documents to be used in each applying chunk.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        list of (int, float)\u001b[0m\n",
      "\u001b[0;34m            Latent representation of topics in BoW format for document **OR**\u001b[0m\n",
      "\u001b[0;34m        :class:`gensim.matutils.Dense2Corpus`\u001b[0m\n",
      "\u001b[0;34m            Latent representation of corpus in BoW format if `bow` is corpus.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No training data provided - LSI model not initialized yet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# if the input vector is in fact a corpus, return a transformed corpus as a result\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mis_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mis_corpus\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# by default, transform `chunksize` documents at once, when called as `lsi[corpus]`.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# this chunking is completely transparent to the user, but it speeds\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# up internal computations (one mat * mat multiplication, instead of\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# `chunksize` smaller mat * vec multiplications).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_corpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# convert input to scipy.sparse CSC, then do \"sparse * dense = dense\" multiplication\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus2csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtopic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m  \u001b[0;31m# (x^T * u).T = u^-1 * x\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# # convert input to dense, then do dense * dense multiplication\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# # ± same performance as above (BLAS dense * dense is better optimized than scipy.sparse),\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# but consumes more memory\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# vec = matutils.corpus2dense(bow, num_terms=self.num_terms, num_docs=len(bow))\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# topic_dist = np.dot(self.projection.u[:, :self.num_topics].T, vec)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# # use np's advanced indexing to simulate sparse * dense\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# # ± same speed again\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# u = self.projection.u[:, :self.num_topics]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# topic_dist = np.empty((u.shape[1], len(bow)), dtype=u.dtype)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# for vecno, vec in enumerate(bow):\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m#     indices, data = zip(*vec) if vec else ([], [])\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m#     topic_dist[:, vecno] = np.dot(u.take(indices, axis=0).T, np.array(data, dtype=u.dtype))\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_corpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# convert back from matrix into a 1d vec\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtopic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mscaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtopic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtopic_dist\u001b[0m  \u001b[0;31m# s^-1 * u^-1 * x\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# convert a np array to gensim sparse vector = tuples of (feature_id, feature_weight),\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# with no zero weights.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_corpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# lsi[single_document]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull2sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# lsi[chunk of documents]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense2Corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get the topic vectors.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Notes\u001b[0m\n",
      "\u001b[0;34m        -----\u001b[0m\n",
      "\u001b[0;34m        The number of topics can actually be smaller than `self.num_topics`, if there were not enough factors\u001b[0m\n",
      "\u001b[0;34m        in the matrix (real rank of input matrix smaller than `self.num_topics`).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        np.ndarray\u001b[0m\n",
      "\u001b[0;34m            The term topic matrix with shape (`num_topics`, `vocabulary_size`)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprojections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnum_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mshow_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopicno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get the words that define a topic along with their contribution.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        This is actually the left singular vector of the specified topic.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        The most important words in defining the topic (greatest absolute value) are included\u001b[0m\n",
      "\u001b[0;34m        in the output, along with their contribution to the topic.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        topicno : int\u001b[0m\n",
      "\u001b[0;34m            The topics id number.\u001b[0m\n",
      "\u001b[0;34m        topn : int\u001b[0m\n",
      "\u001b[0;34m            Number of words to be included to the result.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        list of (str, float)\u001b[0m\n",
      "\u001b[0;34m            Topic representation in BoW format.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# size of the projection matrix can actually be smaller than `self.num_topics`,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# if there were not enough factors (real rank of input matrix smaller than\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# `self.num_topics`). in that case, return an empty string\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mtopicno\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopicno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Output only (word, score) pairs for `val`s that are within `self.id2word`.  See #3090 for details.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mshow_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get the most significant topics.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        num_topics : int, optional\u001b[0m\n",
      "\u001b[0;34m            The number of topics to be selected, if -1 - all topics will be in result (ordered by significance).\u001b[0m\n",
      "\u001b[0;34m        num_words : int, optional\u001b[0m\n",
      "\u001b[0;34m            The number of words to be included per topics (ordered by significance).\u001b[0m\n",
      "\u001b[0;34m        log : bool, optional\u001b[0m\n",
      "\u001b[0;34m            If True - log topics with logger.\u001b[0m\n",
      "\u001b[0;34m        formatted : bool, optional\u001b[0m\n",
      "\u001b[0;34m            If True - each topic represented as string, otherwise - in BoW format.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        list of (int, str)\u001b[0m\n",
      "\u001b[0;34m            If `formatted=True`, return sequence with (topic_id, string representation of topics) **OR**\u001b[0m\n",
      "\u001b[0;34m        list of (int, list of (str, float))\u001b[0m\n",
      "\u001b[0;34m            Otherwise, return sequence with (topic_id, [(word, value), ... ]).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mshown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnum_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mformatted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mshown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                    \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topic #%i(%.3f): %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mshown\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mprint_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Print (to log) the most salient words of the first `num_topics` topics.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Unlike :meth:`~gensim.models.lsimodel.LsiModel.print_topics`, this looks for words that are significant for\u001b[0m\n",
      "\u001b[0;34m        a particular topic *and* not for others. This *should* result in a\u001b[0m\n",
      "\u001b[0;34m        more human-interpretable description of topics.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Alias for :func:`~gensim.models.lsimodel.print_debug`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        num_topics : int, optional\u001b[0m\n",
      "\u001b[0;34m            The number of topics to be selected (ordered by significance).\u001b[0m\n",
      "\u001b[0;34m        num_words : int, optional\u001b[0m\n",
      "\u001b[0;34m            The number of words to be included per topics (ordered by significance).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# only wrap the module-level fnc\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprint_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Save the model to a file.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Notes\u001b[0m\n",
      "\u001b[0;34m        -----\u001b[0m\n",
      "\u001b[0;34m        Large internal arrays may be stored into separate files, with `fname` as prefix.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Warnings\u001b[0m\n",
      "\u001b[0;34m        --------\u001b[0m\n",
      "\u001b[0;34m        Do not save as a compressed file if you intend to load the file back with `mmap`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        fname : str\u001b[0m\n",
      "\u001b[0;34m            Path to output file.\u001b[0m\n",
      "\u001b[0;34m        *args\u001b[0m\n",
      "\u001b[0;34m            Variable length argument list, see :meth:`gensim.utils.SaveLoad.save`.\u001b[0m\n",
      "\u001b[0;34m        **kwargs\u001b[0m\n",
      "\u001b[0;34m            Arbitrary keyword arguments, see :meth:`gensim.utils.SaveLoad.save`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        See Also\u001b[0m\n",
      "\u001b[0;34m        --------\u001b[0m\n",
      "\u001b[0;34m        :meth:`~gensim.models.lsimodel.LsiModel.load`\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.projection'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'projection'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dispatcher'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Load a previously saved object using :meth:`~gensim.models.lsimodel.LsiModel.save` from file.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Notes\u001b[0m\n",
      "\u001b[0;34m        -----\u001b[0m\n",
      "\u001b[0;34m        Large arrays can be memmap'ed back as read-only (shared memory) by setting the `mmap='r'` parameter.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Parameters\u001b[0m\n",
      "\u001b[0;34m        ----------\u001b[0m\n",
      "\u001b[0;34m        fname : str\u001b[0m\n",
      "\u001b[0;34m            Path to file that contains LsiModel.\u001b[0m\n",
      "\u001b[0;34m        *args\u001b[0m\n",
      "\u001b[0;34m            Variable length argument list, see :meth:`gensim.utils.SaveLoad.load`.\u001b[0m\n",
      "\u001b[0;34m        **kwargs\u001b[0m\n",
      "\u001b[0;34m            Arbitrary keyword arguments, see :meth:`gensim.utils.SaveLoad.load`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        See Also\u001b[0m\n",
      "\u001b[0;34m        --------\u001b[0m\n",
      "\u001b[0;34m        :meth:`~gensim.models.lsimodel.LsiModel.save`\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Returns\u001b[0m\n",
      "\u001b[0;34m        -------\u001b[0m\n",
      "\u001b[0;34m        :class:`~gensim.models.lsimodel.LsiModel`\u001b[0m\n",
      "\u001b[0;34m            Loaded instance.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Raises\u001b[0m\n",
      "\u001b[0;34m        ------\u001b[0m\n",
      "\u001b[0;34m        IOError\u001b[0m\n",
      "\u001b[0;34m            When methods are called on instance (should be called from class).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mmap'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mmap'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mprojection_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.projection'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"failed to load projection from %s: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Build an LSI model.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
      "    Stream of document vectors or a sparse matrix of shape (`num_documents`, `num_terms`).\n",
      "num_topics : int, optional\n",
      "    Number of requested factors (latent dimensions)\n",
      "id2word : dict of {int: str}, optional\n",
      "    ID to word mapping, optional.\n",
      "chunksize :  int, optional\n",
      "    Number of documents to be used in each training chunk.\n",
      "decay : float, optional\n",
      "    Weight of existing observations relatively to new ones.\n",
      "distributed : bool, optional\n",
      "    If True - distributed mode (parallel execution on several machines) will be used.\n",
      "onepass : bool, optional\n",
      "    Whether the one-pass algorithm should be used for training.\n",
      "    Pass `False` to force a multi-pass stochastic algorithm.\n",
      "power_iters: int, optional\n",
      "    Number of power iteration steps to be used.\n",
      "    Increasing the number of power iterations improves accuracy, but lowers performance\n",
      "extra_samples : int, optional\n",
      "    Extra samples to be used besides the rank `k`. Can improve accuracy.\n",
      "dtype : type, optional\n",
      "    Enforces a type for elements of the decomposed matrix.\n",
      "random_seed: {None, int}, optional\n",
      "    Random seed used to initialize the pseudo-random number generator,\n",
      "    a local instance of numpy.random.RandomState instance."
     ]
    }
   ],
   "source": [
    "lsi_model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('-', 0.22351318340115048),\n",
       "   ('The', 0.2215239063088244),\n",
       "   ('New', 0.17796204560949086),\n",
       "   ('York', 0.17756587389260567),\n",
       "   ('Trump', 0.17264271527176195),\n",
       "   ('Post', 0.17259230796850328),\n",
       "   ('Washington', 0.17162889486593416),\n",
       "   ('to', 0.15920168051786623),\n",
       "   ('the', 0.15364586274262773),\n",
       "   ('Times', 0.13648862164500525)]),\n",
       " (1,\n",
       "  [('Trump', -0.28581640550184273),\n",
       "   ('Stormy', -0.24777864902725394),\n",
       "   ('Daniels', -0.24777864902725374),\n",
       "   ('For', -0.19186268975074833),\n",
       "   ('Post', 0.17433705456220477),\n",
       "   ('Washington', 0.16956888149999652),\n",
       "   ('Publicly', -0.16138937678528795),\n",
       "   ('Story', -0.1613893767852879),\n",
       "   ('Attention:', -0.16138937678528784),\n",
       "   ('Confirming', -0.16138937678528772)]),\n",
       " (2,\n",
       "  [('Latest', 0.26481514526156763),\n",
       "   ('HuffPost', 0.26463734148522233),\n",
       "   ('News', 0.1737214508032373),\n",
       "   ('Stormy', -0.15542373897468745),\n",
       "   ('Daniels', -0.1554237389746874),\n",
       "   ('In', 0.1461348108894938),\n",
       "   ('Vox', -0.14088344358419605),\n",
       "   ('From', 0.14007085104724343),\n",
       "   ('|', 0.13597099708126167),\n",
       "   ('charged', -0.12980240762980927)]),\n",
       " (3,\n",
       "  [('George', -0.24330196792705538),\n",
       "   ('For', -0.23701408121522854),\n",
       "   ('Santos', -0.19053884994775208),\n",
       "   ('Committees', -0.18755778574644386),\n",
       "   ('Steps', -0.18532929784109048),\n",
       "   ('End', -0.18187805560455025),\n",
       "   ('Near', -0.18029724743562847),\n",
       "   ('He', -0.16801452550549806),\n",
       "   ('As', -0.16740041224418029),\n",
       "   ('York', 0.16275830570981373)]),\n",
       " (4,\n",
       "  [('5th', -0.29684346054230326),\n",
       "   ('Devastates', -0.29684346054230326),\n",
       "   ('Matters', -0.29684346054230326),\n",
       "   ('Nicolle', -0.29684346054230326),\n",
       "   ('Taking', -0.2930178555580593),\n",
       "   ('Shows', -0.2912529273786382),\n",
       "   ('Why', -0.28743799714911467),\n",
       "   ('Wallace', -0.28024841316864285),\n",
       "   ('Him', -0.2797702160301103),\n",
       "   ('And', -0.2666231369928956)]),\n",
       " (5,\n",
       "  [('Santos', -0.20117976030340556),\n",
       "   ('Vox', -0.1856082447571093),\n",
       "   ('Down', -0.18257795807912944),\n",
       "   ('Committees', -0.17840390133950187),\n",
       "   ('George', -0.1744976871971757),\n",
       "   ('Steps', -0.17346457360022818),\n",
       "   ('End', -0.1693798680128013),\n",
       "   ('Near', -0.16891016600240757),\n",
       "   ('As', -0.16474830029152995),\n",
       "   ('He', -0.16167386018483862)]),\n",
       " (6,\n",
       "  [('York', 0.2922812266865212),\n",
       "   ('Times', 0.2768231778918796),\n",
       "   ('New', 0.27617596670732597),\n",
       "   ('Vox', -0.17549326117434857),\n",
       "   ('charged', -0.16818150765900294),\n",
       "   ('HuffPost', -0.15603988336090324),\n",
       "   ('Latest', -0.15601661166152234),\n",
       "   ('Alec', -0.15227487545244844),\n",
       "   ('News', -0.1408309778943021),\n",
       "   ('involuntary', -0.14016348390764835)]),\n",
       " (7,\n",
       "  [('Computers', -0.31416460590885725),\n",
       "   ('Investigators', -0.3141646059088571),\n",
       "   ('Push', -0.31081318641137406),\n",
       "   ('Access', -0.3065076641200989),\n",
       "   ('Staff', -0.2981492348974993),\n",
       "   ('To', -0.19224985471630684),\n",
       "   ('Stormy', 0.1485958595159968),\n",
       "   ('Daniels', 0.14859585951599674),\n",
       "   ('Publicly', 0.13976335415335658),\n",
       "   ('Thanks', 0.13976335415335647)]),\n",
       " (8,\n",
       "  [('primary', -0.21917625930368417),\n",
       "   ('calendar', -0.21785856529696732),\n",
       "   ('will', -0.19811441312785813),\n",
       "   ('by', 0.1950644007954282),\n",
       "   ('Democrats', -0.19110573151590016),\n",
       "   ('rearranging', -0.19013936407753385),\n",
       "   ('vote', -0.1884463004546166),\n",
       "   ('presidential', -0.1857671960467649),\n",
       "   ('on', -0.18243956708851788),\n",
       "   ('resign', 0.17193763560136244)]),\n",
       " (9,\n",
       "  [('to', 0.23363337648729154),\n",
       "   ('or', -0.2206451624547229),\n",
       "   ('Supreme', -0.21193946541632383),\n",
       "   ('Kagan', -0.20985430080929618),\n",
       "   ('Democrats?', -0.20985430080929618),\n",
       "   ('Sotomayor', -0.20985430080929618),\n",
       "   ('Court', -0.20968851592778806),\n",
       "   ('open', -0.20466792604630085),\n",
       "   ('seat', -0.2035858050479882),\n",
       "   ('Will', -0.18608918806193983)]),\n",
       " (10,\n",
       "  [('other', -0.22096619965552414),\n",
       "   ('RealClearPolitics', -0.16814151532539595),\n",
       "   ('at', -0.1553790719330297),\n",
       "   ('involuntary', 0.1508449656268598),\n",
       "   ('manslaughter', 0.15084496562685978),\n",
       "   ('Baldwin', 0.15015925493747226),\n",
       "   ('Alec', 0.149877046115717),\n",
       "   ('POLITICO', -0.14732039644266),\n",
       "   ('Tyre', -0.14276609760217196),\n",
       "   ('charged', 0.14269028621997942)]),\n",
       " (11,\n",
       "  [('other', -0.2596019469599179),\n",
       "   ('Tyre', 0.21427550937720444),\n",
       "   ('broken', 0.17487152909143686),\n",
       "   ('funeral,', 0.17487152909143686),\n",
       "   ('bodycam', 0.17487152909143686),\n",
       "   ('for', -0.17459749084828421),\n",
       "   ('video', 0.17407210700727763),\n",
       "   ('promise', 0.1708437561681927),\n",
       "   (\"Nichols'\", 0.1681022048178286),\n",
       "   ('police', 0.16659621172045902)]),\n",
       " (12,\n",
       "  [('Biden', 0.24423966943083503),\n",
       "   ('primary', -0.20034856213118377),\n",
       "   ('calendar', -0.19979604901601755),\n",
       "   ('rearranging', -0.18994774059734956),\n",
       "   ('vote', -0.18188249683269256),\n",
       "   ('Democrats', -0.17168699154280095),\n",
       "   ('presidential', -0.16848796141075048),\n",
       "   ('will', -0.14611555843749752),\n",
       "   ('Money', 0.136836753387026),\n",
       "   ('Americans', 0.12189169932771729)]),\n",
       " (13,\n",
       "  [('be', 0.15564042413644658),\n",
       "   ('the', 0.15108281961112513),\n",
       "   ('involuntary', -0.14932270903586478),\n",
       "   ('manslaughter', -0.14932270903586473),\n",
       "   ('Baldwin', -0.14876929301871564),\n",
       "   ('Alec', -0.14601041427428135),\n",
       "   ('formally', -0.13792229935010883),\n",
       "   ('been', -0.13332991072723774),\n",
       "   ('hush', 0.132992680594519),\n",
       "   ('criminally', 0.132992680594519)]),\n",
       " (14,\n",
       "  [('Money', -0.1516977743145851),\n",
       "   ('ABC', 0.14225988714719184),\n",
       "   ('So', 0.14083281125456057),\n",
       "   ('Cuts', -0.13455186844162695),\n",
       "   ('Maddow', -0.13164601307436247),\n",
       "   ('Bone', -0.13164601307436244),\n",
       "   ('Hush', -0.13164601307436244),\n",
       "   ('Rachel', -0.13164601307436244),\n",
       "   ('Analysis', -0.13164601307436236),\n",
       "   ('News', 0.12173862746811615)]),\n",
       " (15,\n",
       "  [('Daily', 0.18707094618725525),\n",
       "   ('Caller', 0.18707094618725525),\n",
       "   ('DeSantis', 0.18606030510114005),\n",
       "   ('Money', -0.14359232049798223),\n",
       "   ('His', 0.14099387538192446),\n",
       "   ('On', 0.12259435452518061),\n",
       "   ('from', -0.11408589342037422),\n",
       "   ('Withheld', -0.11310888139930445),\n",
       "   ('Trump:', 0.10879930850138649),\n",
       "   ('bodycam', -0.10841776865872828)]),\n",
       " (16,\n",
       "  [('Haley', 0.25057382596552347),\n",
       "   ('Nikki', 0.2179379199231373),\n",
       "   ('15', 0.1694698772955896),\n",
       "   ('February', 0.166052156002826),\n",
       "   ('Report:', 0.15428415848180405),\n",
       "   ('calendar', -0.13498479616447637),\n",
       "   ('Announce', 0.1339177547179384),\n",
       "   ('primary', -0.13375700699799498),\n",
       "   ('be', 0.1315096618864703),\n",
       "   ('rearranging', -0.12973056179938036)]),\n",
       " (17,\n",
       "  [('in', -0.17110026937482198),\n",
       "   ('Money', 0.14650435458799752),\n",
       "   ('While', 0.14329339813010533),\n",
       "   ('Haley', 0.1411495541115696),\n",
       "   ('Nikki', 0.13570340652870447),\n",
       "   ('Analysis', 0.1267140394246156),\n",
       "   ('Hush', 0.12671403942461554),\n",
       "   ('Maddow', 0.12671403942461548),\n",
       "   ('Rachel', 0.12671403942461543),\n",
       "   ('Bone', 0.12671403942461537)]),\n",
       " (18,\n",
       "  [('ABC', -0.1891978822014135),\n",
       "   ('DeSantis', 0.16282633599462212),\n",
       "   ('by', -0.16239737392817213),\n",
       "   ('Daily', -0.14101418954854922),\n",
       "   ('Caller', -0.14101418954854922),\n",
       "   ('I', 0.13926308474372986),\n",
       "   ('Rare,', -0.1376548882675647),\n",
       "   ('comet', -0.1376548882675647),\n",
       "   ('pass', -0.13720096240770058),\n",
       "   ('green', -0.13598062594591454)]),\n",
       " (19,\n",
       "  [('POLITICO', 0.23795343983552622),\n",
       "   ('ABC', 0.19815392415658056),\n",
       "   ('other', -0.1847784301825821),\n",
       "   ('reelected', 0.17748516468683434),\n",
       "   ('got', 0.17748516468683434),\n",
       "   ('snaps', 0.17748516468683434),\n",
       "   ('Trump:', 0.17183667381123316),\n",
       "   ('back', 0.16347407160717156),\n",
       "   ('I', 0.15422463984026238),\n",
       "   ('-', 0.13359654827590878)]),\n",
       " (20,\n",
       "  [('While', -0.15379824143534038),\n",
       "   ('House', -0.13931648944862166),\n",
       "   ('RealClearPolitics', 0.13785426819665164),\n",
       "   ('Duty', -0.1193427327612321),\n",
       "   (\"'Serving\", -0.1193427327612321),\n",
       "   ('Officer', -0.1193427327612321),\n",
       "   ('Community', -0.1193427327612321),\n",
       "   (\"Loved'\", -0.1193427327612321),\n",
       "   ('Suddenly', -0.11809443114385795),\n",
       "   ('Dies', -0.11809443114385795)]),\n",
       " (21,\n",
       "  [('Give', 0.20469544028838293),\n",
       "   (\"Don't\", 0.1440958684962344),\n",
       "   ('Student', 0.1440958684962344),\n",
       "   ('Beats', 0.1440958684962344),\n",
       "   ('Teacher', 0.1440958684962344),\n",
       "   ('Female', 0.1440958684962344),\n",
       "   ('Physically', 0.1440958684962344),\n",
       "   ('F***', 0.1440958684962344),\n",
       "   ('9th-Grade', 0.1440958684962344),\n",
       "   (\"Not':\", 0.1440958684962344)]),\n",
       " (22,\n",
       "  [('RealClearPolitics', -0.23598204529457295),\n",
       "   ('Committee', -0.15134425491409748),\n",
       "   ('ABC', 0.14595178003134213),\n",
       "   ('Ethics', -0.14585266196950417),\n",
       "   ('With', -0.13748309547997295),\n",
       "   ('Election', -0.1354526315420725),\n",
       "   ('Deniers', -0.13129792375881205),\n",
       "   ('Stacked', -0.13129792375881205),\n",
       "   ('House', -0.12177367201814664),\n",
       "   ('High-Tech', -0.11427000224468511)]),\n",
       " (23,\n",
       "  [('RealClearPolitics', -0.21193359963460795),\n",
       "   ('Americans', 0.18619279126834765),\n",
       "   ('skeptical', 0.18562837152735714),\n",
       "   (\"GOP's\", 0.18562837152735714),\n",
       "   ('already', 0.18341564525018),\n",
       "   ('probes', 0.18297569344982423),\n",
       "   ('have', 0.1761666116417821),\n",
       "   ('Vox', 0.15234058590663657),\n",
       "   ('House', 0.14783239230958625),\n",
       "   ('lie', 0.13776259171216315)]),\n",
       " (24,\n",
       "  [('RealClearPolitics', 0.21019963832803887),\n",
       "   (\"That's\", -0.15517381728043084),\n",
       "   ('Customers', -0.15517381728043084),\n",
       "   ('Developing', -0.15517381728043084),\n",
       "   ('Attempts', -0.15517381728043084),\n",
       "   (\"'Invisible'\", -0.15517381728043084),\n",
       "   (\"Lowe's\", -0.15517381728043084),\n",
       "   ('Rampant', -0.15517381728043084),\n",
       "   ('System', -0.15517381728043084),\n",
       "   ('High-Tech', -0.15517381728043084)])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model_topics = lsi_model.show_topics(formatted=False)\n",
    "lsi_model_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.23242971e-02,  4.23242971e-02,  9.74917372e-02, ...,\n",
       "         9.46665739e-05,  9.46665739e-05,  9.46665739e-05],\n",
       "       [-1.61389377e-01, -1.61389377e-01, -2.47778649e-01, ...,\n",
       "         4.05416730e-05,  4.05416730e-05,  4.05416730e-05],\n",
       "       [-8.53912153e-02, -8.53912153e-02, -1.55423739e-01, ...,\n",
       "        -5.37195072e-05, -5.37195072e-05, -5.37195072e-05],\n",
       "       ...,\n",
       "       [ 1.42144228e-02,  1.42144228e-02, -1.00674276e-02, ...,\n",
       "        -3.32726230e-06, -3.32726230e-06, -3.32726230e-06],\n",
       "       [ 3.19839330e-02,  3.19839330e-02, -1.18478838e-02, ...,\n",
       "        -2.62584525e-05, -2.62584525e-05, -2.62584525e-05],\n",
       "       [ 5.76181789e-03,  5.76181789e-03,  1.83985712e-02, ...,\n",
       "         4.42204864e-06,  4.42204864e-06,  4.42204864e-06]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.224*\"-\" + 0.222*\"The\" + 0.178*\"New\" + 0.178*\"York\" + 0.173*\"Trump\"'),\n",
       " (1,\n",
       "  '-0.286*\"Trump\" + -0.248*\"Stormy\" + -0.248*\"Daniels\" + -0.192*\"For\" + 0.174*\"Post\"'),\n",
       " (2,\n",
       "  '0.265*\"Latest\" + 0.265*\"HuffPost\" + 0.174*\"News\" + -0.155*\"Stormy\" + -0.155*\"Daniels\"'),\n",
       " (3,\n",
       "  '-0.243*\"George\" + -0.237*\"For\" + -0.191*\"Santos\" + -0.188*\"Committees\" + -0.185*\"Steps\"'),\n",
       " (4,\n",
       "  '-0.297*\"5th\" + -0.297*\"Devastates\" + -0.297*\"Matters\" + -0.297*\"Nicolle\" + -0.293*\"Taking\"'),\n",
       " (5,\n",
       "  '-0.201*\"Santos\" + -0.186*\"Vox\" + -0.183*\"Down\" + -0.178*\"Committees\" + -0.174*\"George\"'),\n",
       " (6,\n",
       "  '0.292*\"York\" + 0.277*\"Times\" + 0.276*\"New\" + -0.175*\"Vox\" + -0.168*\"charged\"'),\n",
       " (7,\n",
       "  '-0.314*\"Computers\" + -0.314*\"Investigators\" + -0.311*\"Push\" + -0.307*\"Access\" + -0.298*\"Staff\"'),\n",
       " (8,\n",
       "  '-0.219*\"primary\" + -0.218*\"calendar\" + -0.198*\"will\" + 0.195*\"by\" + -0.191*\"Democrats\"'),\n",
       " (9,\n",
       "  '0.234*\"to\" + -0.221*\"or\" + -0.212*\"Supreme\" + -0.210*\"Kagan\" + -0.210*\"Democrats?\"')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(num_topics=10, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
