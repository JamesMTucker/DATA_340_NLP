{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Applied NLP Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Overview\n",
    "\n",
    "* Named Entity Recognition (NER) using SpaCy and Transformers\n",
    "* Text summarization using Transformers\n",
    "* Text generation using Transformers\n",
    "* Analyzing `Fake news` using Transformers and ChatGPT\n",
    "* Semantic role labeling using Transformers and ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) using SpaCy and Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER) is the task of identifying named entities in text and classifying them into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text} is type {ent.label_} - index_location: {ent.start_char}:{ent.end_char}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### more complex data\n",
    "\n",
    "# https://pubmed.ncbi.nlm.nih.gov/37071411/\n",
    "text = \"\"\"\n",
    "Most patients were initially certified for a 1:1 (∆9-tetrahydrocannabinol:cannabidiol) tincture.\n",
    "Eight-seven percent of patients (n = 60) were noted to exhibit an improvement in any PD symptom after starting MC.\n",
    "Symptoms with the highest incidence of improvement included cramping/dystonia, pain, spasticity, lack of appetite, dyskinesia, and tremor.\n",
    "After starting MC, 56% of opioid users (n = 14) were able to decrease or discontinue opioid use with an average daily morphine milligram equivalent change from 31 at baseline to 22 at the last follow-up visit.\n",
    "The MC was well-tolerated with no severe AEs reported and low rate of MC discontinuation due to AEs (n = 4).\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text} is type {ent.label_} - index_location: {ent.start_char}:{ent.end_char}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using SciSpacy\n",
    "\n",
    "import spacy\n",
    "import scispacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_scibert\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text} is type {ent.label_} - index_location: {ent.start_char}:{ent.end_char}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Entity: patients is type ENTITY - index_location: 6:14\n",
    "Entity: ∆9-tetrahydrocannabinol:cannabidiol) tincture is type ENTITY - index_location: 51:96\n",
    "Entity: patients is type ENTITY - index_location: 121:129\n",
    "Entity: improvement is type ENTITY - index_location: 164:175\n",
    "Entity: PD is type ENTITY - index_location: 183:185\n",
    "Entity: symptom is type ENTITY - index_location: 186:193\n",
    "Entity: MC is type ENTITY - index_location: 209:211\n",
    "Entity: Symptoms is type ENTITY - index_location: 213:221\n",
    "Entity: incidence is type ENTITY - index_location: 239:248\n",
    "Entity: improvement is type ENTITY - index_location: 252:263\n",
    "Entity: cramping/dystonia is type ENTITY - index_location: 273:290\n",
    "Entity: pain is type ENTITY - index_location: 292:296\n",
    "Entity: spasticity is type ENTITY - index_location: 298:308\n",
    "Entity: lack of appetite is type ENTITY - index_location: 310:326\n",
    "Entity: dyskinesia is type ENTITY - index_location: 328:338\n",
    "Entity: tremor is type ENTITY - index_location: 344:350\n",
    "Entity: MC is type ENTITY - index_location: 367:369\n",
    "Entity: opioid is type ENTITY - index_location: 378:384\n",
    "Entity: users is type ENTITY - index_location: 385:390\n",
    "Entity: decrease is type ENTITY - index_location: 413:421\n",
    "Entity: discontinue is type ENTITY - index_location: 425:436\n",
    "Entity: opioid is type ENTITY - index_location: 437:443\n",
    "Entity: daily is type ENTITY - index_location: 464:469\n",
    "Entity: morphine is type ENTITY - index_location: 470:478\n",
    "Entity: milligram is type ENTITY - index_location: 479:488\n",
    "...\n",
    "Entity: low rate is type ENTITY - index_location: 620:628\n",
    "Entity: MC is type ENTITY - index_location: 632:634\n",
    "Entity: discontinuation is type ENTITY - index_location: 635:650\n",
    "Entity: AEs is type ENTITY - index_location: 658:661\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline('ner', model='dslim/bert-base-NER', tokenizer='dslim/bert-base-NER', grouped_entities=True)\n",
    "ner(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/d4data/biomedical-ner-all\n",
    "ner = pipeline('ner', model='d4data/biomedical-ner-all', tokenizer='d4data/biomedical-ner-all', grouped_entities=True)\n",
    "ner(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your own pipeline\n",
    "\n",
    "* Extract semantic triples from the text then perform NER on the extracted triples\n",
    "* Use Stanford CoreNLP to extract semantic triples from the text then perform NER on the extracted triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "from stanza.server import CoreNLPClient\n",
    "stanza.install_corenlp()\n",
    "\n",
    "## extract triples from the text\n",
    "triples = []\n",
    "\n",
    "# define the properties\n",
    "config = {\n",
    "    \"annotators\": \"tokenize,ssplit,pos,lemma,ner,parse,depparse,coref,openie\",\n",
    "    \"openie.max_entailments_per_clause\": \"100\",\n",
    "    \"openie.threads\": \"4\",\n",
    "    \"memory\": \"16G\",\n",
    "    \"endpoint\": \"http://localhost:9020\",\n",
    "}\n",
    "\n",
    "client = CoreNLPClient(annotators=config['annotators'], memory=config['memory'], endpoint=config['endpoint'])\n",
    "\n",
    "document = client.annotate(text)\n",
    "for i, sentence in enumerate(document.sentence):\n",
    "    for triple in sentence.openieTriple:\n",
    "        triples.append([triple.subject, triple.relation, triple.object])\n",
    "        \n",
    "triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze the triples for NER\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for i, triple in enumerate(triples):\n",
    "    doc = \" \".join(triple)\n",
    "    doc = nlp(doc)\n",
    "    for ent in doc.ents:\n",
    "        print(f'Entity: {ent.text} is type {ent.label_} - index_location: {ent.start_char}:{ent.end_char}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = pipeline('ner', model='dslim/bert-base-NER', tokenizer='dslim/bert-base-NER', grouped_entities=True)\n",
    "\n",
    "for i, triple in enumerate(triples):\n",
    "    doc = \" \".join(triple)\n",
    "    doc = ner(doc)\n",
    "    for ent in doc:\n",
    "        print(f'Entity: {ent[\"word\"]} is type {ent[\"entity_group\"]} - index_location: {ent[\"start\"]}:{ent[\"end\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization using Transformers\n",
    "\n",
    "There are two types of text summarization:\n",
    "\n",
    "* Extractive summarization: Extracting a subset of the original text to form the summary\n",
    "* Abstractive summarization: Generating new text to form the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive summarization\n",
    "\n",
    "There are several extractive summarization techniques:\n",
    "\n",
    "* LexRank - LexRank is a graph-based algorithm that uses the PageRank algorithm to rank sentences based on their similarity to other sentences in the text.\n",
    "* SentRank - SentRank is a graph-based algorithm that uses the PageRank algorithm to rank sentences based on their similarity to other sentences in the text.\n",
    "* Luhn - Uses TF-IDF to rank sentences based on their similarity to other sentences in the text.\n",
    "* SumBasic - Utilize the frequency of words in the text to rank sentences. (abstract-like)\n",
    "* KL-Sum - Kullback-Leibler divergence is used to rank sentences based on their similarity to other sentences in the text.\n",
    "* LSA - Latent semantic analysis or indexing uses singular value decomposition to compute matrices for analyzing relationships between sets of observations.\n",
    "* K-Means - K-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import BloomTokenizerFast\n",
    "from transformers import BloomForCausalLM\n",
    "\n",
    "MODEL = BloomForCausalLM.from_pretrained('bigscience/bloom-560m')\n",
    "TOKENIZER = BloomTokenizerFast.from_pretrained('bigscience/bloom-560m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarization of our text using the Bloom model\n",
    "\n",
    "def summarize_text(text: str, tokenizer=TOKENIZER, min_output=40, max_output=100, max_length=80, model=MODEL):\n",
    "    \"\"\"Take a string of text and generate a summary\"\"\"\n",
    "    tokens_input = tokenizer.encode(\"summarize: \" + text, return_tensors='pt', max_length=max_length, truncation=True)\n",
    "    ids = model.generate(tokens_input, min_length=min_output, max_length=max_output)\n",
    "    summary = tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_text(text)\n",
    "display(HTML(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News\n",
    "\n",
    "What is fake news?\n",
    "\n",
    "* Fake news is a type of yellow journalism or propaganda that consists of deliberate misinformation or hoaxes spread via traditional print and broadcast news media or online social media.\n",
    "* Fake news can be published to intentionally or circumstantially damage the reputation of a person or entity, or make money through advertising revenue.\n",
    "* But ... fake news is not always false. The label can be used to discredit news that is critical of a person or organization, or to draw attention away from critical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake News Detection processing\n",
    "\n",
    "* Is it a news article?\n",
    "* Is there consensus on the truthfulness of the article?\n",
    "* If yes, return the consensus\n",
    "* If no, continue\n",
    "  * What is challenged in the article?\n",
    "    * Sentiment analysis - can shed light on the overall tone of the article (positive, negative, neutral) - heatmap of the article by paragraph or section\n",
    "    * Named entity recognition - can we identify the entities in the article (people, places, organizations, etc.)\n",
    "    * Can we perform semantic role labeling on the article?\n",
    "    * Are there references to other sources?\n",
    "\n",
    "\n",
    "\n",
    "adapted from Rothman, D. _Transformers for Natural Language Processing_. O'Reilly Media, Inc., 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using vectors to analyze sentence or document similarity\n",
    "\n",
    "#### Dot product\n",
    "\n",
    "The dot product or inner product of two vectors is defined as:\n",
    "\n",
    "$$ \\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i b_i $$\n",
    "\n",
    "\n",
    "With our vectors are defined as:\n",
    "\n",
    "$$ |v| = \\sqrt{\\sum^{N}_{i=1} v^2_i}$$\n",
    "\n",
    "* The longer the vector, the larger the magnitude\n",
    "* More frequent words will have larger magnitude\n",
    "* Raw dot product is not normalized - how can we use it to measure similarity?\n",
    "\n",
    "\n",
    "#### Normalized dot product\n",
    "\n",
    "$$ \\vec{a} \\cdot \\vec{b} = \\frac{\\sum_{i=1}^{n} a_i b_i}{\\sqrt{\\sum^{N}_{i=1} a^2_i} \\sqrt{\\sum^{N}_{i=1} b^2_i}} $$\n",
    "\n",
    "#### Cosine similarity\n",
    "\n",
    "With the cosine similarity, we can measure the angle between two vectors. The cosine similarity is defined as:\n",
    "\n",
    "$$ \\text{cosine(a, b)  = } \\frac{a\\cdot b}{|a||b|} = \\frac{\\sum_{i=1}^{n} a_i b_i}{\\sqrt{\\sum^{N}_{i=1} a^2_i} \\sqrt{\\sum^{N}_{i=1} b^2_i}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity of words\n",
    "\n",
    "https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin\n",
    "\n",
    "### Cosine similarity of sentences and documents\n",
    "\n",
    "How can we use the word vectors to measure the similarity between sentences or documents?\n",
    "\n",
    "* Average the word vectors in the sentence or document\n",
    "* Calculate the cosine similarity between the two sentences or documents\n",
    "* Train a classifier to predict the similarity between sentences or documents\n",
    "* Train a sentence embedding model to generate sentence or document vectors\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc2Vec??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.infer_vector([\"system\", \"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "df = pd.read_csv('../datasets/news-2023-02-01.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df['title'].tolist()\n",
    "titles = [title for title in titles if type(title) == str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([title.split() for title in titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the reviews into Gensim bag-of-words vectors\n",
    "corpus = [dictionary.doc2bow(title.split()) for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Gensim TF-IDF model on the corpus\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the corpus into Gensim TF-IDF vectors\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the first 10 documents\n",
    "for doc in tfidf_corpus[:10]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Gensim LSI model on the TF-IDF vectors\n",
    "lsi_model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model_topics = lsi_model.show_topics(formatted=False)\n",
    "lsi_model_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model.print_topics(num_topics=10, num_words=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
