{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving NLP Problems with Recurrent Neural Networks\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [Part 1: Understanding RNNs](#part1)\n",
    "- [Part 2: Part of Speech Tagging](#part2)\n",
    "- [Part 3: Text Generation](#part3)\n",
    "- [Part 4: Sentiment Analysis](#part4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.load_ipython_extensions([\n",
    "  \"nb-mermaid/nb-mermaid\"\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding RNNs <a id='part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Sequence to Sequence Models\n",
    "\n",
    "* Sequence data is data that is ordered in some way. For example, a sequence of words in a sentence, a sequence of characters in a word, a sequence of pixels in an image, a sequence of notes in a song, a sequence of frames in a video, and so on.\n",
    "\n",
    "* Unlike Bag-of-Words models, sequence models can take into account the order of the words in a sentence. This makes them ideal for tasks such as machine translation, speech recognition, and text summarization.\n",
    "\n",
    "* We will follow the standard conventions and model sequence data as follows:\n",
    "\n",
    "$$x^{(i)} = (x_1^{(i)}, x_2^{(i)}, \\ldots, x_T^{(i)})$$\n",
    "\n",
    "Where $T$ is the length of the sequence and $x_t^{(i)}$ is the $t^{th}$ element of the $i^{th}$ sequence in the training set.\n",
    "\n",
    "### 1.2 Different categories of sequence models\n",
    "\n",
    "* one to one - input layer is a single value (vector or scalar), output layer is a single value (vector or scalar). For example, image classification is a one to one model.\n",
    "* one to many - input layer is a single value (vector or scalar), output layer is a sequence. For example, image captioning is a one to many model.\n",
    "* many to one - input layer is a sequence, output layer is a single value (vector or scalar). For example, sentiment analysis is a many to one model.\n",
    "* many to many - input layer is a sequence, output layer is a sequence. For example, machine translation is a many to many model. Some variants of this model depend on the synchronization of the input and output sequences. For example, in video classification, the input and output sequences are synchronized, whereas in machine translation, the input and output sequences are not synchronized.\n",
    "\n",
    "<center><img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width=\"800\" height=\"300\"></center>\n",
    "\n",
    "N.B.: a rectangle is a vector and arrows are functions. \n",
    "\n",
    "source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Introduction to RNNs\n",
    "\n",
    "Recurrent Neural Networks (RNNs) represent a class of artificial neural networks designed to recognize patterns in sequences of data, such as time series data, speech, text, and more. They are distinguished by their \"memory\", realized through loops that allow information persistenceâ€”a feature critical for tasks requiring the understanding of context or the handling of sequential data.\n",
    "\n",
    "1. **Sequential Data Handling**: RNNs are specifically structured to handle sequential data by maintaining a form of \"memory\" of previous inputs while processing current ones. This is crucial in fields like Natural Language Processing (NLP) where the order of words (sequence) carries significant meaning.\n",
    "\n",
    "2. **Temporal Dynamics**: Unlike traditional feedforward neural networks, RNNs possess connections that loop back, enabling them to maintain information over time. This aspect introduces temporal dynamics into the network, allowing it to keep track of temporal dependencies in the input data.\n",
    "\n",
    "3. **Backpropagation Through Time (BPTT)**: Training RNNs involves a variant of backpropagation called Backpropagation Through Time (BPTT), which unrolls the network over time and computes gradients to update the weights to minimize a loss function.\n",
    "\n",
    "4. **Vanishing and Exploding Gradient Problems**: RNNs are known to suffer from vanishing and exploding gradient problems during training, which are challenges tied to the mathematical computations of gradients in the network. Variants like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) have been introduced to mitigate these issues.\n",
    "\n",
    "5. **Applications**: RNNs find applications across a variety of fields including NLP for tasks like language modeling, translation, and sentiment analysis, and in other domains like time-series prediction, and audio recognition.\n",
    "\n",
    "6. **Statistical Concepts**: The functioning and training of RNNs are deeply rooted in statistical concepts such as probability theory and optimization. They represent a probabilistic approach to modeling sequential data.\n",
    "\n",
    "#### RNNs in TensorFlow and PyTorch\n",
    "\n",
    "Implementing RNNs using frameworks like TensorFlow and PyTorch is a standard practice in the field. Both frameworks provide user-friendly APIs for building and training RNNs. \n",
    "\n",
    "- **TensorFlow**:\n",
    "   - Official Documentation on RNNs: [TensorFlow Recurrent Neural Networks](https://www.tensorflow.org/guide/keras/rnn)\n",
    "   - Tutorial: [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)\n",
    "\n",
    "- **PyTorch**:\n",
    "   - Official Documentation on RNNs: [PyTorch Recurrent Neural Networks](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "   - Tutorial: [Time Sequence Prediction](https://pytorch.org/tutorials/beginner/time_sequence_prediction_train.html)\n",
    "  \n",
    "\n",
    "```python\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(128, activation='tanh', input_shape=(None, 1)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#...\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = SimpleRNN()\n",
    "#...\n",
    "```\n",
    "\n",
    "In these snippets, a simple RNN is defined with 128 hidden units. In TensorFlow, the `SimpleRNN` layer is used, while in PyTorch, the `nn.RNN` module is utilized. The network is then compiled (TensorFlow) or instantiated (PyTorch), ready to be trained on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Architecture of RNNs\n",
    "\n",
    "#### Standard feedforward neural network\n",
    "\n",
    "Standarad Feedforward Neural Network\n",
    "\n",
    "```mermaid\n",
    "    graph BT\n",
    "    i[Input] --> h((Hidden Layer))\n",
    "    h --> o[Output]\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network feedforward\n",
    "\n",
    "```mermaid\n",
    "    graph BT\n",
    "    i[Input] --> h((Hidden Layer))\n",
    "    h --> h\n",
    "    h --> o[Output]\n",
    "```\n",
    "\n",
    "Recall that in standard neural network data is processed by passing the inputs to the forward layer (or hidden layer) and then to the output layer. In a recurrent neural network, the hidden layer receives the input and the current time step from the previous step. This allows the network to process the data sequentially.\n",
    "\n",
    "<center><img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_07.png?raw=true\" width=\"800\" height=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img source: https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_07.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Single and Multi Layer RNNs\n",
    "\n",
    "\n",
    "#### Single-layer RNNs\n",
    "\n",
    "1. **Architecture**: A single-layer RNN consists of a single layer of recurrent neurons. Each neuron receives input from the current time step and also has a recurrent connection that captures information from the previous time step.\n",
    "   \n",
    "2. **Recurrent Connections**: These connections enable the network to maintain a form of memory, which is crucial for processing sequences of data. The state of the recurrent neurons at any given time step is influenced by the input at that time step and the state of the recurrent neurons at the previous time step.\n",
    "\n",
    "3. **Training**: Training a single-layer RNN typically involves unfolding the network through time and applying backpropagation, a process known as Backpropagation Through Time (BPTT).\n",
    "\n",
    "4. **Limitations**: Single-layer RNNs are often limited in their ability to capture long-term dependencies in the data due to the vanishing or exploding gradient problem, which arises during the training process.\n",
    "\n",
    "#### Multi-layer RNNs\n",
    "\n",
    "1. **Architecture**: Multi-layer RNNs, often referred to as Deep Recurrent Neural Networks, consist of multiple layers of recurrent neurons. Each layer receives input from the preceding layer, which allows the network to learn hierarchical representations of the data.\n",
    "   \n",
    "2. **Hierarchical Learning**: The ability to learn hierarchical representations is beneficial in many tasks, as it enables the network to capture more complex patterns in the data. Each layer can learn to represent different levels of abstraction, which can be particularly useful in tasks like language modeling or speech recognition.\n",
    "   \n",
    "3. **Training**: Training multi-layer RNNs also involves BPTT. However, the presence of multiple layers can exacerbate the vanishing or exploding gradient problem and often necessitates the use of techniques like gradient clipping or advanced recurrent units like LSTMs or GRUs to mitigate these issues.\n",
    "   \n",
    "4. **Improved Performance**: Multi-layer RNNs often exhibit better performance on complex tasks as compared to single-layer RNNs due to their ability to learn more complex representations of the data.\n",
    "\n",
    "#### Summary\n",
    "In summary, single-layer RNNs consist of a single layer of recurrent neurons, making them simpler but often less capable of handling complex patterns in data. On the other hand, multi-layer RNNs have multiple layers of recurrent neurons, which enable them to learn hierarchical representations of the data, often yielding better performance on complex tasks.\n",
    "\n",
    "Here's a simplified Python code snippet to illustrate the difference between single and multi-layer RNNs using TensorFlow:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Single-layer RNN\n",
    "single_layer_rnn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(128, activation='tanh', input_shape=(None, 1))\n",
    "])\n",
    "\n",
    "# Multi-layer RNN\n",
    "multi_layer_rnn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(128, activation='tanh', input_shape=(None, 1), return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(128, activation='tanh')\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "The key difference here is the 'return_sequences=True' parameter in the first layer of the multi-layer RNN, which ensures that the output from the first layer is passed as a sequence to the second layer.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "<center><img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_04.png?raw=true\" width=\"800\" height=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 RNN activation functions\n",
    "\n",
    "Activation functions are a pivotal part of neural networks, including Recurrent Neural Networks (RNNs). They introduce non-linear properties to the system, enabling the network to learn from the error back-propagated through the network, and consequently, capture complex patterns in the data.\n",
    "\n",
    "#### Activation Functions for RNNs:\n",
    "\n",
    "1. **Hyperbolic Tangent (tanh)**: \n",
    "   - The tanh function squashes its input to be between -1 and 1, making it a good choice for maintaining the values within a reasonable range during backpropagation through time. \n",
    "   \n",
    "2. **Rectified Linear Unit (ReLU) and its Variants**:\n",
    "   - ReLU is popular due to its simplicity and the fact that it reduces the likelihood of the vanishing gradient problem.\n",
    "   - Variants of ReLU like Leaky ReLU or Parametric ReLU can be used to prevent dead neurons and the vanishing gradient problem.\n",
    "\n",
    "3. **Sigmoid**:\n",
    "   - The sigmoid function squashes its input to be between 0 and 1. It is particularly useful in binary classification tasks like in the output layer of a network.\n",
    "\n",
    "4. **Gated Activation Functions**:\n",
    "   - Gated recurrent units (GRUs) and Long Short-Term Memory units (LSTMs) use gated activation functions to control the flow of information through the network which can be particularly useful in learning long-term dependencies.\n",
    "\n",
    "#### Considerations for Selecting an Activation Function:\n",
    "\n",
    "1. **Vanishing and Exploding Gradients**:\n",
    "   - The choice of activation function can influence the stability of the training process. For instance, ReLU and its variants can mitigate the vanishing gradient problem, a common issue in RNNs.\n",
    "   \n",
    "2. **Learning Long-term Dependencies**:\n",
    "   - Gated activation functions in LSTMs and GRUs help in learning long-term dependencies by controlling the flow of information, which can be crucial in many sequence processing tasks.\n",
    "   \n",
    "3. **Computational Efficiency**:\n",
    "   - Simpler activation functions like ReLU are computationally more efficient as compared to more complex gated activation functions.\n",
    "   \n",
    "4. **Task-Specific Requirements**:\n",
    "   - The nature of the task at hand also dictates the choice of the activation function. For instance, a sigmoid activation function might be suitable for binary classification tasks.\n",
    "\n",
    "5. **Empirical Performance**:\n",
    "   - Often the choice of activation function might come down to empirical performance on a specific task or dataset.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "In RNNs, the choice of activation function is critical. Popular choices include tanh, ReLU and its variants, sigmoid, and gated activation functions like those used in LSTMs and GRUs. The decision on which activation function to use can be influenced by a variety of factors including the problem of vanishing and exploding gradients, the necessity to learn long-term dependencies, computational efficiency, the specific requirements of the task, empirical performance, and theoretical insights into the data or problem at hand.\n",
    "\n",
    "##### Tensorflow example\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Creating an RNN with a tanh activation function\n",
    "rnn_tanh = tf.keras.layers.SimpleRNN(units=128, activation='tanh', input_shape=(None, 1))\n",
    "\n",
    "# Creating an RNN with a ReLU activation function\n",
    "rnn_relu = tf.keras.layers.SimpleRNN(units=128, activation='relu', input_shape=(None, 1))\n",
    "```\n",
    "\n",
    "##### Pytorch example\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Creating an RNN with a tanh activation function\n",
    "rnn_tanh = nn.RNN(input_size=1, hidden_size=128, nonlinearity='tanh', batch_first=True)\n",
    "\n",
    "# Creating an RNN with a ReLU activation function\n",
    "rnn_relu = nn.RNN(input_size=1, hidden_size=128, nonlinearity='relu', batch_first=True)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Some problems with RNNs\n",
    "\n",
    "#### Vanishing and Exploding Gradients\n",
    "\n",
    "<center><img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_08.png?raw=true\" width=\"900\" height=\"500\"></center>\n",
    "\n",
    "The training of neural networks involves a process known as backpropagation, which is the method of computing gradients of the loss function with respect to the model parameters for updating these parameters. However, this process can sometimes be hindered due to the issues of vanishing and exploding gradients, particularly in recurrent neural networks (RNNs) which deal with sequential data.\n",
    "\n",
    "##### Vanishing Gradient Problem\n",
    "\n",
    "1. **Mechanism**: The vanishing gradient problem arises when the gradients of the loss function become too small for the network to learn effectively. As the gradient values approach zero, the updates to the weights during the training process become negligible, leading to a network that cannot learn from the data.\n",
    "\n",
    "2. **Cause**: This often occurs in deep networks or RNNs with long sequences due to the repeated multiplication of gradients through layers or time steps, especially when using activation functions like the sigmoid or tanh that squash their input into a small range.\n",
    "\n",
    "3. **Impact**: The vanishing gradient problem can cause training to be very slow, and the network may get stuck during training, leading to poor performance.\n",
    "\n",
    "<center><img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_08.png?raw=true\" width=\"600\" height=\"250\"></center>\n",
    "\n",
    "##### Exploding Gradient Problem\n",
    "\n",
    "1. **Mechanism**: Conversely, the exploding gradient problem occurs when gradient values become too large, leading to very large updates to the weights during the training process.\n",
    "\n",
    "2. **Cause**: This can occur due to the repeated multiplication of gradients through layers or time steps, especially in the presence of large parameter values or large input values.\n",
    "\n",
    "3. **Impact**: The exploding gradient problem can cause training to diverge, leading to an unstable network and, often, poor performance.\n",
    "\n",
    "<center><img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_08.png?raw=true\" width=\"600\" height=\"250\"></center>\n",
    "\n",
    "##### Desirable Scenario\n",
    "\n",
    "1. **Controlled Gradient Magnitude**: A desirable scenario is one where the magnitudes of the gradients are controlled and remain within a reasonable range throughout the training process.\n",
    "\n",
    "2. **Stable Training**: Stable and consistent training with a well-tuned learning rate, proper initialization of weights, and potentially regularization to prevent overfitting.\n",
    "\n",
    "3. **Mitigation Techniques**: Employing techniques to mitigate vanishing and exploding gradients, such as:\n",
    "   - Gradient clipping to prevent gradients from exceeding a defined threshold.\n",
    "   - Truncated backpropagation through time (TBPTT) which limits the number of time steps considered during backpropagation.\n",
    "   - Using advanced recurrent units like Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU) which are designed to combat the vanishing gradient problem.\n",
    "\n",
    "<center><img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_08.png?raw=true\" width=\"600\" height=\"250\"></center>\n",
    "\n",
    "##### Summary\n",
    "\n",
    "In summary, the vanishing gradient problem is characterized by gradients becoming too small to effectively update the network weights during training, often caused by the choice of activation function or network depth. On the other hand, the exploding gradient problem is marked by overly large gradients causing unstable training and potentially divergent behavior. A desirable scenario maintains gradient magnitudes within a controlled range, enabling stable training and effective learning. Techniques like gradient clipping, proper initialization, and the use of particular activation functions or advanced recurrent units can help achieve this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Long Short-Term Memory (LSTM) Units\n",
    "\n",
    "* \"Long Short-Term Memory\" S. Hochreiter, J. Schmidhuber, _Neural Computation_ 9(8):1735-1780, 1997\n",
    "* \"Learning to Forget: Continual Prediction with LSTM\" F. A. Gers, J. Schmidhuber, _Neural Computation_ 12(10):2451-2471, 2000\n",
    "\n",
    "#### LSTM \"Memory Cell\"\n",
    "\n",
    "<center><img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch15/figures/15_09.png?raw=true\" width=\"800\" height=\"450\"></center>\n",
    "\n",
    "$\\odot$ = element-wise multiplication \\\n",
    "$\\oplus$ = element-wise summation \\\n",
    "$x^{(t)}$ = input vector at time step $t$ \\\n",
    "$h^{(t-1)}$ = hidden units at time $t - 1$ \\\n",
    "$\\~C$ = candidate values \\\n",
    "$\\text{forget-gate}$ = forget gate allows the network to forget information from the previous time step \\\n",
    "$\\text{input-gate}$ = input gate allows the network to update the memory cell \\\n",
    "$\\text{output-gate}$ = output gate decides how to update the values of hidden units\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Part of Speech Tagging <a id='part2'></a>\n",
    "\n",
    "We encountered Part of Speech Tags in previous lectures, but we used existing models. Let's train a new model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from nltk.corpus import treebank, brown, conll2000\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Part of Speech\n",
    "\n",
    "We can create our own data set by drawing on previous data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize the POS Tags\n",
    "\n",
    "We will standardize the tag sets with the universal tag set.\n",
    "\n",
    "The universal tag set is a list of 12 tags that are used across all languages. They can be found online: https://universaldependencies.org/u/pos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tagged = treebank.tagged_sents(tagset='universal') + brown.tagged_sents(tagset='universal') + conll2000.tagged_sents(tagset='universal') + conll2000.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sentence example:', sentences_tagged[0])\n",
    "print('Dataset size: ', len(sentences_tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n",
    "\n",
    "Let's get the data in a shape we can train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, sent_tags = [], []\n",
    "\n",
    "for s in sentences_tagged:\n",
    "    sentence, tags = zip(*s)\n",
    "    sents.append(list(sentence))\n",
    "    sent_tags.append(list(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sents[0])\n",
    "print(sent_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sents), len(sent_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create our train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sents, sent_tags, test_size=1 - train_ratio, random_state=42)\n",
    "\n",
    "X_val, x_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train: {len(X_train)}, X_val: {len(X_val)}, x_text: {len(x_test)}')\n",
    "print(f'y_train: {len(y_train)}, y_val: {len(y_val)}, y_text: {len(y_test)}')\n",
    "print(f'X_train[0]: {X_train[0]}')\n",
    "print(f'y_train[0]: {y_train[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(oov_token='UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "print(f'Vocabulary size: {len(tokenizer.word_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'POS Tags: {len(tag_tokenizer.word_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seqs = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train_seqs[0]: {X_train_seqs[0]}')\n",
    "print(f'X_train[0]: {X_train[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize our tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_seqs = tag_tokenizer.texts_to_sequences(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'y_train_seqs[0]: {y_train_seqs[0]}')\n",
    "print(f'y_train[0]: {y_train[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_seqs = tokenizer.texts_to_sequences(X_val)\n",
    "y_val_seqs = tag_tokenizer.texts_to_sequences(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "Padding is a way to make sure all of our sentences are the same length. We will use the pad_sequences function from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = len(max(X_train_seqs, key=len))\n",
    "print(f'Max length: {MAX_LEN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = keras.preprocessing.sequence.pad_sequences(X_train_seqs, maxlen=MAX_LEN, padding='post')\n",
    "print(f'X_train_padded[0]: {X_train_padded[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_padded = keras.preprocessing.sequence.pad_sequences(y_train_seqs, maxlen=MAX_LEN, padding='post')\n",
    "X_val_padded = keras.preprocessing.sequence.pad_sequences(X_val_seqs, maxlen=MAX_LEN, padding='post')\n",
    "y_val_padded = keras.preprocessing.sequence.pad_sequences(y_val_seqs, maxlen=MAX_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert our tags to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_categories = keras.utils.to_categorical(y_train_padded)\n",
    "print(f'y_train_categories[0]: {y_train_categories[0][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check labels\n",
    "idx = np.argmax(y_train_categories[0][0])\n",
    "print(f'idx: {idx}')\n",
    "print(f'Label: {tag_tokenizer.index_word[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the validation labels\n",
    "y_val_categories = keras.utils.to_categorical(y_val_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(tokenizer.word_index) + 1 # add 1 for padding\n",
    "embedding_dim = 128\n",
    "num_classes = len(tag_tokenizer.word_index) + 1 # add 1 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Embedding(input_dim=num_tokens,\n",
    "                           output_dim=embedding_dim,\n",
    "                           input_length=MAX_LEN,\n",
    "                           mask_zero=True))\n",
    "\n",
    "model.add(\n",
    "    layers.Bidirectional(\n",
    "        layers.LSTM(128,\n",
    "                    return_sequences=True,\n",
    "                    kernel_initializer=tf.keras.initializers.random_normal(seed=42)\n",
    "                    )\n",
    "        )\n",
    "    )\n",
    "\n",
    "model.add(layers.Dense(num_classes, activation='softmax', kernel_initializer=tf.keras.initializers.random_normal(seed=42)))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit(X_train_padded,\n",
    "                    y_train_categories,\n",
    "                    epochs=50,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_val_padded, y_val_categories),\n",
    "                    callbacks=[callback]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the test data and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seqs = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_padded = keras.preprocessing.sequence.pad_sequences(X_test_seqs, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "y_test_seqs = tag_tokenizer.texts_to_sequences(y_test)\n",
    "y_test_padded = keras.preprocessing.sequence.pad_sequences(y_test_seqs, maxlen=MAX_LEN, padding='post')\n",
    "y_test_categories = keras.utils.to_categorical(y_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test_padded, y_test_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "\n",
    "We want to productionize our model. We will use the model to predict the part of speech tags for new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_data = [\n",
    "    'The University was closed today because it snowed.',\n",
    "    'The White House released an executive order on the use of AI in government.',\n",
    "    'Richard Feynman was a professor at Caltech.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_(sentences: list[str]) -> list[list[str]]:\n",
    "    sent_seqs = tokenizer.texts_to_sequences(sentences)\n",
    "    sents_padded = keras.preprocessing.sequence.pad_sequences(sent_seqs, \n",
    "                                                              maxlen=MAX_LEN,\n",
    "                                                              padding='post')\n",
    "    \n",
    "    # predict the tags of the client sentences\n",
    "    predictions = model.predict(sents_padded)\n",
    "    \n",
    "    # create softmax predictions\n",
    "    predictions_ = tf.nn.softmax(predictions)\n",
    "    print(f'predictions: {predictions_[0][0]}')\n",
    "    \n",
    "    sentence_tags = []\n",
    "    \n",
    "    for i, preds in enumerate(predictions):\n",
    "        \n",
    "        # extract the indices of the highest predictions\n",
    "        tags_seq = [np.argmax(p) for p in preds[:len(sent_seqs[i])]]\n",
    "        \n",
    "        words = [tokenizer.index_word[w] for w in sent_seqs[i]]\n",
    "        tags = [tag_tokenizer.index_word[t] for t in tags_seq]\n",
    "        sentence_tags.append(list(zip(words, tags)))\n",
    "    \n",
    "    return sentence_tags    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_client_sents = predict_(client_data)\n",
    "print(f'Sample: {tagged_client_sents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# save the model\n",
    "model.save('/content/drive/MyDrive/Colab Notebooks/13_Recurrent_Neural_Networks/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text Generation <a id='part3'></a>\n",
    "\n",
    "Let's see if we can improve on our Tolkien text generator. If you recall that our Naive Bayes model was able to learn the character level probabilities of the text. But the output of the model was not very good. Let's see if we can improve on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Character Level Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from nltk.corpus import treebank, brown, conll2000\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "here = Path().cwd()\n",
    "\n",
    "# go one level up\n",
    "parent = here.parent\n",
    "\n",
    "# read the LOTR files into memory\n",
    "files = list(parent.glob('datasets/*.txt'))\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for f in files:\n",
    "    # if LOTR is in the file name\n",
    "    if 'lotr' in f.name.lower():\n",
    "        print(f'Reading {f.name}')\n",
    "        with open(f, 'r') as file:\n",
    "            corpus.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus: list[str]) -> list[str]:\n",
    "    # concatenate the corpus into a single string\n",
    "    corpus = ' '.join(corpus)\n",
    "    # remove unneccessary whitespace\n",
    "    corpus = \" \".join(corpus.split())\n",
    "    # remove underscores\n",
    "    corpus = corpus.replace('_', '')\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolkien = clean_corpus(corpus)\n",
    "tolkien[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolkien = tolkien[:1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([tolkien])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Examine the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Vocabulary size: {len(tokenizer.word_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = tokenizer.texts_to_sequences([tolkien])[0]\n",
    "print(f'Text length: {len(seq)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts([seq[:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset from the sequence\n",
    "slices = tf.data.Dataset.from_tensor_slices(seq)\n",
    "type(slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator to list\n",
    "list(slices.take(5).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_time_steps = 100  # length of the input sequences\n",
    "window_size = input_time_steps + 1\n",
    "windows = slices.window(window_size, shift=1, drop_remainder=True)  # shift by one for next character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in windows.take(3):\n",
    "    arr = list(w.as_numpy_iterator())\n",
    "    print(len(arr), arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset from the windows\n",
    "dataset = windows.flat_map(lambda w: w.batch(window_size))\n",
    "\n",
    "for d in dataset.take(2):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the batches for training\n",
    "batch_size = 32\n",
    "\n",
    "batches = dataset.shuffle(1024).batch(batch_size)\n",
    "\n",
    "for b in batches.take(2):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_batches = batches.map(lambda batch: (batch[:, :-1], batch[:, 1:]))\n",
    "\n",
    "for b in xy_batches.take(2):\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in xy_batches.take(1):\n",
    "  print(\"x1 length: \", len(b[0][0].numpy()))\n",
    "  print(\"x1: \", b[0][0].numpy())\n",
    "  print(\"\\n\")\n",
    "  print(\"y1 length: \", len(b[1][0].numpy()))\n",
    "  print(\"y1: \", b[1][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(tokenizer.word_index) + 1 # add 1 for padding\n",
    "\n",
    "xy_batches = xy_batches.map(lambda inputs, labels: (tf.one_hot(inputs, depth=num_tokens), labels))\n",
    "\n",
    "for b in xy_batches.take(1):\n",
    "  print(\"x1: \", b[0][0].numpy())\n",
    "  print(\"\\n\")\n",
    "  print(\"y1: \", b[1][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the autotune option will automatically tune the buffer size\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.LSTM(128, return_sequences=True, input_shape=[None, num_tokens], recurrent_dropout=0.2))\n",
    "model.add(layers.LSTM(128, return_sequences=True, input_shape=[None, num_tokens], recurrent_dropout=0.2))\n",
    "\n",
    "model.add(layers.Dense(num_tokens, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our callbacks to track the model performance during training\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath='.', save_weights_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(xy_batches, epochs=10, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "trained_model = keras.models.load_model('/content/drive/MyDrive/Models/tolkien/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seed_text, num_chars=200, temperature=1):\n",
    "\n",
    "  text = seed_text\n",
    "\n",
    "  for _ in range(num_chars):\n",
    "\n",
    "    # Encode the input string.\n",
    "    input = np.array(tokenizer.texts_to_sequences([text[-100:]]))\n",
    "    input = tf.one_hot(input, num_tokens)\n",
    "\n",
    "    # compute the next character probabilities.\n",
    "    preds = model.predict(input)[0, -1:, :]\n",
    "    preds = tf.math.log(preds) / temperature\n",
    "\n",
    "    # Sample next character and add to running text.\n",
    "    next_char = tf.random.categorical(preds, num_samples=1)\n",
    "    next_char = tokenizer.sequences_to_texts(next_char.numpy())[0]\n",
    "\n",
    "    text += next_char\n",
    "\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(trained_model, tokenizer, \"Sam, Frodo, and Gandalf were running when\", num_chars=300, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Sentiment Analysis <a id='part4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB reviews dataset\n",
    "dataset, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train, validate = dataset['train'], dataset['test']\n",
    "\n",
    "# Examine the dataset\n",
    "train.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a review\n",
    "for eg, label in train.take(1):\n",
    "  print(\"text: \", eg.numpy())\n",
    "  print(\"label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the counts of the labels in the training and validation sets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_labels = [label.numpy() for _, label in train]\n",
    "validate_labels = [label.numpy() for _, label in validate]\n",
    "\n",
    "# plot the counts of the labels in the training and validation sets\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_labels)\n",
    "plt.title('Training labels')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(validate_labels)\n",
    "plt.title('Validation labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch the data\n",
    "BUFFER_SIZE = 10_000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# create a dataset of batches - see https://www.tensorflow.org/guide/data_performance#prefetching\n",
    "train_dataset = train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "validate_dataset = validate.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eg, label in train_dataset.take(1):\n",
    "  print(\"texts: \", eg.numpy()[:3])\n",
    "  print(\"labels: \", label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Tokenize and vectorize our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our vocabulary size\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Create a text vectorization layer\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Examine the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the vocabulary\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the encoded text\n",
    "encoder_example = encoder(eg)[:3].numpy()\n",
    "encoder_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the original text to the encoded text\n",
    "for n in range(3):\n",
    "  print(\"Original: \", eg[n].numpy())\n",
    "  print(\"Round-trip: \", \" \".join(vocab[encoder_example[n]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=validate_dataset, validation_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 Validate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate our model\n",
    "val_loss, val_acc = model.evaluate(validate_dataset)\n",
    "\n",
    "print('Test Loss:', val_loss)\n",
    "print('Test Accuracy:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate our model\n",
    "val_loss, val_acc = model.evaluate(validate_dataset)\n",
    "\n",
    "print('Test Loss:', val_loss)\n",
    "print('Test Accuracy:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "  \n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test our model\n",
    "sample_text = ('The movie was a joke. The animation and the graphics '\n",
    "               'were out of this world, but the acting was horrendous.'\n",
    "               'I would not recommend this movie.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the sentiment\n",
    "prediction = model.predict([sample_text])\n",
    "\n",
    "# Show the results\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Visualize our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our LSTM model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw plot of the model\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 BiLSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_bilstm = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Compile our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile our bidirectional LSTM model\n",
    "model_bilstm.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our model\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=validate_dataset, validation_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "  \n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Notebook](https://colab.research.google.com/drive/1Et8IO-BCBdSYkhkTcCbo624gqfJD9H7h#scrollTo=cTqhw4K0qIBx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
