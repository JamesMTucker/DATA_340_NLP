{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Overview\n",
    "\n",
    "- Classification as a Machine Learning Problem and Human Learning Problem\n",
    "- Naive Bayes Classification Formal Definitions\n",
    "- Naive Bayes Classification Example\n",
    "- Refactoring Code for Naive Bayes Classification\n",
    "  - Naive Bayes Classification Example: Spam Filtering\n",
    "  - Naive Bayes Classification Example: Twitter Sentiment Analysis\n",
    "- Beyond Naivete: Where Naive Bayes Classification Breaks Down"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification as a Machine Learning Problem and Human Learning Problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"float: right; padding: 0px 0px 0px 3px;\" src=\"https://pictures.abebooks.com/inventory/31359579518.jpg\" height=\"500\" width=\"400\"> \"Categorization is not a matter to be taken lightly. There is nothing more basic than categorization to our thought, perception, action, and speech. Every time we see something as a kind of thing, for example, a tree, we are categorizing. Whenever we reason about kinds of things--chairs, nations, illnesses, emotions, any kind of thing at all--we are employing categories.\" [5-6] --[George Lakoff](https://en.wikipedia.org/wiki/George_Lakoff)\n",
    "\n",
    "\n",
    "In machine learning we can think of classification as a problem of assigning a label to an input. In human learning we can think of classification as a problem of assigning a label to an input. In both cases, the labels are discrete categories. In both cases, the inputs are features. In both cases, the goal is to learn a function that maps inputs to labels.\n",
    "\n",
    "Some examples of classification problems:\n",
    "\n",
    "* Spam detection\n",
    "* Sentiment analysis\n",
    "* Language identification\n",
    "* Authorship identification\n",
    "* Topic identification\n",
    "* Part-of-speech tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The goal of classification is to take a single observation, extract some useful features, and thereby classify the observation into one of a set of discrete classes.\" [59] Jurafsky and Martin."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification Formal Definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning overview:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Neuron.drawio.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs, Outputs, and Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Inputs = features = x`\n",
    "\n",
    "* `Outputs = labels or categories = y` = $Y \\in \\{c_1, c_2, \\ldots, c_n\\}$\n",
    "\n",
    "* Training set = $X_{train} = \\{(x_1, c_1), (x_2, c_2), \\ldots, (x_m, c_m)\\}$ \n",
    "\n",
    "* Validation set = $y_{val} = \\{(y_1, c_1), (y_2, c_2), \\ldots, (y_m, c_m)\\}$\n",
    "\n",
    "* Test set = $X_{test} = \\{(x_1, c_1), (x_2, c_2), \\ldots, (x_m, c_m)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes (1701-1761) was an English mathematician and statistician. He is known for his work on probability theory, including the law of total probability, the law of conditional probability, and the Bayes theorem. Bayes theorem is a fundamental result in statistics and machine learning. It is used to compute the posterior probability of an event given some evidence.\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "\n",
    "Thus, when we train we are computing the posterior probability of a label given some evidence. When we test we are computing the posterior probability of a label given some evidence.\n",
    "\n",
    "$$\\hat{c} = argmax_{c\\in C} P(c|d) = argmax_{c\\in C} \\frac{P(d|c)P(c)}{P(d)}$$\n",
    "\n",
    "We can simplify the above equation by dropping the denominator $P(d)$ because it is the same for all $c$.\n",
    "\n",
    "$$\\hat{y} = argmax_{c\\in C} P(c|d) = argmax_{c\\in C} P(d|c)P(c)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Probability and Likelihood\n",
    "\n",
    "Choose the class with the highest product of two probabilities: the prior probability of the class and the likelihood of the data given the class.\n",
    "\n",
    "$\\hat{c} = argmax_{c\\in C} P(c|d) =$ likelihood $P(d|c)$ prior $P(c)$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Assumption\n",
    "\n",
    "We have used the bag of words method in previous lectures. We are assuming that the order of the words in a document does not matter. We are assuming that the words in a document are independent of each other. This is the Naive Bayes assumption. It is a useful assumption because it allows us to simplify the computation of the likelihood. We can compute the likelihood by multiplying the probabilities of the individual words.\n",
    "\n",
    "$$P(d|c) = P(w_1, w_2, \\ldots, w_n|c) = P(w_1|c)P(w_2|c)\\ldots P(w_n|c)$$\n",
    "\n",
    "Ergo,\n",
    "\n",
    "$$c_{NB} = argmax_{c\\in C} P(c) = \\prod_{i\\in positions} P(w_i|c)$$\n",
    "\n",
    "We can optimize compute time by using the log of the likelihood. Postions = all word positions in the document.\n",
    "\n",
    "$$c_{NB} = argmax_{c\\in C} P(c) = \\sum_{i\\in positions} log(P(w_i|c))$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-Code from Jurafsky and Martin (63)\n",
    "\n",
    "```markdown\n",
    "\n",
    "function Train Naive Bayes(D,C) returns log P(c) and log P(w|c)\n",
    "    for each class c in C do\n",
    "        N_doc ← number of documents in D\n",
    "        N_c ← total number of documents in D in class c\n",
    "        logprior(c) ← log(N_c/N_doc)\n",
    "        V ← vocabulary of all words in D\n",
    "        bigdoc ← all words in documents with class c\n",
    "        for each word w in V do # calculate log P(w|c) terms\n",
    "            N_cw ← number of times w occurs in bigdoc with class c\n",
    "            N_w ← total number of words in bigdoc with class c\n",
    "            log P(w|c) ← log((N_cw + 1)/(N_w + |V| + 1))\n",
    "    return log P(c) and log P(w|c)\n",
    "```\n",
    "\n",
    "```markdown\n",
    "\n",
    "function Test Naive Bayes(d) returns class c\n",
    "    for each class c do\n",
    "        score(c) ← logprior(c)\n",
    "        for each position i in test-d do\n",
    "            word w ← word at position i in test-d\n",
    "            if w is in V then \n",
    "                score(c) ← score(c) + log P(w|c)\n",
    "    return class c with highest score # argmax\n",
    "```\n",
    "\n",
    "In sum, we calculate the mean, variance, and prior (frequency) for each class. Then, we make predictions by computing the log of the likelihood for each class and choosing the class with the highest log of the likelihood."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Evaluation\n",
    "\n",
    "* gold labels = human labels of the input data\n",
    "* predicted labels = labels predicted by the model\n",
    "\n",
    "* precision = $\\frac{TP}{TP + FP}$\n",
    "* recall = $\\frac{TP}{TP + FN}$\n",
    "* F1 or F-measure = $F\\beta \\frac{(\\beta^2 + 1)\\times precision \\times recall}{\\beta^2 precision + recall}$\n",
    "* accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code out the above in practice. We will create our own dataset for the first example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Get the number of samples and features\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "        \n",
    "        # initialize to zeros\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "        \n",
    "        # calculate the mean, variance, and prior for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            self._mean[idx, :] = X_c.mean(axis=0) # numpy.mean\n",
    "            self._var[idx, :] = X_c.var(axis=0) # numpy.var\n",
    "            self._priors[idx] = X_c.shape[0] / float(n_samples) # number of samples\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # calculate posterior probability for each class\n",
    "        posteriors = []\n",
    "        \n",
    "        # calculate posterior probability for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            posterior = np.sum(np.log(self._pdf(idx, x))) # gaussian distribution\n",
    "            posterior = posterior + prior\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        # return class with highest posterior probability\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "    \n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(- (x - mean) ** 2 / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data set creation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "nb = NaiveBayes()\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "predictions = nb.predict(X_test)\n",
    "\n",
    "print(f'Naive bayes classification accuracy: {accuracy(y_test, predictions):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print a confusion matrix using the scikit-learn library\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create the confusion matrix\n",
    "cfm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "## Plot the confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "## Visualize the confusion matrix\n",
    "sns.heatmap(cfm, annot=True, fmt='d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create a scatter plot of the features in the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a distribution plot of the features\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the features of the test data\n",
    "\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the distribution plot of the features\n",
    "sns.histplot(X[:, 0], kde=True, color='red', label='Feature 1')\n",
    "sns.histplot(X[:, 1], kde=True, color='blue', label='Feature 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the distribution of the test data\n",
    "\n",
    "sns.histplot(X_test[:, 0], kde=True, color='red', label='Feature 1')\n",
    "sns.histplot(X_test[:, 1], kde=True, color='blue', label='Feature 2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactoring Code for Naive Bayes Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification Example: Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the nltk data\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the positive and negative tweets\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "len(positive_tweets), len(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's plot the number of positive and negative tweets\n",
    "\n",
    "sns.histplot([len(tweet) for tweet in positive_tweets], kde=True, color='green', label='Positive')\n",
    "sns.histplot([len(tweet) for tweet in negative_tweets], kde=True, color='red', label='Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's use sklearn to split the data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(positive_tweets + negative_tweets,\n",
    "                                                    np.append(np.ones(len(positive_tweets)),\n",
    "                                                    np.zeros(len(negative_tweets))),\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0], X_test[0], y_train[0], y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the length of the tweets re. positive and negative tweets in the training and test sets\n",
    "\n",
    "sns.histplot([len(tweet) for tweet in X_train], kde=True, color='green', label='Positive', legend=True)\n",
    "sns.histplot([len(tweet) for tweet in X_test], kde=True, color='red', label='Negative', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the distribution of the classes in the training and test sets\n",
    "\n",
    "sns.barplot(x=['Positive', 'Negative'], y=[len(y_train[y_train == 1]), len(y_train[y_train == 0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=['Positive', 'Negative'], y=[len(y_test[y_test == 1]), len(y_test[y_test == 0])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and cleaning the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess the data\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"Input: tweet a string containing a tweet\n",
    "    Return:\n",
    "    tweets_clean: a list of words containing the processed tweet\n",
    "    \"\"\"\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # remove retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # Instantiate stemmer class\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Create stopwords list\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # Tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    \n",
    "    # Tokenize the tweets\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "    \n",
    "    return tweets_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Sntwitter to scrape tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test the function\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "test_tweets = []\n",
    "\n",
    "# for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#MurdaughTrial').get_items()):\n",
    "#     if i > 2:\n",
    "#         break\n",
    "#     test_tweets.append(tweet.content)\n",
    "    \n",
    "test_tweets = ['RT @Barnes_Law: The #MurdaughTrial is a great example of how the media can manipulate the public. The media has been pushing the narrative…',\n",
    "               'The #MurdaughTrial is a great example of how the media can manipulate the public. The media has been pushing the… https://t.co/2Z2Z2Z2Z2Z',\n",
    "               'I thought the #MurdaughTrial was rigged. Turns out it’s not so much a trial about a murder as it is about a killing an entire family.']\n",
    "               "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the scraped tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean the tweets\n",
    "\n",
    "test_tweets_cleaned = [process_tweet(tweet) for tweet in test_tweets]\n",
    "test_tweets_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of words and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create a dictionary of words and their frequencies\n",
    "def count_tweets(result, tweets, ys):\n",
    "    \"\"\"Input:\n",
    "    result: a dictionary that will contain the frequency of each pair (word, label)\n",
    "    tweets: a list of tweets\n",
    "    ys: an m x 1 array with the sentiment label of each tweet (either 0 or 1)\n",
    "    \"\"\"\n",
    "    # iterate through each tweet and its label\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        # process the tweet to get the words in the form of a list\n",
    "        for word in process_tweet(tweet):\n",
    "            # increment the word count for the pair (word, label)\n",
    "            pair = (word, y)\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a pseudo-labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test our function with our test tweets\n",
    "\n",
    "result = {}\n",
    "tweets = test_tweets\n",
    "ys = np.ones(len(tweets))\n",
    "count_tweets(result, tweets, ys)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's train our model\n",
    "\n",
    "freqs = count_tweets({}, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the results dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create a function to extract the features from the tweets\n",
    "\n",
    "def naive_bayes(freqs, X_train, y_train):\n",
    "    \"\"\"Train a Naive Bayes classifier on twitter data.\n",
    "\n",
    "    Args:\n",
    "        freqs (dict): dictionary of (word, label): frequency pairs\n",
    "        X_train (list): list of tweets\n",
    "        y_train (list): list of tweets\n",
    "        \n",
    "    returns:\n",
    "    logprior (float): log prior\n",
    "    loglikelihood (dict): dictionary of (word, label): log likelihood pairs\n",
    "    \"\"\"\n",
    "    ## Compare the code here with Jurafsky and Martin's pseudocode\n",
    "    \n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    \n",
    "    vocab = set([pair[0] for pair in freqs.keys()]) # words in the vocabulary\n",
    "    V = len(vocab) # number of unique words in the vocabulary\n",
    "    \n",
    "    # Calculate N_pos and N_neg tweets (number of positive and negative tweets)\n",
    "    N_pos, N_neg = 0, 0 # number of positive and negative tweets\n",
    "    \n",
    "    # Calculate the number of positive and negative tweets\n",
    "    for pair in freqs.keys():\n",
    "        # positive tweets\n",
    "        if pair[1] > 0:\n",
    "            N_pos += freqs[pair]\n",
    "        # negative tweets\n",
    "        else:\n",
    "            N_neg += freqs[pair]\n",
    "    \n",
    "    # Documents = total number of tweets\n",
    "    D = len(X_train)\n",
    "    \n",
    "    # Calculate # of positive and negative documents\n",
    "    D_pos = np.sum(y_train)\n",
    "    D_neg = D - D_pos\n",
    "    \n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    \n",
    "    for word in vocab:\n",
    "        \n",
    "        freq_pos = freqs.get((word, 1.0), 0)\n",
    "        freq_neg = freqs.get((word, 0.0), 0)\n",
    "        \n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "        \n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "    \n",
    "    return logprior, loglikelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test our function\n",
    "logprior, loglikelihood = naive_bayes(freqs, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior, loglikelihood # most probable class given the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test our model\n",
    "\n",
    "def predict_naive_bayes(tweet, logprior, loglikelihood):\n",
    "    \"\"\"Input:\n",
    "    tweet: a string\n",
    "    logprior: a number\n",
    "    loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "    p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "    \"\"\"\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    p = 0\n",
    "    p += logprior\n",
    "    \n",
    "    for word in word_l:\n",
    "        if word in loglikelihood:\n",
    "            p += loglikelihood[word]\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's use our tweets from above\n",
    "\n",
    "for t in test_tweets:\n",
    "    print( '%s -> %f' % (t, predict_naive_bayes(t, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test our model on the test set\n",
    "\n",
    "def test_naive_bayes(X_test, y_test, logprior, loglikelihood):\n",
    "    \"\"\"Input:\n",
    "    X_test: a list of tweets\n",
    "    y_test: (m, 1) array with the sentiment label of each tweet (either 0 or 1)\n",
    "    logprior: a number\n",
    "    loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "    accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Let's score the accuracy of our model\n",
    "    accuracy = 0\n",
    "    \n",
    "    # Our predictions will be stored in y_hat\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in X_test:\n",
    "        if predict_naive_bayes(tweet, logprior, loglikelihood) > 0:\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            y_hat.append(0)\n",
    "    \n",
    "    # error is the average of the absolute values of the differences between y_hat and y_test\n",
    "    error = np.mean(np.abs(y_hat - y_test))\n",
    "    \n",
    "    accuracy = 1 - error\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test our function\n",
    "print(\"Naive Bayes accuracy = %f\" % test_naive_bayes(X_test, y_test, logprior, loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's visualize some examples\n",
    "X_test[0:10], y_test[0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Naivete: Where Naive Bayes Classification Breaks Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(freqs, word):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary containing the words\n",
    "        word: string to lookup\n",
    "\n",
    "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    '''\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    \n",
    "    \n",
    "    pos_neg_ratio['positive'] = freqs.get((word, 1.0), 0)\n",
    "\n",
    "    pos_neg_ratio['negative'] = freqs.get((word, 0.0), 0)\n",
    "\n",
    "    # calculate the ratio of positive to negative counts for the word\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1)/(pos_neg_ratio['negative'] + 1)\n",
    "    \n",
    "    return pos_neg_ratio\n",
    "\n",
    "\n",
    "def threshold_lookup(freqs, label, threshold):\n",
    "    \"\"\"Input:\n",
    "    freqs: dictionary of (word, label): frequency pairs\n",
    "    threshold: words position in the scored list\n",
    "    \"\"\"\n",
    "\n",
    "    words = {}\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    for key in freqs.keys():\n",
    "        word, _ = key\n",
    "\n",
    "        # get the positive/negative ratio for a word\n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "\n",
    "        # if the label is 1 and the ratio is greater than or equal to the threshold...\n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
    "\n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            words[word] = pos_neg_ratio\n",
    "\n",
    "        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "\n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            words[word] = pos_neg_ratio\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ratio(freqs, 'peopl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_lookup(freqs, label=1, threshold=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some error analysis done for you\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(X_test, y_test):\n",
    "    # get the label prediction for the tweet\n",
    "    y_hat = predict_naive_bayes(x, logprior, loglikelihood)\n",
    "    \n",
    "    # if the prediction is not equal to the label, print the tweet and the prediction\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        \n",
    "        # print out the gold label ('y'), the predicted label ('y_hat'), and the tweet ('x')\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
