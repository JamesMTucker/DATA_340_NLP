{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Overview\n",
    "\n",
    "- Classification as a Machine Learning Problem and Human Learning Problem\n",
    "- Naive Bayes Classification Formal Definitions\n",
    "- Naive Bayes Classification Example\n",
    "- Refactoring Code for Naive Bayes Classification\n",
    "  - Naive Bayes Classification Example: Spam Filtering\n",
    "  - Naive Bayes Classification Example: Twitter Sentiment Analysis\n",
    "- Beyond Naivete: Where Naive Bayes Classification Breaks Down"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification as a Machine Learning Problem and Human Learning Problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"float: right; padding: 0px 0px 0px 3px;\" src=\"https://pictures.abebooks.com/inventory/31359579518.jpg\" height=\"500\" width=\"400\"> \"Categorization is not a matter to be taken lightly. There is nothing more basic than categorization to our thought, perception, action, and speech. Every time we see something as a kind of thing, for example, a tree, we are categorizing. Whenever we reason about kinds of things--chairs, nations, illnesses, emotions, any kind of thing at all--we are employing categories.\" [5-6] --[George Lakoff](https://en.wikipedia.org/wiki/George_Lakoff)\n",
    "\n",
    "\n",
    "In machine learning we can think of classification as a problem of assigning a label to an input. In human learning we can think of classification as a problem of assigning a label to an input. In both cases, the labels are discrete categories. In both cases, the inputs are features. In both cases, the goal is to learn a function that maps inputs to labels.\n",
    "\n",
    "Some examples of classification problems:\n",
    "\n",
    "* Spam detection\n",
    "* Sentiment analysis\n",
    "* Language identification\n",
    "* Authorship identification\n",
    "* Topic identification\n",
    "* Part-of-speech tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The goal of classification is to take a single observation, extract some useful features, and thereby classify the observation into one of a set of discrete classes.\" [59] Jurafsky and Martin."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification Formal Definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning overview:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Neuron.drawio.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs, Outputs, and Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Inputs = features = x`\n",
    "\n",
    "* `Outputs = labels or categories = y` = $Y \\in \\{c_1, c_2, \\ldots, c_n\\}$\n",
    "\n",
    "* Training set = $X_{train} = \\{(x_1, c_1), (x_2, c_2), \\ldots, (x_m, c_m)\\}$ \n",
    "\n",
    "* Validation set = $y_{val} = \\{(y_1, c_1), (y_2, c_2), \\ldots, (y_m, c_m)\\}$\n",
    "\n",
    "* Test set = $X_{test} = \\{(x_1, c_1), (x_2, c_2), \\ldots, (x_m, c_m)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes (1701-1761) was an English mathematician and statistician. He is known for his work on probability theory, including the law of total probability, the law of conditional probability, and the Bayes theorem. Bayes theorem is a fundamental result in statistics and machine learning. It is used to compute the posterior probability of an event given some evidence.\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "\n",
    "Thus, when we train we are computing the posterior probability of a label given some evidence. When we test we are computing the posterior probability of a label given some evidence.\n",
    "\n",
    "$$\\hat{c} = argmax_{c\\in C} P(c|d) = argmax_{c\\in C} \\frac{P(d|c)P(c)}{P(d)}$$\n",
    "\n",
    "We can simplify the above equation by dropping the denominator $P(d)$ because it is the same for all $c$.\n",
    "\n",
    "$$\\hat{y} = argmax_{c\\in C} P(c|d) = argmax_{c\\in C} P(d|c)P(c)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Probability and Likelihood\n",
    "\n",
    "Choose the class with the highest product of two probabilities: the prior probability of the class and the likelihood of the data given the class.\n",
    "\n",
    "$\\hat{c} = argmax_{c\\in C} P(c|d) =$ likelihood $P(d|c)$ prior $P(c)$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Assumption\n",
    "\n",
    "We have used the bag of words method in previous lectures. We are assuming that the order of the words in a document does not matter. We are assuming that the words in a document are independent of each other. This is the Naive Bayes assumption. It is a useful assumption because it allows us to simplify the computation of the likelihood. We can compute the likelihood by multiplying the probabilities of the individual words.\n",
    "\n",
    "$$P(d|c) = P(w_1, w_2, \\ldots, w_n|c) = P(w_1|c)P(w_2|c)\\ldots P(w_n|c)$$\n",
    "\n",
    "Ergo,\n",
    "\n",
    "$$c_{NB} = argmax_{c\\in C} P(c) = \\prod_{i\\in positions} P(w_i|c)$$\n",
    "\n",
    "We can optimize compute time by using the log of the likelihood. Postions = all word positions in the document.\n",
    "\n",
    "$$c_{NB} = argmax_{c\\in C} P(c) = \\sum_{i\\in positions} log(P(w_i|c))$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Naive Bayes\n",
    "\n",
    "Naive Bayes is a statistical classification technique based on Bayes Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that a particular fruit is an apple or an orange or a banana and that is why it is known as ‘Naive’.\n",
    "\n",
    "Bayes theorem provides a way of calculating posterior probability $P(c|x)$ from $P(c)$, $P(x)$, and $P(x|c)$. Look at the equation below:\n",
    "\n",
    "$$P(c|x) = \\frac{P(x|c) \\cdot P(c)}{P(x)}$$\n",
    "\n",
    "Here,\n",
    "- $P(c|x)$ is the posterior probability of class $c$ given predictor $x$,\n",
    "- $P(c)$ is the prior probability of class,\n",
    "- $P(x|c)$ is the likelihood which is the probability of predictor $x$ given class $c$,\n",
    "- $P(x)$ is the prior probability of predictor.\n",
    "\n",
    "\n",
    "Informally, Naive Bayes classifier is a fast, easy to understand, and highly scalable algorithm that's a good choice when dealing with text data and categorical data. Despite its simplicity and the naive assumption of feature independence, it can perform surprisingly well and is widely used in practice including spam filtering, sentiment analysis, and many other classification problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Evaluation\n",
    "\n",
    "* gold labels = human labels of the input data\n",
    "* predicted labels = labels predicted by the model\n",
    "\n",
    "* precision = $\\frac{TP}{TP + FP}$\n",
    "* recall = $\\frac{TP}{TP + FN}$\n",
    "* F1 or F-measure = $F\\beta \\frac{(\\beta^2 + 1)\\times precision \\times recall}{\\beta^2 precision + recall}$\n",
    "* accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code out the above in practice. We will create our own dataset for the first example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Get the number of samples and features\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "        \n",
    "        # initialize to zeros\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "        \n",
    "        # calculate the mean, variance, and prior for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            self._mean[idx, :] = X_c.mean(axis=0) # numpy.mean\n",
    "            self._var[idx, :] = X_c.var(axis=0) # numpy.var\n",
    "            self._priors[idx] = X_c.shape[0] / float(n_samples) # number of samples\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # calculate posterior probability for each class\n",
    "        posteriors = []\n",
    "        \n",
    "        # calculate posterior probability for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            posterior = np.sum(np.log(self._pdf(idx, x))) # gaussian distribution\n",
    "            posterior = posterior + prior\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        # return class with highest posterior probability\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "    \n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(- (x - mean) ** 2 / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data set creation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "nb = NaiveBayes()\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "predictions = nb.predict(X_test)\n",
    "\n",
    "print(f'Naive bayes classification accuracy: {accuracy(y_test, predictions):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print a confusion matrix using the scikit-learn library\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create the confusion matrix\n",
    "cfm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "## Plot the confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "## Visualize the confusion matrix\n",
    "sns.heatmap(cfm, annot=True, fmt='d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create a scatter plot of the features in the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a distribution plot of the features\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the features of the test data\n",
    "\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=predictions, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the distribution plot of the features\n",
    "sns.histplot(X[:, 0], kde=True, color='red', label='Feature 1')\n",
    "sns.histplot(X[:, 1], kde=True, color='blue', label='Feature 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the distribution of the test data\n",
    "\n",
    "sns.histplot(X_test[:, 0], kde=True, color='red', label='Feature 1')\n",
    "sns.histplot(X_test[:, 1], kde=True, color='blue', label='Feature 2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactoring Code for Naive Bayes Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification Example: Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the nltk data\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the positive and negative tweets\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "len(positive_tweets), len(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's plot the number of positive and negative tweets\n",
    "\n",
    "sns.histplot([len(tweet) for tweet in positive_tweets], kde=True, color='green', label='Positive')\n",
    "sns.histplot([len(tweet) for tweet in negative_tweets], kde=True, color='red', label='Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's use sklearn to split the data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(positive_tweets + negative_tweets,\n",
    "                                                    np.append(np.ones(len(positive_tweets)),\n",
    "                                                    np.zeros(len(negative_tweets))),\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=123)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0], X_test[0], y_train[0], y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the length of the tweets re. positive and negative tweets in the training and test sets\n",
    "\n",
    "sns.histplot([len(tweet) for tweet in X_train], kde=True, color='green', label='Positive', legend=True)\n",
    "sns.histplot([len(tweet) for tweet in X_test], kde=True, color='red', label='Negative', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the distribution of the classes in the training and test sets\n",
    "\n",
    "sns.barplot(x=['Positive', 'Negative'], y=[len(y_train[y_train == 1]), len(y_train[y_train == 0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=['Positive', 'Negative'], y=[len(y_test[y_test == 1]), len(y_test[y_test == 0])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and cleaning the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess the data\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"Input: tweet a string containing a tweet\n",
    "    Return:\n",
    "    tweets_clean: a list of words containing the processed tweet\n",
    "    \"\"\"\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    \n",
    "    # remove retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # Instantiate stemmer class\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Create stopwords list\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # Tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    \n",
    "    # Tokenize the tweets\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "    \n",
    "    return tweets_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dictionary of words and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create a dictionary of words and their frequencies\n",
    "def count_tweets(result, tweets, ys):\n",
    "    \"\"\"Input:\n",
    "    result: a dictionary that will contain the frequency of each pair (word, label)\n",
    "    tweets: a list of tweets\n",
    "    ys: an m x 1 array with the sentiment label of each tweet (either 0 or 1)\n",
    "    \"\"\"\n",
    "    # iterate through each tweet and its label\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        # process the tweet to get the words in the form of a list\n",
    "        for word in process_tweet(tweet):\n",
    "            # increment the word count for the pair (word, label)\n",
    "            pair = (word, y)\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our Naive Bayes classifier\n",
    "\n",
    "To train our model, we first need to build a dictionary of the tokens in our training set. Let's call the dictionary `freqs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's train our model\n",
    "\n",
    "freqs = count_tweets({}, X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the results dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create a function to extract the features from the tweets\n",
    "\n",
    "def naive_bayes(freqs, X_train, y_train):\n",
    "    \"\"\"Train a Naive Bayes classifier on twitter data.\n",
    "\n",
    "    Args:\n",
    "        freqs (dict): dictionary of (word, label): frequency pairs\n",
    "        X_train (list): list of tweets\n",
    "        y_train (list): list of tweets\n",
    "        \n",
    "    returns:\n",
    "    logprior (float): log prior\n",
    "    loglikelihood (dict): dictionary of (word, label): log likelihood pairs\n",
    "    \"\"\"\n",
    "    ## Compare the code here with Jurafsky and Martin's pseudocode\n",
    "    \n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    \n",
    "    vocab = set([pair[0] for pair in freqs.keys()]) # words in the vocabulary\n",
    "    V = len(vocab) # number of unique words in the vocabulary\n",
    "    \n",
    "    # Calculate N_pos and N_neg tweets (number of positive and negative tweets)\n",
    "    N_pos, N_neg = 0, 0 # number of positive and negative tweets\n",
    "    \n",
    "    # Calculate the number of positive and negative tweets\n",
    "    for pair in freqs.keys():\n",
    "        # positive tweets\n",
    "        if pair[1] > 0:\n",
    "            N_pos += freqs[pair]\n",
    "        # negative tweets\n",
    "        else:\n",
    "            N_neg += freqs[pair]\n",
    "    \n",
    "    # Documents = total number of tweets\n",
    "    D = len(X_train)\n",
    "    \n",
    "    # Calculate # of positive and negative documents\n",
    "    D_pos = np.sum(y_train)\n",
    "    D_neg = D - D_pos\n",
    "    \n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    \n",
    "    for word in vocab:\n",
    "        \n",
    "        freq_pos = freqs.get((word, 1.0), 0)\n",
    "        freq_neg = freqs.get((word, 0.0), 0)\n",
    "        \n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "        \n",
    "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
    "    \n",
    "    return logprior, loglikelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test our function\n",
    "logprior, loglikelihood = naive_bayes(freqs, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior, loglikelihood # most probable class given the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test our model\n",
    "\n",
    "def predict_naive_bayes(tweet, logprior, loglikelihood):\n",
    "    \"\"\"Input:\n",
    "    tweet: a string\n",
    "    logprior: a number\n",
    "    loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "    p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "    \"\"\"\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    p = 0\n",
    "    p += logprior\n",
    "    \n",
    "    for word in word_l:\n",
    "        if word in loglikelihood:\n",
    "            p += loglikelihood[word]\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's use our tweets from above\n",
    "\n",
    "test_tweets = [\n",
    "    'This is a happy tweet because I am learning NLP',\n",
    "    'The AI revolution will not silence us! We will fight for our rights!',\n",
    "    'Yesterday was a sad day for me. I lost my job.',\n",
    "]\n",
    "\n",
    "for t in test_tweets:\n",
    "    print( '%s -> %f' % (t, predict_naive_bayes(t, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test our model on the test set\n",
    "\n",
    "def test_naive_bayes(X_test, y_test, logprior, loglikelihood):\n",
    "    \"\"\"Input:\n",
    "    X_test: a list of tweets\n",
    "    y_test: (m, 1) array with the sentiment label of each tweet (either 0 or 1)\n",
    "    logprior: a number\n",
    "    loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "    accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Let's score the accuracy of our model\n",
    "    accuracy = 0\n",
    "    \n",
    "    # Our predictions will be stored in y_hat\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in X_test:\n",
    "        if predict_naive_bayes(tweet, logprior, loglikelihood) > 0:\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            y_hat.append(0)\n",
    "    \n",
    "    # error is the average of the absolute values of the differences between y_hat and y_test\n",
    "    error = np.mean(np.abs(y_hat - y_test))\n",
    "    \n",
    "    accuracy = 1 - error\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's test our function\n",
    "print(\"Naive Bayes accuracy = %f\" % test_naive_bayes(X_test, y_test, logprior, loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's visualize some examples\n",
    "X_test[0:10], y_test[0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Naivete: Where Naive Bayes Classification Breaks Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(freqs, word):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary containing the words\n",
    "        word: string to lookup\n",
    "\n",
    "    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n",
    "        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
    "    '''\n",
    "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
    "    \n",
    "    \n",
    "    pos_neg_ratio['positive'] = freqs.get((word, 1.0), 0)\n",
    "\n",
    "    pos_neg_ratio['negative'] = freqs.get((word, 0.0), 0)\n",
    "\n",
    "    # calculate the ratio of positive to negative counts for the word\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1)/(pos_neg_ratio['negative'] + 1)\n",
    "    \n",
    "    return pos_neg_ratio\n",
    "\n",
    "\n",
    "def threshold_lookup(freqs, label, threshold):\n",
    "    \"\"\"Input:\n",
    "    freqs: dictionary of (word, label): frequency pairs\n",
    "    threshold: words position in the scored list\n",
    "    \"\"\"\n",
    "\n",
    "    words = {}\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    for key in freqs.keys():\n",
    "        word, _ = key\n",
    "\n",
    "        # get the positive/negative ratio for a word\n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "\n",
    "        # if the label is 1 and the ratio is greater than or equal to the threshold...\n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
    "\n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            words[word] = pos_neg_ratio\n",
    "\n",
    "        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "\n",
    "            # Add the pos_neg_ratio to the dictionary\n",
    "            words[word] = pos_neg_ratio\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ratio(freqs, 'peopl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_lookup(freqs, label=1, threshold=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some error analysis done for you\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(X_test, y_test):\n",
    "    # get the label prediction for the tweet\n",
    "    y_hat = predict_naive_bayes(x, logprior, loglikelihood)\n",
    "    \n",
    "    # if the prediction is not equal to the label, print the tweet and the prediction\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        \n",
    "        # print out the gold label ('y'), the predicted label ('y_hat'), and the tweet ('x')\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification Example: Movie Review\n",
    "\n",
    "In the above, we precomputed the frequency of each word in our vocabulary. We can, however, implement a Naive Bayes classifier in a different manner. In this example, we will create some classes that will allow us to train a Naive Bayes classifier on the fly. We will use the [IMDB Movie Review Dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) from Kaggle. The dataset contains 50,000 movie reviews from IMDB. The reviews are labeled as positive or negative. The goal is to predict the sentiment of a movie review given the text of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## html display\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's create a dataframe to store the results\n",
    "\n",
    "## if you are working in Colab replace the path with the url:\n",
    "# imdb_dataset = 'https://github.com/JamesMTucker/DATA_340_NLP/blob/master/Notebooks/data/IMDB_Dataset.csv'\n",
    "\n",
    "df = pd.read_csv('../datasets/IMDB_Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create our train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly sample our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly select a sample from the training set\n",
    "idx = random.randint(0, len(X_train))\n",
    "\n",
    "# display the sample in HTML format\n",
    "print(display(HTML(f\"<p>Tweet: {X_train.iloc[idx]}</p>\")))\n",
    "print('label:', y_train.iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.positive_word_counts = {}\n",
    "        self.negative_word_counts = {}\n",
    "        self.positive_total_count = 0\n",
    "        self.negative_total_count = 0\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, data):\n",
    "        for text, label in data:\n",
    "            if label == 'positive':\n",
    "                self.positive_total_count += 1\n",
    "                for word in text.split():\n",
    "                    self.positive_word_counts[word] = self.positive_word_counts.get(word, 0) + 1\n",
    "                    self.vocab.add(word)\n",
    "            elif label == 'negative':\n",
    "                self.negative_total_count += 1\n",
    "                for word in text.split():\n",
    "                    self.negative_word_counts[word] = self.negative_word_counts.get(word, 0) + 1\n",
    "                    self.vocab.add(word)\n",
    "\n",
    "    def predict(self, text):\n",
    "        # Calculate the prior probability of each class\n",
    "        positive_prior = self.positive_total_count / (self.positive_total_count + self.negative_total_count + 1e-10)\n",
    "        negative_prior = self.negative_total_count / (self.positive_total_count + self.negative_total_count + 1e-10)\n",
    "\n",
    "        # Calculate the likelihood of the text given each class\n",
    "        positive_likelihood = 0\n",
    "        negative_likelihood = 0\n",
    "        for word in text.split():\n",
    "            if word in self.vocab:\n",
    "                # Add Laplace smoothing to avoid zero probability\n",
    "                positive_likelihood += math.log((self.positive_word_counts.get(word, 0) + 1) / (self.positive_total_count + len(self.vocab) + 1))\n",
    "                negative_likelihood += math.log((self.negative_word_counts.get(word, 0) + 1) / (self.negative_total_count + len(self.vocab) + 1))\n",
    "\n",
    "        # Calculate the posterior probability of each class\n",
    "        positive_posterior = math.exp(positive_likelihood) * positive_prior\n",
    "        negative_posterior = math.exp(negative_likelihood) * negative_prior\n",
    "\n",
    "        # Return the class with the highest posterior probability\n",
    "        if positive_posterior > negative_posterior:\n",
    "            return 'positive'\n",
    "        else:\n",
    "            return 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shape our data\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.train(zip(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test random sample\n",
    "random_idx = random.randint(0, len(X_test))\n",
    "result = nb.predict(X_test.iloc[random_idx])\n",
    "print(display(HTML(f\"<p>Tweet: {X_test.iloc[random_idx]}</p> <p>{y_test.iloc[random_idx]}</p>\")))\n",
    "print('prediction:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test accuracy\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = len(X_test)\n",
    "\n",
    "for text, label in zip(X_test, y_test):\n",
    "    pred = nb.predict(text)\n",
    "    if pred == label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we improve our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the following libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if you are working in Colab replace the path with the url:\n",
    "# imdb_dataset = 'https://github.com/JamesMTucker/DATA_340_NLP/blob/master/Notebooks/data/IMDB_Dataset.csv'\n",
    "\n",
    "df = pd.read_csv('../datasets/IMDB_Dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's convert the sentiment to labels\n",
    "sentiment = {'positive': 1, 'negative': 0}\n",
    "\n",
    "df['sentiment'] = df['sentiment'].map(sentiment)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's define our X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorize our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using the CountVectorizer class\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "validation_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors.shape, validation_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors[0], validation_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier using the MultinomialNB class\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_vectors, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = classifier.predict(validation_vectors)\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')  # Output: Accuracy: 0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a confusion matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "## visualize the confusion matrix\n",
    "cfm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "## plot the confusion matrix\n",
    "sns.heatmap(cfm, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'positive'], yticklabels=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output some of the misclassified tweets\n",
    "misclassified = np.where(y_pred != y_test)[0]\n",
    "print('Misclassified tweets:', len(misclassified))\n",
    "\n",
    "## create a dataframe to store the misclassified tweets\n",
    "misclassified_df = pd.DataFrame({'text': X_test.iloc[misclassified], 'actual': y_test.iloc[misclassified], 'predicted': y_pred[misclassified]})\n",
    "misclassified_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
