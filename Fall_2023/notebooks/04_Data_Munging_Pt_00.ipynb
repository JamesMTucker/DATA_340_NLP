{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/Notebooks/Lecture_07_2023_02_16.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 07: Regular Expressions and Webscraping\n",
    "\n",
    "## Lecture Overview\n",
    "\n",
    "* Regular Expressions\n",
    "* Webscraping\n",
    "* API Usage\n",
    "* Dataset Creation\n",
    "* Assignment 2 and 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://imgs.xkcd.com/comics/regular_expressions.png\"  width=\"600\" height=\"600\"></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions: Taking quasi-structured data and making it structured"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Regular Expressions?\n",
    "\n",
    "Regular expressions are a way to match patterns in text in the data type of a string. It takes some time to get used to reading regexes, but they are very powerful and can be used to extract pertinent information, clean up messy data, and/or format data for downstream analyses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example\n",
    "\n",
    "Let's say we have a string that contains a date in the format of `YYYY-MM-DD`. We want to extract the year, month, and day from this string. We can do this with the following regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year found: 2020\n",
      "Month found: 20\n",
      "Day found: 02\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "date = \"2020-02-16\"\n",
    "\n",
    "# Extract the year, month, and day from the date string\n",
    "year = re.findall(r\"\\d{4}\", date)[0]\n",
    "month = re.findall(r\"\\d{2}\", date)[1]\n",
    "day = re.findall(r\"\\d{2}\", date)[2]\n",
    "\n",
    "print(f'Year found: {year}\\nMonth found: {month}\\nDay found: {day}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our expression is written relative to the string from which we want to extract. If our data was more complex, we would need to use more complex regexes. For example, if we wanted to extract the year, month, and day from a string that contained multiple dates, we would need to use a regex that looks like this: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020', '9018', '2020']\n",
      "['20', '20', '02', '16', '49', '30', '90', '18', '20', '20', '20', '02', '17']\n",
      "['20', '20', '02', '16', '49', '30', '90', '18', '20', '20', '20', '02', '17']\n"
     ]
    }
   ],
   "source": [
    "conversation_023 = \"\"\"\n",
    "On 2020-02-16, I had a conversation with my friend.\n",
    "She was visiting family in Germany, so I had a hard time understanding the German phone number +49 30 901820.\n",
    "I also had a conversation with my mother on 2020-02-17.\n",
    "\"\"\"\n",
    "\n",
    "# Extract the year, month, and day from the date string\n",
    "year = re.findall(r\"\\d{4}\", conversation_023)\n",
    "month = re.findall(r\"\\d{2}\", conversation_023)\n",
    "day = re.findall(r\"\\d{2}\", conversation_023)\n",
    "\n",
    "print(year, month, day, sep=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to have some familarity with our data to know how simple or complex our regex needs to be. Regexes can be very complex, as we can see here:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Perl\n",
    "\n",
    "my @matches;\n",
    "\n",
    "say \"Matched!\" if m/\n",
    "    (?(DEFINE)\n",
    "        (?<QUOTE_MARK> ['\"])\n",
    "        (?<NOT_QUOTE_MARK> [^'\"])\n",
    "        (?<QUOTE>)\n",
    "            (\n",
    "                (?<quote>(?&QUOTE_MARK))\n",
    "                (?:\n",
    "                    (?&NOT_QUOTE_MARK)++\n",
    "                    |\n",
    "                    (?&QUOTE_MARK)\n",
    "            )*\n",
    "            \\g{quote}\n",
    "        )\n",
    "        (?{ @{$^R}, = $^N}})\n",
    "    )\n",
    "    (?&QUOTE) (?( @matches = @{$^R} ) )\n",
    "    /x;\n",
    "```\n",
    "\n",
    "Example from Brain D. Foy's [Mastering Regular Expressions](https://www.oreilly.com/library/view/mastering-regular-expressions/0596528124/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Regex syntax\n",
    "\n",
    "Regexes are written in a special syntax that is different from the syntax of Python. The syntax is as follows:\n",
    "\n",
    "* `.` matches any character\n",
    "* `^` matches the beginning of the string\n",
    "* `$` matches the end of the string\n",
    "* `*` matches zero or more of the preceding character\n",
    "* `+` matches one or more of the preceding character\n",
    "* `?` matches zero or one of the preceding character\n",
    "* `{n}` matches exactly n of the preceding character\n",
    "* `{n,}` matches n or more of the preceding character\n",
    "* `{,n}` matches 0 to n of the preceding character\n",
    "* `{m,n}` matches at least m and at most n of the preceding character\n",
    "* `[abc]` matches any character in the brackets\n",
    "* `[^abc]` matches any character not in the brackets\n",
    "* `|` matches either the preceding or the following character\n",
    "* `()` groups the preceding match\n",
    "\n",
    "### Regex functions\n",
    "\n",
    "There are a few functions in Python that can be used to work with regexes. The most common are `re.search()` and `re.findall()`. `re.search()` will return the first match of a regex in a string. `re.findall()` will return all matches of a regex in a string. Let's look at an example of each using some sample data from the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ISBN: 0-345-24223-8']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get the HTML from the webpage\n",
    "url = 'https://en.wikipedia.org/wiki/ISBN'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# Use BeautifulSoup to parse the HTML\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Get the text from the HTML\n",
    "text = soup.get_text()\n",
    "\n",
    "# Use regex to find all the ISBNs in the text\n",
    "regex = r'ISBN: [0-9-]+'\n",
    "matches = re.findall(regex, text)\n",
    "print(matches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'a take a look at this in more detail:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An SBN may be converted to an ISBN by prefixing the digit \"0\". For example, the second edition of Mr. J. G. Reeder Returns, published by Hodder in 1965, has \"SBN 340 01381 8\", where \"340\" indicates the publisher, \"01381\" is the serial number assigned by the publisher, and \"8\" is the check digit. By prefixing a zero, this can be converted to ISBN 0-340-01381-8; the check digit does not need to be re-calculated. Some publishers, such as Ballantine Books, would sometimes use 12-digit SBNs where the last three digits indicated the price of the book;[12] for example, Woodstock Handmade Houses had a 12-digit Standard Book Number of 345-24223-8-595 (valid SBN: 345-24223-8, ISBN: 0-345-24223-8),[13] and it cost US$5.95.[14]\n",
    "\n",
    "\n",
    "We can examine how this regex is working with the wonderful tool [Regex101](https://regex101.com/). And it's always best practice to consult the [Python documentation](https://docs.python.org/3/library/re.html) for more information about the `re` module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping: legalized data theft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"There is no glory in data.\" Something said to me by my MA thesis advisor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/dss_abegg.png\" height=\"400\", width=\"800\"></img></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the process of web scraping is not illegal, it is often frowned upon by the websites that you are scraping. It is important to be respectful of the websites that you are scraping and to make sure that you are not violating any terms of service. In addition, some websites will block your IP address if they want to prevent you from scraping their data. Web developers and web technologies can also be used to deter you from quick access from their data. For more information, see this [Join TechCrunch article](https://techcrunch.com/2022/04/18/web-scraping-legal-court/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAANzYrguMdHcwNL6AKW_UCTQORBn4CH1Idbtrp6YD-aNUv_Basl5x3ZyIgM54Z5bTbzuImyx5P0biFVkhswFoUQjUTRE-HDCFd_KOSGP1UT15q2N9JxZRYXBhjOkXJ1r7JDRlwm0fM4JVOeeJv6UfeuJGV3LFlKSn-UXMDWfP667p)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Scraping\n",
    "\n",
    "Simple scraping is extracting data from websites that do not use the more advanced web technologies, such as JavaScript, databases, and/or APIs. These pages are often static webpages. We can use simple methods to extract such kind of data.\n",
    "\n",
    "For example, we can use the `pandas` module to read in a table from a website. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Country/Territory</th>\n",
       "      <th>UN region</th>\n",
       "      <th colspan=\"2\" halign=\"left\">IMF[1][13]</th>\n",
       "      <th colspan=\"2\" halign=\"left\">World Bank[14]</th>\n",
       "      <th colspan=\"2\" halign=\"left\">United Nations[15]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Country/Territory</th>\n",
       "      <th>UN region</th>\n",
       "      <th>Forecast</th>\n",
       "      <th>Year</th>\n",
       "      <th>Estimate</th>\n",
       "      <th>Year</th>\n",
       "      <th>Estimate</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World</td>\n",
       "      <td>—</td>\n",
       "      <td>105568776</td>\n",
       "      <td>2023</td>\n",
       "      <td>100562011</td>\n",
       "      <td>2022</td>\n",
       "      <td>96698005</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>Americas</td>\n",
       "      <td>26854599</td>\n",
       "      <td>2023</td>\n",
       "      <td>25462700</td>\n",
       "      <td>2022</td>\n",
       "      <td>23315081</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China</td>\n",
       "      <td>Asia</td>\n",
       "      <td>19373586</td>\n",
       "      <td>[n 1]2023</td>\n",
       "      <td>17963171</td>\n",
       "      <td>[n 3]2022</td>\n",
       "      <td>17734131</td>\n",
       "      <td>[n 1]2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Asia</td>\n",
       "      <td>4409738</td>\n",
       "      <td>2023</td>\n",
       "      <td>4231141</td>\n",
       "      <td>2022</td>\n",
       "      <td>4940878</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>Europe</td>\n",
       "      <td>4308854</td>\n",
       "      <td>2023</td>\n",
       "      <td>4072192</td>\n",
       "      <td>2022</td>\n",
       "      <td>4259935</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>Americas</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>303</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Kiribati</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>248</td>\n",
       "      <td>2023</td>\n",
       "      <td>223</td>\n",
       "      <td>2022</td>\n",
       "      <td>227</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Nauru</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>151</td>\n",
       "      <td>2023</td>\n",
       "      <td>151</td>\n",
       "      <td>2022</td>\n",
       "      <td>155</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Montserrat</td>\n",
       "      <td>Americas</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>—</td>\n",
       "      <td>72</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Tuvalu</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>65</td>\n",
       "      <td>2023</td>\n",
       "      <td>60</td>\n",
       "      <td>2022</td>\n",
       "      <td>60</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Country/Territory UN region IMF[1][13]            World Bank[14]   \n",
       "    Country/Territory UN region   Forecast       Year       Estimate   \n",
       "0               World         —  105568776       2023      100562011  \\\n",
       "1       United States  Americas   26854599       2023       25462700   \n",
       "2               China      Asia   19373586  [n 1]2023       17963171   \n",
       "3               Japan      Asia    4409738       2023        4231141   \n",
       "4             Germany    Europe    4308854       2023        4072192   \n",
       "..                ...       ...        ...        ...            ...   \n",
       "209          Anguilla  Americas          —          —              —   \n",
       "210          Kiribati   Oceania        248       2023            223   \n",
       "211             Nauru   Oceania        151       2023            151   \n",
       "212        Montserrat  Americas          —          —              —   \n",
       "213            Tuvalu   Oceania         65       2023             60   \n",
       "\n",
       "               United Nations[15]             \n",
       "          Year           Estimate       Year  \n",
       "0         2022           96698005       2021  \n",
       "1         2022           23315081       2021  \n",
       "2    [n 3]2022           17734131  [n 1]2021  \n",
       "3         2022            4940878       2021  \n",
       "4         2022            4259935       2021  \n",
       "..         ...                ...        ...  \n",
       "209          —                303       2021  \n",
       "210       2022                227       2021  \n",
       "211       2022                155       2021  \n",
       "212          —                 72       2021  \n",
       "213       2022                 60       2021  \n",
       "\n",
       "[214 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
    "# url = \"https://en.wikipedia.org/wiki/Minnesota\"\n",
    "\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "tables[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium Compexity Scraping\n",
    "\n",
    "Medium complexity scraping is extracting data from websites that use hyperlinks to organize their data. These pages are often static webpages generated from Perl or PHP scripts. We will have to use more steps to accomplish our goal to build a dataset.\n",
    "\n",
    "Two really import modules we can use are BeautifulSoup and Selenium. BeautifulSoup is a Python library for pulling data out of HTML and XML files. Selenium is a Python library that allows us to automate web browser interactions. We can use these two modules to scrape data from websites that use hyperlinks to organize their data.\n",
    "\n",
    "* BeautifulSoup documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "* Selenium documentation: https://selenium-python.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a website HTML\n",
    "import requests\n",
    "\n",
    "url = \"https://www.gutenberg.org/ebooks/search/?sort_order=downloads\"\n",
    "\n",
    "gutenberg_most_read = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!DOCTYPE html>\\n<!--\\n\\nDON\\'T USE THIS PAGE FOR SCRAPING.\\n\\nSeriously. You\\'ll only get your IP blocked.\\n\\nDownload https://www.gutenberg.org/feeds/catalog.rdf.bz2 instead,\\nwhich contains *all* Project Gutenberg metadata in one RDF/XML file.\\n\\n--><html lang=\"en\">\\n\\n\\n\\n\\n<head>\\n<style>\\n.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg_most_read\n",
    "\n",
    "\"\"\"\n",
    "<!DOCTYPE html>\\n<!--\\n\\nDON\\'T USE THIS PAGE FOR SCRAPING.\\n\\nSeriously. You\\'ll only get your IP blocked.\\n\\nDownload https://www.gutenberg.org/feeds/catalog.rdf.bz2 instead,\\nwhich contains *all* Project Gutenberg metadata in one RDF/XML file.\\n\\n--><html lang=\"en\">\\n\\n\\n\\n\\n<head>\\n<style>\\n.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's download some books and store the file in our data directory\n",
    "# ! wget https://www.gutenberg.org/cache/epub/feeds/rdf-files.tar.bz2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at the data\n",
    "\n",
    "N.B.: The archive is a little over 1 GB. For the purpose of our course, I have extracted the first e-text from the archive. The file is called `pg1.rdf`. The file is in the `datasets` folder of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this notebook in Google colab, uncomment this line of code and run\n",
    "# import re\n",
    "# from bs4 import BeautifulSoup as bs4\n",
    "#\n",
    "# from pathlib import Path\n",
    "# from google.colab import drive\n",
    "# \n",
    "# drive.mount('/content/gdrive/', force_remount=True)\n",
    "# file = Path('gdrive/MyDrive/DATA_340_3_NLP/Datasets/pg1.rdf')\n",
    "# with open(file, 'r') as f:\n",
    "#    gutenberg = f.read().decode('utf-8')\n",
    "# gutenberg = bs4(gutenberg, features='lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<html>\n",
      " <body>\n",
      "  <rdf:rdf xml:base=\"http://www.gutenberg.org/\" xmlns:cc=\"http://web.resource.org/cc/\" xmlns:dcam=\"http://purl.org/dc/dcam/\" xmlns:dcterms=\"http://purl.org/dc/terms/\" xmlns:pgterms=\"http://www.gutenberg.org/2009/pgterms/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\" xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\">\n",
      "   <pgterms:ebook rdf:about=\"ebooks/1\">\n",
      "    <dcterms:publisher>\n",
      "     Project Gutenberg\n",
      "    </dcterms:publisher>\n",
      "    <dcterms:license rdf:resource=\"license\">\n",
      "    </dcterms:license>\n",
      "    <dcterms:issued rdf:datatype=\"http://www.w3.org/2001/XMLSchema#date\">\n",
      "     1971-12-01\n",
      "    </dcterms:issued>\n",
      "    <dcterms:rights>\n",
      "     Public domain in the USA.\n",
      "    </dcterms:rights>\n",
      "    <pgterms:downloads rdf:datatype=\"http://www.w3.org/2001/XMLSchema#integer\">\n",
      "     1202\n",
      "    </pgterms:downloads>\n",
      "    <dcterms:creator>\n",
      "     <pgterms:agent rdf:about=\"2009/agents/1638\">\n",
      "      <pgterms:name>\n",
      "       Jefferson, Thomas\n",
      "      </pgterms:n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/envs/pytorch/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "file_name = 'pg1.rdf'\n",
    "gutenberg_file = Path.cwd() / '..' / 'datasets' / file_name\n",
    "\n",
    "# Read file and create a beautiful soup object\n",
    "with open(gutenberg_file, \"rb\") as f:\n",
    "    gutenberg = f.read().decode(\"utf-8\")\n",
    "\n",
    "# we need to use an XML parser\n",
    "gutenberg = bs4(gutenberg, features=\"lxml\")\n",
    "\n",
    "print(gutenberg.prettify()[:1000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Declaration of Independence of the United States of America',\n",
       " 'This is the original PG edition.\\r\\nSee also our revised edition: #16780\\r\\nSee also #300',\n",
       " '\\nJefferson, Thomas\\n1743\\n1826\\nUnited States President (1801-1809)\\n\\n')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the title of the book, description, and author\n",
    "title = gutenberg.find(\"dcterms:title\").text\n",
    "description = gutenberg.find(\"dcterms:description\").text\n",
    "author = gutenberg.find(\"pgterms:agent\").text\n",
    "\n",
    "title, description, author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the link to the text\n",
    "links = []\n",
    "\n",
    "file_types = gutenberg.find_all(\"pgterms:file\")\n",
    "for f in file_types:\n",
    "    if f[\"rdf:about\"].endswith(\".txt\"):\n",
    "        links.append(f[\"rdf:about\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.gutenberg.org/files/1/1-0.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at our links\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create another beautiful soup object for the text\n",
    "text = bs4(requests.get(links[0]).text, features=\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "===========================================================\r\n",
       "\r\n",
       "     NOTE:  This file combines the first two Project Gutenberg\r\n",
       "     files, both of which were given the filenumber #1. There are\r\n",
       "     several duplicate files here. There were many updates over\r\n",
       "     the years.  All of the original files are included in the\r\n",
       "     \"old\" subdirectory which may be accessed under the \"More\r\n",
       "     Files\" listing in the PG Catalog of this file. No changes\r\n",
       "     have been made in these original etexts.\r\n",
       "\r\n",
       "===========================================================\r\n",
       "\r\n",
       "\r\n",
       "**Welcome To The World of Free Plain Vanilla Electronic Texts**\r\n",
       "\r\n",
       "**Etexts Readable By Both Humans and By Computers, Since 1971**\r\n",
       "\r\n",
       "*These Etexts Prepared By Hundreds of Volunteers and Donations*\r\n",
       "\r\n",
       "Below you will find the first nine Project Gutenberg Etexts, in\r\n",
       "one file, with one header for the entire file.  This is to keep\r\n",
       "the overhead down, and in response to requests from Gopher site\r\n",
       "keeper to eliminate as much of the heade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display as HTML\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(text.body.text[:1000]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data quality check and cleaning\n",
    "\n",
    "It turns out the above is corpus of documents. Each document is headed by a date along with an e-text and number. Thankfully we know regular expressions, so we can use them to clean up the data and extract the e-texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_check = re.findall(r\"\\[Etext #\\d+\\]\", text.body.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Etext #1]',\n",
       " '[Etext #2]',\n",
       " '[Etext #3]',\n",
       " '[Etext #4]',\n",
       " '[Etext #5]',\n",
       " '[Etext #6]',\n",
       " '[Etext #7]',\n",
       " '[Etext #8]',\n",
       " '[Etext #9]']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher Complexity Scraping with Selenium\n",
    "\n",
    "Selenium enables us to automate web browser interactions. We can use Selenium to scrape data from websites that use JavaScript to generate their webpages. If we go the following website, we can see that the data is generated by JavaScript: https://osf.io/preprints/discover?subject=bepress%7CSocial%20and%20Behavioral%20Sciences%7CLinguistics. If we use the requests module to get the HTML of the page, we will not get the data we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example from preprints archive\n",
    "## use requests to get the html\n",
    "import requests\n",
    "\n",
    "linguistics_preprints = \"https://osf.io/preprints/discover?subject=bepress%7CSocial%20and%20Behavioral%20Sciences%7CLinguistics\"\n",
    "\n",
    "html = requests.get(linguistics_preprints)\n",
    "\n",
    "if html.status_code == 200:\n",
    "    linguistics_html_text = html.text\n",
    "    \n",
    "with open('preprints_wo_content.html', 'w') as html_out:\n",
    "    html_out.write(linguistics_html_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we get the data from the loaded webpage?\n",
    "\n",
    "In order to get the data from the loaded webpage, we need to use Selenium. Selenium is a Python library that allows us to automate web browser interactions. We can use Selenium to scrape data from websites that use JavaScript to generate their webpages. When we use Selenium, we need to use a web driver. A web driver is a program that allows Selenium to interact with a web browser. We can use the following web drivers:\n",
    "\n",
    "* ChromeDriver\n",
    "* EdgeDriver\n",
    "* SafariDriver\n",
    "* FirefoxDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-21 15:25:18--  https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/117.0.5938.92/linux64/chrome-linux64.zip\n",
      "Resolving edgedl.me.gvt1.com (edgedl.me.gvt1.com)... 2600:1900:4110:86f::, 34.104.35.123\n",
      "Connecting to edgedl.me.gvt1.com (edgedl.me.gvt1.com)|2600:1900:4110:86f::|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 146688753 (140M) [application/octet-stream]\n",
      "Saving to: ‘chrome-linux64.zip.1’\n",
      "\n",
      "chrome-linux64.zip. 100%[===================>] 139.89M  67.0MB/s    in 2.1s    \n",
      "\n",
      "2023-09-21 15:25:21 (67.0 MB/s) - ‘chrome-linux64.zip.1’ saved [146688753/146688753]\n",
      "\n",
      "unzip:  cannot find or open chrome_linux64.zip, chrome_linux64.zip.zip or chrome_linux64.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "## download the Chrome driver\n",
    "# https://googlechromelabs.github.io/chrome-for-testing/#stable\n",
    "\n",
    "! wget https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/117.0.5938.92/linux64/chrome-linux64.zip && unzip chrome_linux64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Selenium to get the html\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "\n",
    "# Set the path to the Chrome drive\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Get the HTML from the webpage\n",
    "driver.get(linguistics_preprints)\n",
    "time.sleep(2)\n",
    "\n",
    "with open('preprints_w_content.html', 'w') as html_out:\n",
    "    html_out.write(driver.page_source)\n",
    "\n",
    "html_source = driver.page_source\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the beautiful soup object to parse the html and extract all the h4 tags\n",
    "soup = bs4(html_source, \"lxml\")\n",
    "\n",
    "# Use soup to find all the h4 tags\n",
    "articles = soup.find_all(\"h4\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip the text from the h4 tags\n",
    "entries = [a.text.strip() for a in articles]\n",
    "\n",
    "# Strip the hyperlinks from the h4 tags\n",
    "links = [a.find(\"a\") for a in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# our query results for `little is known` in PubMed\n",
    "df = pd.read_csv('data/pubmed_little.csv')\n",
    "df.plot(kind='bar', x='Year', y='Count', figsize=(15, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding our API\n",
    "\n",
    "The National Center for Biotechnology Information [NCBI] documents the PubMed API at: https://www.ncbi.nlm.nih.gov/home/develop/api/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use the API to get varios kinds of data related to each Publication (PMID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "\n",
    "# let's store the base url for the PubMed API\n",
    "pubmed_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id={}\"\n",
    "\n",
    "\n",
    "# Let's get the list of PubMed IDs\n",
    "PMID = 18652081\n",
    "\n",
    "# Format the url\n",
    "our_pmid = pubmed_url.format(PMID)\n",
    "\n",
    "# Let's take a look at the PMID\n",
    "our_pmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the title and abstract\n",
    "response = requests.get(our_pmid)\n",
    "pmid_data = html.fromstring(response.content)\n",
    "\n",
    "pmid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to do some extra work to get the data we want\n",
    "title = \" \".join([title.text for title in pmid_data.xpath(\"//articletitle\")])\n",
    "abstract = \"||||\".join([line.text for line in pmid_data.xpath(\"//abstract/abstracttext\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title, abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Grimmer et al.](./course_readings/Grimmer-Grimmer_48-62.pdf):\n",
    "\n",
    "* Principle 1: Data should be collected in a way that is consistent with the research question.\n",
    "    * Usefulness depends on the question to be answered\n",
    "    * Revision: usefulness depends on the creativity of the researcher\n",
    "* Principle 2: Data should be collected in a way that is consistent with the research design.\n",
    "    * No values free corpus construction\n",
    "    * This is in my opinion the most important principle\n",
    "* Principle 3: Data should be collected in a way that is consistent with the research context.\n",
    "    * There is no `right` way to represent a text but there are considerations\n",
    "    * Follow the insights and conventions of the statstical community\n",
    "* Principle 4: Data should be collected in a way that is consistent with the research ethics.\n",
    "    * Privacy, consent, and anonymity\n",
    "    * Data equally representative of the population - sampling\n",
    "    * Conclusions should be validated by measurements of science and not by the researcher's intuition\n",
    "* Principle 5: Code and results should be reproducable (the curious case of _Arming America_ by Michael Bellesiles; won Bancroft Prize. His thesis that the second ammendment was not about personal ownership of guns, as evidenced by probate records. He was later found to have fabricated his data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff598827a98688517ffbe680bec5493f272109220e95a133fdc272280f9c7acf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
