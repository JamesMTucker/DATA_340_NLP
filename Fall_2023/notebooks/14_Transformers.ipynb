{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "## Introduction to Transformers Overview\n",
    "\n",
    "* Our character RNN trained\n",
    "* Introduction to Transformers\n",
    "* HuggingFace Transformers library\n",
    "* Transformers for NLP\n",
    "* Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Transformers\n",
    "\n",
    "### Milestones in Transformer Models\n",
    "\n",
    "* Vaswani, Ashish, et al. Attention Is All You Need. arXiv:1706.03762, arXiv, 5 Dec. 2017. arXiv.org, https://doi.org/10.48550/arXiv.1706.03762.\n",
    "\n",
    "### Some import models\n",
    "\n",
    "* June 2018: GPT (OpenAI)\n",
    "* October 2018: BERT (Google - summaries of sentences)\n",
    "* February 2019: GPT-2 (OpenAI - not immediately released due to ethical concerns)\n",
    "* October 2019: DistilBERT (Faster and better memory performance than BERT)\n",
    "* October 2019: BART and T5 (large pretrained models)\n",
    "* May 2020: GPT-3 (OpenAI - zero-shot learning)\n",
    "\n",
    "\n",
    "### Key ideas\n",
    "\n",
    "* Pretraining - Input is a very large corpus of text for weeks or months\n",
    "* Fine-tuning - Input is a specific task (e.g. sentiment analysis)\n",
    "* Encoder - Models that are good for understanding the input, like sentence classification or named entity recognition\n",
    "* Decoder - Models that are good for generating output, like text generation or summarization\n",
    "* Attention layers - Model attends to different relationships in different layers [BERT](https://huggingface.co/exbert/?model=bert-base-uncased&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=0&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true)\n",
    "\n",
    "[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Transformers library\n",
    "\n",
    "* [HuggingFace Transformers](https://huggingface.co/transformers/)\n",
    "* [Natural Language Processing Course](https://huggingface.co/course/chapter1/1)\n",
    "\n",
    "<center><img src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" height=\"200\" width=\"200\"></center>\n",
    "\n",
    "### Docs and Tutorials\n",
    "\n",
    "* [Docs](https://huggingface.co/transformers/)\n",
    "* [Tutorials](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "### Installation\n",
    "\n",
    "* `pip install transformers`\n",
    "* `pip install datasets`\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* [Datasets](https://huggingface.co/datasets/)\n",
    "  * Multimodal\n",
    "  * Computer Vision\n",
    "  * NLP\n",
    "  * Audio\n",
    "  * Tabular\n",
    "\n",
    "* NLP Datasets for various tasks\n",
    "  * Text Classification\n",
    "  * Token Classification\n",
    "  * Table Question Answering\n",
    "  * Question Answering\n",
    "  * Zero-Shot Classification\n",
    "  * Translation\n",
    "  * Summarization\n",
    "  * Conversational\n",
    "  * Text Generation\n",
    "  * Text2Text Generation\n",
    "  * Fill Mask\n",
    "  * Sentence similarity\n",
    "  * Table to text\n",
    "  * Multi-choice\n",
    "  * Text retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 74733 datasets available on the HuggingFace Hub\n",
      "The first 10 are: [DatasetInfo: { \n",
      "  {'_id': '621ffdd236468d709f181d58',\n",
      "   'author': None,\n",
      "   'cardData': None,\n",
      "   'citation': '@inproceedings{veyseh-et-al-2020-what,\\n'\n",
      "               '   title={{What Does This Acronym Mean? Introducing a New Dataset for Acronym Identification and '\n",
      "               'Disambiguation}},\\n'\n",
      "               '   author={Amir Pouran Ben Veyseh and Franck Dernoncourt and Quan Hung Tran and Thien Huu Nguyen},\\n'\n",
      "               '   year={2020},\\n'\n",
      "               '   booktitle={Proceedings of COLING},\\n'\n",
      "               '   link={https://arxiv.org/pdf/2010.14678v1.pdf}\\n'\n",
      "               '}',\n",
      "   'description': 'Acronym identification training and development sets for the acronym identification task at '\n",
      "                  'SDU@AAAI-21.',\n",
      "   'disabled': False,\n",
      "   'downloads': 2938,\n",
      "   'gated': False,\n",
      "   'id': 'acronym_identification',\n",
      "   'lastModified': '2023-01-25T14:18:28.000Z',\n",
      "   'likes': 17,\n",
      "   'paperswithcode_id': 'acronym-identification',\n",
      "   'private': False,\n",
      "   'sha': 'c3c245a18bbd57b1682b099e14460eebf154cbdf',\n",
      "   'siblings': [],\n",
      "   'tags': ['task_categories:token-classification', 'annotations_creators:expert-generated', 'language_creators:found',\n",
      "            'multilinguality:monolingual', 'size_categories:10K<n<100K', 'source_datasets:original', 'language:en',\n",
      "            'license:mit', 'acronym-identification', 'arxiv:2010.14678', 'region:us']}\n",
      "}, DatasetInfo: { \n",
      "  {'_id': '621ffdd236468d709f181d59',\n",
      "   'author': None,\n",
      "   'cardData': None,\n",
      "   'citation': '@article{GURULINGAPPA2012885,\\n'\n",
      "               'title = \"Development of a benchmark corpus to support the automatic extraction of drug-related adverse '\n",
      "               'effects from medical case reports\",\\n'\n",
      "               'journal = \"Journal of Biomedical Informatics\",\\n'\n",
      "               'volume = \"45\",\\n'\n",
      "               'number = \"5\",\\n'\n",
      "               'pages = \"885 - 892\",\\n'\n",
      "               'year = \"2012\",\\n'\n",
      "               'note = \"Text Mining and Natural Language Processing in Pharmacogenomics\",\\n'\n",
      "               'issn = \"1532-0464\",\\n'\n",
      "               'doi = \"https://doi.org/10.1016/j.jbi.2012.04.008\",\\n'\n",
      "               'url = \"http://www.sciencedirect.com/science/article/pii/S1532046412000615\",\\n'\n",
      "               'author = \"Harsha Gurulingappa and Abdul Mateen Rajput and Angus Roberts and Juliane Fluck and Martin '\n",
      "               'Hofmann-Apitius and Luca Toldo\",\\n'\n",
      "               'keywords = \"Adverse drug effect, Benchmark corpus, Annotation, Harmonization, Sentence '\n",
      "               'classification\",\\n'\n",
      "               'abstract = \"A significant amount of information about drug-related safety issues such as adverse '\n",
      "               'effects are published in medical case reports that can only be explored by human readers due to their '\n",
      "               'unstructured nature. The work presented here aims at generating a systematically annotated corpus that '\n",
      "               'can support the development and validation of methods for the automatic extraction of drug-related '\n",
      "               'adverse effects from medical case reports. The documents are systematically double annotated in various '\n",
      "               'rounds to ensure consistent annotations. The annotated documents are finally harmonized to generate '\n",
      "               'representative consensus annotations. In order to demonstrate an example use case scenario, the corpus '\n",
      "               'was employed to train and validate models for the classification of informative against the '\n",
      "               'non-informative sentences. A Maximum Entropy classifier trained with simple features and evaluated by '\n",
      "               '10-fold cross-validation resulted in the F1 score of 0.70 indicating a potential useful application of '\n",
      "               'the corpus.\"\\n'\n",
      "               '}',\n",
      "   'description': ' ADE-Corpus-V2  Dataset: Adverse Drug Reaction Data.\\n'\n",
      "                  ' This is a dataset for Classification if a sentence is ADE-related (True) or not (False) and '\n",
      "                  'Relation Extraction between Adverse Drug Event and Drug.\\n'\n",
      "                  ' DRUG-AE.rel provides relations between drugs and adverse effects.\\n'\n",
      "                  ' DRUG-DOSE.rel provides relations between drugs and dosages.\\n'\n",
      "                  ' ADE-NEG.txt provides all sentences in the ADE corpus that DO NOT contain any drug-related adverse '\n",
      "                  'effects.',\n",
      "   'disabled': False,\n",
      "   'downloads': 1704,\n",
      "   'gated': False,\n",
      "   'id': 'ade_corpus_v2',\n",
      "   'lastModified': '2023-06-01T14:59:53.000Z',\n",
      "   'likes': 19,\n",
      "   'private': False,\n",
      "   'sha': '080cf99e1483008322d612c7262c04c8902fdbee',\n",
      "   'siblings': [],\n",
      "   'tags': ['task_categories:text-classification', 'task_categories:token-classification',\n",
      "            'task_ids:coreference-resolution', 'task_ids:fact-checking', 'annotations_creators:expert-generated',\n",
      "            'language_creators:found', 'multilinguality:monolingual', 'size_categories:10K<n<100K',\n",
      "            'size_categories:1K<n<10K', 'size_categories:n<1K', 'source_datasets:original', 'language:en',\n",
      "            'license:unknown', 'region:us']}\n",
      "}, DatasetInfo: { \n",
      "  {'_id': '621ffdd236468d709f181d5a',\n",
      "   'author': None,\n",
      "   'cardData': None,\n",
      "   'citation': '@article{bartolo2020beat,\\n'\n",
      "               '    author = {Bartolo, Max and Roberts, Alastair and Welbl, Johannes and Riedel, Sebastian and '\n",
      "               'Stenetorp, Pontus},\\n'\n",
      "               '    title = {Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension},\\n'\n",
      "               '    journal = {Transactions of the Association for Computational Linguistics},\\n'\n",
      "               '    volume = {8},\\n'\n",
      "               '    number = {},\\n'\n",
      "               '    pages = {662-678},\\n'\n",
      "               '    year = {2020},\\n'\n",
      "               '    doi = {10.1162/tacl_a_00338},\\n'\n",
      "               '    URL = { https://doi.org/10.1162/tacl_a_00338 },\\n'\n",
      "               '    eprint = { https://doi.org/10.1162/tacl_a_00338 },\\n'\n",
      "               '    abstract = { Innovations in annotation methodology have been a catalyst for Reading Comprehension '\n",
      "               '(RC) datasets and models. One recent trend to challenge current RC models is to involve a model in the '\n",
      "               'annotation process: Humans create questions adversarially, such that the model fails to answer them '\n",
      "               'correctly. In this work we investigate this annotation methodology and apply it in three different '\n",
      "               'settings, collecting a total of 36,000 samples with progressively stronger models in the annotation '\n",
      "               'loop. This allows us to explore questions such as the reproducibility of the adversarial effect, '\n",
      "               'transfer from data collected with varying model-in-the-loop strengths, and generalization to data '\n",
      "               'collected without a model. We find that training on adversarially collected samples leads to strong '\n",
      "               'generalization to non-adversarially collected datasets, yet with progressive performance deterioration '\n",
      "               'with increasingly stronger models-in-the-loop. Furthermore, we find that stronger models can still '\n",
      "               'learn from datasets collected with substantially weaker models-in-the-loop. When trained on data '\n",
      "               'collected with a BiDAF model in the loop, RoBERTa achieves 39.9F1 on questions that it cannot answer '\n",
      "               'when trained on SQuAD—only marginally lower than when trained on data collected using RoBERTa itself '\n",
      "               '(41.0F1). }\\n'\n",
      "               '}',\n",
      "   'description': 'AdversarialQA is a Reading Comprehension dataset, consisting of questions posed by crowdworkers on a '\n",
      "                  'set of Wikipedia articles using an adversarial model-in-the-loop.\\n'\n",
      "                  'We use three different models; BiDAF (Seo et al., 2016), BERT-Large (Devlin et al., 2018), and '\n",
      "                  'RoBERTa-Large (Liu et al., 2019) in the annotation loop and construct three datasets; D(BiDAF), '\n",
      "                  'D(BERT), and D(RoBERTa), each with 10,000 training examples, 1,000 validation, and 1,000 test '\n",
      "                  'examples.\\n'\n",
      "                  'The adversarial human annotation paradigm ensures that these datasets consist of questions that '\n",
      "                  'current state-of-the-art models (at least the ones used as adversaries in the annotation loop) find '\n",
      "                  'challenging.',\n",
      "   'disabled': False,\n",
      "   'downloads': 9615,\n",
      "   'gated': False,\n",
      "   'id': 'adversarial_qa',\n",
      "   'lastModified': '2022-11-18T17:31:37.000Z',\n",
      "   'likes': 27,\n",
      "   'paperswithcode_id': 'adversarialqa',\n",
      "   'private': False,\n",
      "   'sha': '2101a4597c2b5517902e2eef903c47b2caedacd8',\n",
      "   'siblings': [],\n",
      "   'tags': ['task_categories:question-answering', 'task_ids:extractive-qa', 'task_ids:open-domain-qa',\n",
      "            'annotations_creators:crowdsourced', 'language_creators:found', 'multilinguality:monolingual',\n",
      "            'size_categories:10K<n<100K', 'source_datasets:original', 'language:en', 'license:cc-by-sa-4.0',\n",
      "            'arxiv:2002.00293', 'arxiv:1606.05250', 'region:us']}\n",
      "}, DatasetInfo: { \n",
      "  {'_id': '621ffdd236468d709f181d5b',\n",
      "   'author': None,\n",
      "   'cardData': None,\n",
      "   'citation': '@misc{zhang2019email,\\n'\n",
      "               '    title={This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation},\\n'\n",
      "               '    author={Rui Zhang and Joel Tetreault},\\n'\n",
      "               '    year={2019},\\n'\n",
      "               '    eprint={1906.03497},\\n'\n",
      "               '    archivePrefix={arXiv},\\n'\n",
      "               '    primaryClass={cs.CL}\\n'\n",
      "               '}',\n",
      "   'description': 'A collection of email messages of employees in the Enron Corporation.\\n'\n",
      "                  '\\n'\n",
      "                  'There are two features:\\n'\n",
      "                  '  - email_body: email body text.\\n'\n",
      "                  '  - subject_line: email subject text.',\n",
      "   'disabled': False,\n",
      "   'downloads': 1620,\n",
      "   'gated': False,\n",
      "   'id': 'aeslc',\n",
      "   'lastModified': '2023-04-05T08:32:58.000Z',\n",
      "   'likes': 5,\n",
      "   'paperswithcode_id': 'aeslc',\n",
      "   'private': False,\n",
      "   'sha': '28d4d3a046750bf18995a77dfb9f5f082fe837f6',\n",
      "   'siblings': [],\n",
      "   'tags': ['task_categories:summarization', 'annotations_creators:crowdsourced', 'language_creators:found',\n",
      "            'multilinguality:monolingual', 'size_categories:10K<n<100K', 'source_datasets:original', 'language:en',\n",
      "            'license:unknown', 'aspect-based-summarization', 'conversations-summarization',\n",
      "            'multi-document-summarization', 'email-headline-generation', 'arxiv:1906.03497', 'region:us']}\n",
      "}, DatasetInfo: { \n",
      "  {'_id': '621ffdd236468d709f181d5c',\n",
      "   'author': None,\n",
      "   'cardData': None,\n",
      "   'citation': '@inproceedings{afrikaans_ner_corpus,\\n'\n",
      "               '  author    = {\\tGerhard van Huyssteen and\\n'\n",
      "               '                Martin Puttkammer and\\n'\n",
      "               '                E.B. Trollip and\\n'\n",
      "               '                J.C. Liversage and\\n'\n",
      "               '              Roald Eiselen},\\n'\n",
      "               '  title     = {NCHLT Afrikaans Named Entity Annotated Corpus},\\n'\n",
      "               '  booktitle = {Eiselen, R. 2016. Government domain named entity recognition for South African '\n",
      "               'languages. Proceedings of the 10th      Language Resource and Evaluation Conference, Portorož, '\n",
      "               'Slovenia.},\\n'\n",
      "               '  year      = {2016},\\n'\n",
      "               '  url       = {https://repo.sadilar.org/handle/20.500.12185/299},\\n'\n",
      "               '}',\n",
      "   'description': 'Named entity annotated data from the NCHLT Text Resource Development: Phase II Project, annotated '\n",
      "                  'with PERSON, LOCATION, ORGANISATION and MISCELLANEOUS tags.',\n",
      "   'disabled': False,\n",
      "   'downloads': 215,\n",
      "   'gated': False,\n",
      "   'id': 'afrikaans_ner_corpus',\n",
      "   'lastModified': '2023-01-25T14:20:30.000Z',\n",
      "   'likes': 3,\n",
      "   'private': False,\n",
      "   'sha': '58eb3b7ec689e17d42ffd858b8e514cbd0ffbc7f',\n",
      "   'siblings': [],\n",
      "   'tags': ['task_categories:token-classification', 'task_ids:named-entity-recognition',\n",
      "            'annotations_creators:expert-generated', 'language_creators:expert-generated', 'multilinguality:monolingual',\n",
      "            'size_categories:1K<n<10K', 'source_datasets:original', 'language:af', 'license:other', 'region:us']}\n",
      "}, DatasetInfo: { \n",
      "  {'_id': '621ffdd236468d709f181d5d',\n",
      "   'author': None,\n",
      "   'cardData': None,\n",
      "   'citation': '@inproceedings{Zhang2015CharacterlevelCN,\\n'\n",
      "               '  title={Character-level Convolutional Networks for Text Classification},\\n'\n",
      "               '  author={Xiang Zhang and Junbo Jake Zhao and Yann LeCun},\\n'\n",
      "               '  booktitle={NIPS},\\n'\n",
      "               '  year={2015}\\n'\n",
      "               '}',\n",
      "   'description': 'AG is a collection of more than 1 million news articles. News articles have been\\n'\n",
      "                  'gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of\\n'\n",
      "                  'activity. ComeToMyHead is an academic news search engine which has been running\\n'\n",
      "                  'since July, 2004. The dataset is provided by the academic comunity for research\\n'\n",
      "                  'purposes in data mining (clustering, classification, etc), information retrieval\\n'\n",
      "                  '(ranking, search, etc), xml, data compression, data streaming, and any other\\n'\n",
      "                  'non-commercial activity. For more information, please refer to the link\\n'\n",
      "                  'http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\\n'\n",
      "                  '\\n'\n",
      "                  \"The AG's news topic classification dataset is constructed by Xiang Zhang\\n\"\n",
      "                  '(xiang.zhang@nyu.edu) from the dataset above. It is used as a text\\n'\n",
      "                  'classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann\\n'\n",
      "                  'LeCun. Character-level Convolutional Networks for Text Classification. Advances\\n'\n",
      "                  'in Neural Information Processing Systems 28 (NIPS 2015).',\n",
      "   'disabled': False,\n",
      "   'downloads': 27309,\n",
      "   'gated': False,\n",
      "   'id': 'ag_news',\n",
      "   'lastModified': '2023-04-05T08:34:57.000Z',\n",
      "   'likes': 74,\n",
      "   'paperswithcode_id': 'ag-news',\n",
      "   'private': False,\n",
      "   'sha': '3dcafdc56638659831698aaa2348334a5ab44513',\n",
      "   'siblings': [],\n",
      "   'tags': ['task_categories:text-classification', 'task_ids:topic-classification', 'annotations_creators:found',\n",
      "            'language_creators:found', 'multilinguality:monolingual', 'size_categories:100K<n<1M',\n",
      "            'source_datasets:original', 'language:en', 'license:unknown', 'region:us']}\n",
      "}, DatasetInfo: { \n",
      "  {'_id': '621ffdd236468d709f181d5e',\n",
      "   'author': None,\n",
      "   'cardData': None,\n",
      "   'citation': '@article{allenai:arc,\\n'\n",
      "               '      author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and\\n'\n",
      "               '                    Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},\\n'\n",
      "               '      title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},\\n'\n",
      "               '      journal   = {arXiv:1803.05457v1},\\n'\n",
      "               '      year      = {2018},\\n'\n",
      "               '}',\n",
      "   'description': 'A new dataset of 7,787 genuine grade-school level, multiple-choice science questions, assembled to '\n",
      "                  'encourage research in\\n'\n",
      "                  ' advanced question-answering. The dataset is partitioned into a Challenge Set and an Easy Set, where '\n",
      "                  'the former contains\\n'\n",
      "                  ' only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence '\n",
      "                  'algorithm. We are also\\n'\n",
      "                  ' including a corpus of over 14 million science sentences relevant to the task, and an implementation '\n",
      "                  'of three neural baseline models for this dataset. We pose ARC as a challenge to the community.',\n",
      "   'disabled': False,\n",
      "   'downloads': 377705,\n",
      "   'gated': False,\n",
      "   'id': 'ai2_arc',\n",
      "   'lastModified': '2023-04-05T09:11:00.000Z',\n",
      "   'likes': 30,\n",
      "   'private': False,\n",
      "   'sha': '870fda1dc455fb8aec468816fb61c29c602c21ba',\n",
      "   'siblings': [],\n",
      "   'tags': ['task_categories:question-answering', 'task_ids:open-domain-qa', 'task_ids:multiple-choice-qa',\n",
      "            'annotations_creators:found', 'language_creators:found', 'multilinguality:monolingual',\n",
      "            'size_categories:1K<n<10K', 'source_datasets:original', 'language:en', 'license:cc-by-sa-4.0', 'region:us']}\n",
      "}, DatasetInfo: { \n",
      "  {'_id': '621ffdd236468d709f181d5f',\n",
      "   'author': None,\n",
      "   'cardData': None,\n",
      "   'citation': '@inproceedings{wei-etal-2018-airdialogue,\\n'\n",
      "               '    title = \"{A}ir{D}ialogue: An Environment for Goal-Oriented Dialogue Research\",\\n'\n",
      "               '    author = \"Wei, Wei  and\\n'\n",
      "               '      Le, Quoc  and\\n'\n",
      "               '      Dai, Andrew  and\\n'\n",
      "               '      Li, Jia\",\\n'\n",
      "               '    booktitle = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language '\n",
      "               'Processing\",\\n'\n",
      "               '    month = oct # \"-\" # nov,\\n'\n",
      "               '    year = \"2018\",\\n'\n",
      "               '    address = \"Brussels, Belgium\",\\n'\n",
      "               '    publisher = \"Association for Computational Linguistics\",\\n'\n",
      "               '    url = \"https://www.aclweb.org/anthology/D18-1419\",\\n'\n",
      "               '    doi = \"10.18653/v1/D18-1419\",\\n'\n",
      "               '    pages = \"3844--3854\",\\n'\n",
      "               '    abstract = \"Recent progress in dialogue generation has inspired a number of studies on dialogue '\n",
      "               'systems that are capable of accomplishing tasks through natural language interactions. A promising '\n",
      "               'direction among these studies is the use of reinforcement learning techniques, such as self-play, for '\n",
      "               'training dialogue agents. However, current datasets are limited in size, and the environment for '\n",
      "               'training agents and evaluating progress is relatively unsophisticated. We present AirDialogue, a large '\n",
      "               'dataset that contains 301,427 goal-oriented conversations. To collect this dataset, we create a '\n",
      "               'context-generator which provides travel and flight restrictions. We then ask human annotators to play '\n",
      "               'the role of a customer or an agent and interact with the goal of successfully booking a trip given the '\n",
      "               'restrictions. Key to our environment is the ease of evaluating the success of the dialogue, which is '\n",
      "               'achieved by using ground-truth states (e.g., the flight being booked) generated by the restrictions. '\n",
      "               'Any dialogue agent that does not generate the correct states is considered to fail. Our experimental '\n",
      "               'results indicate that state-of-the-art dialogue models can only achieve a score of 0.17 while humans '\n",
      "               'can reach a score of 0.91, which suggests significant opportunities for future improvement.\",\\n'\n",
      "               '}',\n",
      "   'description': 'AirDialogue, is a large dataset that contains 402,038 goal-oriented conversations. To collect this '\n",
      "                  'dataset, we create a contextgenerator which provides travel and flight restrictions. Then the human '\n",
      "                  'annotators are asked to play the role of a customer or an agent and interact with the goal of '\n",
      "                  'successfully booking a trip given the restrictions.',\n",
      "   'disabled': False,\n",
      "   'downloads': 260,\n",
      "   'gated': False,\n",
      "   'id': 'air_dialogue',\n",
      "   'lastModified': '2022-11-03T16:31:11.000Z',\n",
      "   'likes': 6,\n",
      "   'private': False,\n",
      "   'sha': '3ef284c2b1ca63cebd46335641fa31b09763f4e5',\n",
      "   'siblings': [],\n",
      "   'tags': ['task_categories:conversational', 'task_categories:text-generation', 'task_categories:fill-mask',\n",
      "            'task_ids:dialogue-generation', 'task_ids:dialogue-modeling', 'task_ids:language-modeling',\n",
      "            'task_ids:masked-language-modeling', 'annotations_creators:crowdsourced',\n",
      "            'language_creators:machine-generated', 'multilinguality:monolingual', 'size_categories:100K<n<1M',\n",
      "            'source_datasets:original', 'language:en', 'license:cc-by-nc-4.0', 'region:us']}\n",
      "}, DatasetInfo: { \n",
      "  {'_id': '621ffdd236468d709f181d60',\n",
      "   'author': None,\n",
      "   'cardData': None,\n",
      "   'citation': '@inproceedings{alomari2017arabic,\\n'\n",
      "               '  title={Arabic tweets sentimental analysis using machine learning},\\n'\n",
      "               '  author={Alomari, Khaled Mohammad and ElSherif, Hatem M and Shaalan, Khaled},\\n'\n",
      "               '  booktitle={International Conference on Industrial, Engineering and Other Applications of Applied '\n",
      "               'Intelligent Systems},\\n'\n",
      "               '  pages={602--610},\\n'\n",
      "               '  year={2017},\\n'\n",
      "               '  organization={Springer}\\n'\n",
      "               '}',\n",
      "   'description': 'Arabic Jordanian General Tweets (AJGT) Corpus consisted of 1,800 tweets annotated as positive and '\n",
      "                  'negative. Modern Standard Arabic (MSA) or Jordanian dialect.',\n",
      "   'disabled': False,\n",
      "   'downloads': 223,\n",
      "   'gated': False,\n",
      "   'id': 'ajgt_twitter_ar',\n",
      "   'lastModified': '2023-01-25T14:26:05.000Z',\n",
      "   'likes': 2,\n",
      "   'private': False,\n",
      "   'sha': '8016dcbb32739004bd1fc36dd0bcc97381dcb7f1',\n",
      "   'siblings': [],\n",
      "   'tags': ['task_categories:text-classification', 'task_ids:sentiment-classification', 'annotations_creators:found',\n",
      "            'language_creators:found', 'multilinguality:monolingual', 'size_categories:1K<n<10K',\n",
      "            'source_datasets:original', 'language:ar', 'license:unknown', 'region:us']}\n",
      "}, DatasetInfo: { \n",
      "  {'_id': '621ffdd236468d709f181d61',\n",
      "   'author': None,\n",
      "   'cardData': None,\n",
      "   'citation': '@inproceedings{rybak-etal-2020-klej,\\n'\n",
      "               '    title = \"{KLEJ}: Comprehensive Benchmark for Polish Language Understanding\",\\n'\n",
      "               '    author = \"Rybak, Piotr and Mroczkowski, Robert and Tracz, Janusz and Gawlik, Ireneusz\",\\n'\n",
      "               '    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational '\n",
      "               'Linguistics\",\\n'\n",
      "               '    month = jul,\\n'\n",
      "               '    year = \"2020\",\\n'\n",
      "               '    address = \"Online\",\\n'\n",
      "               '    publisher = \"Association for Computational Linguistics\",\\n'\n",
      "               '    url = \"https://www.aclweb.org/anthology/2020.acl-main.111\",\\n'\n",
      "               '    pages = \"1191--1201\",\\n'\n",
      "               '}',\n",
      "   'description': 'Allegro Reviews is a sentiment analysis dataset, consisting of 11,588 product reviews written in '\n",
      "                  'Polish and extracted\\n'\n",
      "                  'from Allegro.pl - a popular e-commerce marketplace. Each review contains at least 50 words and has a '\n",
      "                  'rating on a scale\\n'\n",
      "                  'from one (negative review) to five (positive review).\\n'\n",
      "                  '\\n'\n",
      "                  'We recommend using the provided train/dev/test split. The ratings for the test set reviews are kept '\n",
      "                  'hidden.\\n'\n",
      "                  'You can evaluate your model using the online evaluation tool available on klejbenchmark.com.',\n",
      "   'disabled': False,\n",
      "   'downloads': 145,\n",
      "   'gated': False,\n",
      "   'id': 'allegro_reviews',\n",
      "   'lastModified': '2022-11-18T17:41:41.000Z',\n",
      "   'likes': 1,\n",
      "   'paperswithcode_id': 'allegro-reviews',\n",
      "   'private': False,\n",
      "   'sha': '640d5c0c0cf44fba49bfa7fd4d29e7feb51bde21',\n",
      "   'siblings': [],\n",
      "   'tags': ['task_categories:text-classification', 'task_ids:sentiment-scoring', 'task_ids:text-scoring',\n",
      "            'annotations_creators:found', 'language_creators:found', 'multilinguality:monolingual',\n",
      "            'size_categories:10K<n<100K', 'source_datasets:original', 'language:pl', 'license:cc-by-sa-4.0',\n",
      "            'region:us']}\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Datasets https://github.com/huggingface/datasets\n",
    "# !pip install datasets\n",
    "\n",
    "from huggingface_hub import list_datasets\n",
    "# from datasets import list_datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "all_ds = list(list_datasets())\n",
    "print(f'There are {len(all_ds)} datasets available on the HuggingFace Hub')\n",
    "print(f'The first 10 are: {all_ds[:10]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:03<00:00, 68.9MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('We are very happy to show you the 🤗 Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading model.safetensors: 100%|██████████| 1.63G/1.63G [00:23<00:00, 68.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'We are very happy to show you the 🤗 Transformers library.',\n",
       " 'labels': ['technology', 'business', 'sports', 'politics'],\n",
       " 'scores': [0.7958350777626038,\n",
       "  0.08859799802303314,\n",
       "  0.06836581230163574,\n",
       "  0.04720110818743706]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## zero-shot classification\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "classifier('We are very happy to show you the 🤗 Transformers library.', candidate_labels=['politics', 'business', 'sports', 'technology'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Frodo and Sam were walking through the Shire when a group of three men approached the house and started firing. The men fired at the house, but the women came and told Mr. Frye to go back upstairs to the house. There'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text generation\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "generator('Frodo and Sam were walking through the Shire when')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading model.safetensors: 100%|██████████| 1.33G/1.33G [00:19<00:00, 69.2MB/s]\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/media/james/Projects/GitHub/DATA_340_NLP/venv/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9994654,\n",
       "  'word': 'Mary',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.98400134,\n",
       "  'word': 'William and Mary',\n",
       "  'start': 32,\n",
       "  'end': 48},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.9641359,\n",
       "  'word': 'Natural Language Processing',\n",
       "  'start': 77,\n",
       "  'end': 104},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9948832,\n",
       "  'word': 'MIT',\n",
       "  'start': 108,\n",
       "  'end': 111}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# named entity recognition\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline('ner', grouped_entities=True)\n",
    "ner('Mary graduates this spring from William and Mary. She will continue to study Natural Language Processing at MIT.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading model.safetensors: 100%|██████████| 261M/261M [00:03<00:00, 69.8MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9272308349609375, 'start': 108, 'end': 111, 'answer': 'MIT'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question answering\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline('question-answering')\n",
    "\n",
    "question_answerer(\n",
    "    question='Where does Mary study?',\n",
    "    context='Mary graduates this spring from William and Mary. She will continue to study Natural Language Processing at MIT.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 892M/892M [00:13<00:00, 68.0MB/s] \n",
      "/media/james/Projects/GitHub/DATA_340_NLP/venv/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the Transformers library provides state-of-the-art general-purpose architectures for natural language understanding (NLU) and natural language generation (NLG) with over 32+ pretrained models in 100+ languages .'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarization\n",
    "\n",
    "summarizer = pipeline('summarization', max_length=48, min_length=30, do_sample=False, model='t5-base')\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    The Transformers library provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...)\n",
    "    for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and\n",
    "    deep interoperability between TensorFlow 2.0 and PyTorch.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The importance of training data\n",
    "\n",
    "While the above uses are super easy, the real power of Transformers comes from the fact that they can be fine-tuned on a wide variety of tasks with just a few lines of code. This is made possible by the fact that they are pretrained on a large dataset (usually a few hundred million words) and then fine-tuned on a specific task. This is why Transformers are so powerful and why they are so widely used in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein ligand at either 1:1 or 2:2:N with an affinity of about 1/5:1:1 for the activation of transcriptional transcripts that are involved in synaptic transmission. This action in turn results from binding to a protein called interferon 3 (IF4). It is expressed via a large number of GPCRs and therefore has a direct action at'},\n",
       " {'generated_text': 'SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein D6 to inhibit the expression of human adenosine triphosphate (ADP). The results show that if a single amino acid is substituted for another, the human adenosine triphosphate is inhibited. Moreover, if either a single amino acid or another individual is added (like diphthongs and amines), the inhibition can also be reversed or increased'},\n",
       " {'generated_text': 'SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein [J-E1] in the preparation of COVID by a process called methanolysis. The methanolysis is accomplished by an action of COVID-23 on a hydrogen cyanide hydrocarbon glycerin (HCOG). The COVID-23 is used in the glycolysis of some chemicals such as nitric oxide (NO), but'},\n",
       " {'generated_text': 'SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein to generate a short string of protein and to generate its own stable and active components. The long chain protein, CAX-1, has three different bases, which have distinct roles in cell development and metabolism: α-helminin (where α is the glycolytic and is released as glucose in the cells), β-cells (where β-cell growth is regulated'},\n",
       " {'generated_text': 'SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein-binding protein (PPB) to inhibit the binding of a group of protein p38 and inhibit an anti-inflammatory compound that prevents the growth of TNF-α. Moreover, the compound reduces the binding of pro-inflammatory molecules. The PBP-binding protein and the anti-inflammatory compound can stimulate the cell proliferation and induce apoptosis to the cells by inducing an apopt'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model bias - GPT-2 was trained on novels and other story-like texts, so we will get really poor results in specialized domains\n",
    "generator('SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein', num_return_sequences=5, max_length=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical considerations & Subject Matter Experts\n",
    "\n",
    "The above is good example of how large language models kind 'ramble'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "# from transformers import BloomModel, AutoTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
    "           output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contextual embeddings\n",
    "\n",
    "def bert_text_preparation(text, tokenizer):\n",
    "  \"\"\"\n",
    "  Preprocesses text input in a way that BERT can interpret.\n",
    "  \"\"\"\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "  tokenized_text = tokenizer.tokenize(marked_text)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "  # convert inputs to tensors\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "  return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    \"\"\"\n",
    "    Obtains BERT embeddings for tokens, in context of the given sentence.\n",
    "    \"\"\"\n",
    "    # gradient calculation id disabled\n",
    "    with torch.no_grad():\n",
    "      # obtain hidden states\n",
    "      outputs = model(tokens_tensor, segments_tensor)\n",
    "      # print(outputs[0])\n",
    "      hidden_states = outputs[2]\n",
    "\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # where Y is the number of tokens in the sentence\n",
    "\n",
    "    # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # \"token\" is a [12 x 768] tensor\n",
    "\n",
    "        # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    return token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Advancing mHealth-supported Adoption and Sustainment of an Evidence-based Mental Health Intervention for Youth in a School-based Delivery Setting in Sierra Leone',\n",
    "             'Refining and Pilot Testing a Decision Support Intervention to Facilitate Adoption of Evidence-Based Programs to Improve Parent and Child Mental Health',\n",
    "             'Reusable, transparent, and reconfigurable N95-equivalent Respirator Masks: design, fabrication, and trials for enhanced adoption',\n",
    "             'Understanding the Adoption and Impact of New Risk Assessment Technologies in Prostate Cancer Care',\n",
    "             'Addressing adoption barriers to patient transportation services',\n",
    "             'The College Alcohol Intervention Matrix (College AIM): Adoption and Implementation Across College Campuses',\n",
    "             'Social Networks of Diffusion and Adoption: Investigating the Network Effects on implementation of evidence-based interventions for early intervention providers of children',\n",
    "             'HPV ECHO: Increasing the adoption of evidence-based communication strategies for HPV vaccination in rural primary care practices',\n",
    "             'Understanding disparities in the adoption and use of assistive technology by older Hispanics',\n",
    "             'Adoption and Implementation of an Evidence-based Safe Driving Program for High-Risk Teen Drivers',\n",
    "             'Motion Sequencing for All: pipelining, distribution and training to enable broad adoption of a next-generation platform for behavioral and neurobehavioral analysis',\n",
    "             \"The Implementation, Adoption, and Sustainability of Ho'ouna Pono\",\n",
    "             \"The Challenges and Benefits of Adopting Teens: A Comparative Study\",\n",
    "             \"Navigating the Unique Needs of Adolescent Adoption\",\n",
    "             \"The Impact of Timing on Adoption Outcomes: Examining Infant and Teen Adoption\",\n",
    "             \"Supporting the Transition to Adulthood in Adopted Teens\",\n",
    "             \"Exploring the Long-Term Effects of Adopting Teens versus Infants\",\n",
    "             \"Adopting Teens: A Systematic Review of the Literature\",\n",
    "             \"Addressing the Stereotypes and Realities of Adopting Teens\",\n",
    "             \"Comparing the Parenting Experiences of Adopting Infants and Teens\"\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
    "  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "\n",
    "  # make ordered dictionary to keep track of the position of each word\n",
    "  tokens = OrderedDict()\n",
    "\n",
    "  # loop over tokens in sensitive sentence\n",
    "  for token in tokenized_text[1:-1]:\n",
    "    # keep track of position of word and whether it occurs multiple times\n",
    "    if token in tokens:\n",
    "      tokens[token] += 1\n",
    "    else:\n",
    "      tokens[token] = 1\n",
    "\n",
    "    # compute the position of the current token\n",
    "    token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "    current_index = token_indices[tokens[token]-1]\n",
    "\n",
    "    # get the corresponding embedding\n",
    "    token_vec = list_token_embeddings[current_index]\n",
    "    \n",
    "    # save values\n",
    "    context_tokens.append(token)\n",
    "    context_embeddings.append(token_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advancing',\n",
       " 'mh',\n",
       " '##eal',\n",
       " '##th',\n",
       " '-',\n",
       " 'supported',\n",
       " 'adoption',\n",
       " 'and',\n",
       " 'sustain',\n",
       " '##ment',\n",
       " 'of',\n",
       " 'an',\n",
       " 'evidence',\n",
       " '-',\n",
       " 'based',\n",
       " 'mental',\n",
       " 'health',\n",
       " 'intervention',\n",
       " 'for',\n",
       " 'youth',\n",
       " 'in',\n",
       " 'a',\n",
       " 'school',\n",
       " '-',\n",
       " 'based',\n",
       " 'delivery',\n",
       " 'setting',\n",
       " 'in',\n",
       " 'sie',\n",
       " '##rr',\n",
       " '##a',\n",
       " 'leon',\n",
       " '##e',\n",
       " 'ref',\n",
       " '##ining',\n",
       " 'and',\n",
       " 'pilot',\n",
       " 'testing',\n",
       " 'a',\n",
       " 'decision',\n",
       " 'support',\n",
       " 'intervention',\n",
       " 'to',\n",
       " 'facilitate',\n",
       " 'adoption',\n",
       " 'of',\n",
       " 'evidence',\n",
       " '-',\n",
       " 'based',\n",
       " 'programs',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'parent',\n",
       " 'and',\n",
       " 'child',\n",
       " 'mental',\n",
       " 'health',\n",
       " 're',\n",
       " '##usa',\n",
       " '##ble',\n",
       " ',',\n",
       " 'transparent',\n",
       " ',',\n",
       " 'and',\n",
       " 'recon',\n",
       " '##fig',\n",
       " '##urable',\n",
       " 'n',\n",
       " '##95',\n",
       " '-',\n",
       " 'equivalent',\n",
       " 'respir',\n",
       " '##ator',\n",
       " 'masks',\n",
       " ':',\n",
       " 'design',\n",
       " ',',\n",
       " 'fabrication',\n",
       " ',',\n",
       " 'and',\n",
       " 'trials',\n",
       " 'for',\n",
       " 'enhanced',\n",
       " 'adoption',\n",
       " 'understanding',\n",
       " 'the',\n",
       " 'adoption',\n",
       " 'and',\n",
       " 'impact',\n",
       " 'of',\n",
       " 'new',\n",
       " 'risk',\n",
       " 'assessment',\n",
       " 'technologies',\n",
       " 'in',\n",
       " 'prostate',\n",
       " 'cancer',\n",
       " 'care',\n",
       " 'addressing',\n",
       " 'adoption',\n",
       " 'barriers',\n",
       " 'to',\n",
       " 'patient',\n",
       " 'transportation',\n",
       " 'services',\n",
       " 'the',\n",
       " 'college',\n",
       " 'alcohol',\n",
       " 'intervention',\n",
       " 'matrix',\n",
       " '(',\n",
       " 'college',\n",
       " 'aim',\n",
       " ')',\n",
       " ':',\n",
       " 'adoption',\n",
       " 'and',\n",
       " 'implementation',\n",
       " 'across',\n",
       " 'college',\n",
       " 'campus',\n",
       " '##es',\n",
       " 'social',\n",
       " 'networks',\n",
       " 'of',\n",
       " 'diffusion',\n",
       " 'and',\n",
       " 'adoption',\n",
       " ':',\n",
       " 'investigating',\n",
       " 'the',\n",
       " 'network',\n",
       " 'effects',\n",
       " 'on',\n",
       " 'implementation',\n",
       " 'of',\n",
       " 'evidence',\n",
       " '-',\n",
       " 'based',\n",
       " 'interventions',\n",
       " 'for',\n",
       " 'early',\n",
       " 'intervention',\n",
       " 'providers',\n",
       " 'of',\n",
       " 'children',\n",
       " 'hpv',\n",
       " 'echo',\n",
       " ':',\n",
       " 'increasing',\n",
       " 'the',\n",
       " 'adoption',\n",
       " 'of',\n",
       " 'evidence',\n",
       " '-',\n",
       " 'based',\n",
       " 'communication',\n",
       " 'strategies',\n",
       " 'for',\n",
       " 'hpv',\n",
       " 'vaccination',\n",
       " 'in',\n",
       " 'rural',\n",
       " 'primary',\n",
       " 'care',\n",
       " 'practices',\n",
       " 'understanding',\n",
       " 'disparities',\n",
       " 'in',\n",
       " 'the',\n",
       " 'adoption',\n",
       " 'and',\n",
       " 'use',\n",
       " 'of',\n",
       " 'assist',\n",
       " '##ive',\n",
       " 'technology',\n",
       " 'by',\n",
       " 'older',\n",
       " 'hispanics',\n",
       " 'adoption',\n",
       " 'and',\n",
       " 'implementation',\n",
       " 'of',\n",
       " 'an',\n",
       " 'evidence',\n",
       " '-',\n",
       " 'based',\n",
       " 'safe',\n",
       " 'driving',\n",
       " 'program',\n",
       " 'for',\n",
       " 'high',\n",
       " '-',\n",
       " 'risk',\n",
       " 'teen',\n",
       " 'drivers',\n",
       " 'motion',\n",
       " 'sequencing',\n",
       " 'for',\n",
       " 'all',\n",
       " ':',\n",
       " 'pip',\n",
       " '##elin',\n",
       " '##ing',\n",
       " ',',\n",
       " 'distribution',\n",
       " 'and',\n",
       " 'training',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'broad',\n",
       " 'adoption',\n",
       " 'of',\n",
       " 'a',\n",
       " 'next',\n",
       " '-',\n",
       " 'generation',\n",
       " 'platform',\n",
       " 'for',\n",
       " 'behavioral',\n",
       " 'and',\n",
       " 'neurobehavioral',\n",
       " 'analysis',\n",
       " 'the',\n",
       " 'implementation',\n",
       " ',',\n",
       " 'adoption',\n",
       " ',',\n",
       " 'and',\n",
       " 'sustainability',\n",
       " 'of',\n",
       " 'ho',\n",
       " \"'\",\n",
       " 'ou',\n",
       " '##na',\n",
       " 'pon',\n",
       " '##o',\n",
       " 'the',\n",
       " 'challenges',\n",
       " 'and',\n",
       " 'benefits',\n",
       " 'of',\n",
       " 'adopting',\n",
       " 'teen',\n",
       " '##s',\n",
       " ':',\n",
       " 'a',\n",
       " 'comparative',\n",
       " 'study',\n",
       " 'navig',\n",
       " '##ating',\n",
       " 'the',\n",
       " 'unique',\n",
       " 'needs',\n",
       " 'of',\n",
       " 'adolescent',\n",
       " 'adoption',\n",
       " 'the',\n",
       " 'impact',\n",
       " 'of',\n",
       " 'timing',\n",
       " 'on',\n",
       " 'adoption',\n",
       " 'outcomes',\n",
       " ':',\n",
       " 'examining',\n",
       " 'infant',\n",
       " 'and',\n",
       " 'teen',\n",
       " 'adoption',\n",
       " 'supporting',\n",
       " 'the',\n",
       " 'transition',\n",
       " 'to',\n",
       " 'adulthood',\n",
       " 'in',\n",
       " 'adopted',\n",
       " 'teen',\n",
       " '##s',\n",
       " 'exploring',\n",
       " 'the',\n",
       " 'long',\n",
       " '-',\n",
       " 'term',\n",
       " 'effects',\n",
       " 'of',\n",
       " 'adopting',\n",
       " 'teen',\n",
       " '##s',\n",
       " 'versus',\n",
       " 'infants',\n",
       " 'adopting',\n",
       " 'teen',\n",
       " '##s',\n",
       " ':',\n",
       " 'a',\n",
       " 'systematic',\n",
       " 'review',\n",
       " 'of',\n",
       " 'the',\n",
       " 'literature',\n",
       " 'addressing',\n",
       " 'the',\n",
       " 'stereot',\n",
       " '##yp',\n",
       " '##es',\n",
       " 'and',\n",
       " 'real',\n",
       " '##ities',\n",
       " 'of',\n",
       " 'adopting',\n",
       " 'teen',\n",
       " '##s',\n",
       " 'comparing',\n",
       " 'the',\n",
       " 'parenting',\n",
       " 'experiences',\n",
       " 'of',\n",
       " 'adopting',\n",
       " 'infants',\n",
       " 'and',\n",
       " 'teen',\n",
       " '##s']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>Advancing mHealth-supported Adoption and Susta...</td>\n",
       "      <td>0.753539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>Refining and Pilot Testing a Decision Support ...</td>\n",
       "      <td>0.746505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>Understanding the Adoption and Impact of New R...</td>\n",
       "      <td>0.736815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Reusable, transparent, and reconfigurable N95-...</td>\n",
       "      <td>Addressing adoption barriers to patient transp...</td>\n",
       "      <td>0.766890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>The Implementation, Adoption, and Sustainabili...</td>\n",
       "      <td>0.906431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>The Challenges and Benefits of Adopting Teens:...</td>\n",
       "      <td>0.732473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>Navigating the Unique Needs of Adolescent Adop...</td>\n",
       "      <td>0.820002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Motion Sequencing for All: pipelining, distrib...</td>\n",
       "      <td>The Impact of Timing on Adoption Outcomes: Exa...</td>\n",
       "      <td>0.736598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence_1  \\\n",
       "30   Reusable, transparent, and reconfigurable N95-...   \n",
       "31   Reusable, transparent, and reconfigurable N95-...   \n",
       "32   Reusable, transparent, and reconfigurable N95-...   \n",
       "33   Reusable, transparent, and reconfigurable N95-...   \n",
       "34   Reusable, transparent, and reconfigurable N95-...   \n",
       "..                                                 ...   \n",
       "160  Motion Sequencing for All: pipelining, distrib...   \n",
       "161  Motion Sequencing for All: pipelining, distrib...   \n",
       "162  Motion Sequencing for All: pipelining, distrib...   \n",
       "163  Motion Sequencing for All: pipelining, distrib...   \n",
       "164  Motion Sequencing for All: pipelining, distrib...   \n",
       "\n",
       "                                            sentence_2  distance  \n",
       "30   Advancing mHealth-supported Adoption and Susta...  0.753539  \n",
       "31   Refining and Pilot Testing a Decision Support ...  0.746505  \n",
       "32   Reusable, transparent, and reconfigurable N95-...  1.000000  \n",
       "33   Understanding the Adoption and Impact of New R...  0.736815  \n",
       "34   Addressing adoption barriers to patient transp...  0.766890  \n",
       "..                                                 ...       ...  \n",
       "160  Motion Sequencing for All: pipelining, distrib...  1.000000  \n",
       "161  The Implementation, Adoption, and Sustainabili...  0.906431  \n",
       "162  The Challenges and Benefits of Adopting Teens:...  0.732473  \n",
       "163  Navigating the Unique Needs of Adolescent Adop...  0.820002  \n",
       "164  The Impact of Timing on Adoption Outcomes: Exa...  0.736598  \n",
       "\n",
       "[75 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# embeddings for the word 'record' \n",
    "token = 'adoption'\n",
    "indices = [i for i, t in enumerate(context_tokens) if t == token]\n",
    "\n",
    "token_embeddings = [context_embeddings[i] for i in indices]\n",
    "\n",
    "# # compare 'record' with different contexts\n",
    "list_of_distances = []\n",
    "for sentence_1, embed1 in zip(sentences, token_embeddings):\n",
    "  for sentence_2, embed2 in zip(sentences, token_embeddings):\n",
    "    cos_dist = 1 - cosine(embed1, embed2)\n",
    "    list_of_distances.append([sentence_1, sentence_2, cos_dist])\n",
    "\n",
    "distances_df = pd.DataFrame(list_of_distances, columns=['sentence_1', 'sentence_2', 'distance'])\n",
    "distances_df[distances_df.sentence_1.str.contains('adoption')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filepath = os.path.join('gdrive/My Drive/projections/')\n",
    "\n",
    "name = 'metadata.tsv'\n",
    "\n",
    "with open(os.path.join(filepath, name), 'w+') as file_metadata:\n",
    "  for i, token in enumerate(context_tokens):\n",
    "    file_metadata.write(token + '\\n')\n",
    "    \n",
    "import csv\n",
    "\n",
    "name = 'embeddings.tsv'\n",
    "\n",
    "with open(os.path.join(filepath, name), 'w+') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "    for embedding in context_embeddings:\n",
    "        writer.writerow(embedding.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
