{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "## Introduction to Transformers Overview\n",
    "\n",
    "* Our character RNN trained\n",
    "* Introduction to Transformers\n",
    "* HuggingFace Transformers library\n",
    "* Transformers for NLP\n",
    "* Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Transformers\n",
    "\n",
    "### Milestones in Transformer Models\n",
    "\n",
    "* Vaswani, Ashish, et al. Attention Is All You Need. arXiv:1706.03762, arXiv, 5 Dec. 2017. arXiv.org, https://doi.org/10.48550/arXiv.1706.03762.\n",
    "\n",
    "### Some import models\n",
    "\n",
    "* June 2018: GPT (OpenAI)\n",
    "* October 2018: BERT (Google - summaries of sentences)\n",
    "* February 2019: GPT-2 (OpenAI - not immediately released due to ethical concerns)\n",
    "* October 2019: DistilBERT (Faster and better memory performance than BERT)\n",
    "* October 2019: BART and T5 (large pretrained models)\n",
    "* May 2020: GPT-3 (OpenAI - zero-shot learning)\n",
    "\n",
    "\n",
    "### Key ideas\n",
    "\n",
    "* Pretraining - Input is a very large corpus of text for weeks or months\n",
    "* Fine-tuning - Input is a specific task (e.g. sentiment analysis)\n",
    "* Encoder - Models that are good for understanding the input, like sentence classification or named entity recognition\n",
    "* Decoder - Models that are good for generating output, like text generation or summarization\n",
    "* Attention layers - Model attends to different relationships in different layers [BERT](https://huggingface.co/exbert/?model=bert-base-uncased&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=0&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true)\n",
    "\n",
    "[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Transformers library\n",
    "\n",
    "* [HuggingFace Transformers](https://huggingface.co/transformers/)\n",
    "* [Natural Language Processing Course](https://huggingface.co/course/chapter1/1)\n",
    "\n",
    "<center><img src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" height=\"200\" width=\"200\"></center>\n",
    "\n",
    "### Docs and Tutorials\n",
    "\n",
    "* [Docs](https://huggingface.co/transformers/)\n",
    "* [Tutorials](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "### Installation\n",
    "\n",
    "* `pip install transformers`\n",
    "* `pip install datasets`\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* [Datasets](https://huggingface.co/datasets/)\n",
    "  * Multimodal\n",
    "  * Computer Vision\n",
    "  * NLP\n",
    "  * Audio\n",
    "  * Tabular\n",
    "\n",
    "* NLP Datasets for various tasks\n",
    "  * Text Classification\n",
    "  * Token Classification\n",
    "  * Table Question Answering\n",
    "  * Question Answering\n",
    "  * Zero-Shot Classification\n",
    "  * Translation\n",
    "  * Summarization\n",
    "  * Conversational\n",
    "  * Text Generation\n",
    "  * Text2Text Generation\n",
    "  * Fill Mask\n",
    "  * Sentence similarity\n",
    "  * Table to text\n",
    "  * Multi-choice\n",
    "  * Text retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Datasets https://github.com/huggingface/datasets\n",
    "# !pip install datasets\n",
    "\n",
    "from huggingface_hub import list_datasets\n",
    "# from datasets import list_datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "all_ds = list(list_datasets())\n",
    "print(f'There are {len(all_ds)} datasets available on the HuggingFace Hub')\n",
    "print(f'The first 10 are: {all_ds[:10]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('We are very happy to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## zero-shot classification\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "classifier('We are very happy to show you the ðŸ¤— Transformers library.', candidate_labels=['politics', 'business', 'sports', 'technology'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "generator('Frodo and Sam were walking through the Shire when')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named entity recognition\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline('ner', grouped_entities=True)\n",
    "ner('Mary graduates this spring from William and Mary. She will continue to study Natural Language Processing at MIT.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question answering\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline('question-answering')\n",
    "\n",
    "question_answerer(\n",
    "    question='Where does Mary study?',\n",
    "    context='Mary graduates this spring from William and Mary. She will continue to study Natural Language Processing at MIT.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarization\n",
    "\n",
    "summarizer = pipeline('summarization', max_length=48, min_length=30, do_sample=False, model='t5-base')\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    The Transformers library provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...)\n",
    "    for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and\n",
    "    deep interoperability between TensorFlow 2.0 and PyTorch.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The importance of training data\n",
    "\n",
    "While the above uses are super easy, the real power of Transformers comes from the fact that they can be fine-tuned on a wide variety of tasks with just a few lines of code. This is made possible by the fact that they are pretrained on a large dataset (usually a few hundred million words) and then fine-tuned on a specific task. This is why Transformers are so powerful and why they are so widely used in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model bias - GPT-2 was trained on novels and other story-like texts, so we will get really poor results in specialized domains\n",
    "generator('SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein', num_return_sequences=5, max_length=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical considerations & Subject Matter Experts\n",
    "\n",
    "The above is good example of how large language models kind 'ramble'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "# from transformers import BloomModel, AutoTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
    "           output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contextual embeddings\n",
    "\n",
    "def bert_text_preparation(text, tokenizer):\n",
    "  \"\"\"\n",
    "  Preprocesses text input in a way that BERT can interpret.\n",
    "  \"\"\"\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "  tokenized_text = tokenizer.tokenize(marked_text)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "  # convert inputs to tensors\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "  return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    \"\"\"\n",
    "    Obtains BERT embeddings for tokens, in context of the given sentence.\n",
    "    \"\"\"\n",
    "    # gradient calculation id disabled\n",
    "    with torch.no_grad():\n",
    "      # obtain hidden states\n",
    "      outputs = model(tokens_tensor, segments_tensor)\n",
    "      # print(outputs[0])\n",
    "      hidden_states = outputs[2]\n",
    "\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # where Y is the number of tokens in the sentence\n",
    "\n",
    "    # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # \"token\" is a [12 x 768] tensor\n",
    "\n",
    "        # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    return token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Advancing mHealth-supported Adoption and Sustainment of an Evidence-based Mental Health Intervention for Youth in a School-based Delivery Setting in Sierra Leone',\n",
    "             'Refining and Pilot Testing a Decision Support Intervention to Facilitate Adoption of Evidence-Based Programs to Improve Parent and Child Mental Health',\n",
    "             'Reusable, transparent, and reconfigurable N95-equivalent Respirator Masks: design, fabrication, and trials for enhanced adoption',\n",
    "             'Understanding the Adoption and Impact of New Risk Assessment Technologies in Prostate Cancer Care',\n",
    "             'Addressing adoption barriers to patient transportation services',\n",
    "             'The College Alcohol Intervention Matrix (College AIM): Adoption and Implementation Across College Campuses',\n",
    "             'Social Networks of Diffusion and Adoption: Investigating the Network Effects on implementation of evidence-based interventions for early intervention providers of children',\n",
    "             'HPV ECHO: Increasing the adoption of evidence-based communication strategies for HPV vaccination in rural primary care practices',\n",
    "             'Understanding disparities in the adoption and use of assistive technology by older Hispanics',\n",
    "             'Adoption and Implementation of an Evidence-based Safe Driving Program for High-Risk Teen Drivers',\n",
    "             'Motion Sequencing for All: pipelining, distribution and training to enable broad adoption of a next-generation platform for behavioral and neurobehavioral analysis',\n",
    "             \"The Implementation, Adoption, and Sustainability of Ho'ouna Pono\",\n",
    "             \"The Challenges and Benefits of Adopting Teens: A Comparative Study\",\n",
    "             \"Navigating the Unique Needs of Adolescent Adoption\",\n",
    "             \"The Impact of Timing on Adoption Outcomes: Examining Infant and Teen Adoption\",\n",
    "             \"Supporting the Transition to Adulthood in Adopted Teens\",\n",
    "             \"Exploring the Long-Term Effects of Adopting Teens versus Infants\",\n",
    "             \"Adopting Teens: A Systematic Review of the Literature\",\n",
    "             \"Addressing the Stereotypes and Realities of Adopting Teens\",\n",
    "             \"Comparing the Parenting Experiences of Adopting Infants and Teens\"\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
    "  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "\n",
    "  # make ordered dictionary to keep track of the position of each word\n",
    "  tokens = OrderedDict()\n",
    "\n",
    "  # loop over tokens in sensitive sentence\n",
    "  for token in tokenized_text[1:-1]:\n",
    "    # keep track of position of word and whether it occurs multiple times\n",
    "    if token in tokens:\n",
    "      tokens[token] += 1\n",
    "    else:\n",
    "      tokens[token] = 1\n",
    "\n",
    "    # compute the position of the current token\n",
    "    token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "    current_index = token_indices[tokens[token]-1]\n",
    "\n",
    "    # get the corresponding embedding\n",
    "    token_vec = list_token_embeddings[current_index]\n",
    "    \n",
    "    # save values\n",
    "    context_tokens.append(token)\n",
    "    context_embeddings.append(token_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# embeddings for the word 'record' \n",
    "token = 'adoption'\n",
    "indices = [i for i, t in enumerate(context_tokens) if t == token]\n",
    "\n",
    "token_embeddings = [context_embeddings[i] for i in indices]\n",
    "\n",
    "# # compare 'record' with different contexts\n",
    "list_of_distances = []\n",
    "for sentence_1, embed1 in zip(sentences, token_embeddings):\n",
    "  for sentence_2, embed2 in zip(sentences, token_embeddings):\n",
    "    cos_dist = 1 - cosine(embed1, embed2)\n",
    "    list_of_distances.append([sentence_1, sentence_2, cos_dist])\n",
    "\n",
    "distances_df = pd.DataFrame(list_of_distances, columns=['sentence_1', 'sentence_2', 'distance'])\n",
    "distances_df[distances_df.sentence_1.str.contains('adoption')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filepath = os.path.join('gdrive/My Drive/projections/')\n",
    "\n",
    "name = 'metadata.tsv'\n",
    "\n",
    "with open(os.path.join(filepath, name), 'w+') as file_metadata:\n",
    "  for i, token in enumerate(context_tokens):\n",
    "    file_metadata.write(token + '\\n')\n",
    "    \n",
    "import csv\n",
    "\n",
    "name = 'embeddings.tsv'\n",
    "\n",
    "with open(os.path.join(filepath, name), 'w+') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "    for embedding in context_embeddings:\n",
    "        writer.writerow(embedding.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
