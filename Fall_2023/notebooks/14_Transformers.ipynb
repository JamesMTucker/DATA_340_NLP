{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "## Introduction to Transformers Overview\n",
    "\n",
    "* Our character RNN trained\n",
    "* Introduction to Transformers\n",
    "* HuggingFace Transformers library\n",
    "* Transformers for NLP\n",
    "* Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Transformers\n",
    "\n",
    "### Milestones in Transformer Models\n",
    "\n",
    "* Vaswani, Ashish, et al. Attention Is All You Need. arXiv:1706.03762, arXiv, 5 Dec. 2017. arXiv.org, https://doi.org/10.48550/arXiv.1706.03762.\n",
    "\n",
    "### Some import models\n",
    "\n",
    "* June 2018: GPT (OpenAI)\n",
    "* October 2018: BERT (Google - summaries of sentences)\n",
    "* February 2019: GPT-2 (OpenAI - not immediately released due to ethical concerns)\n",
    "* October 2019: DistilBERT (Faster and better memory performance than BERT)\n",
    "* October 2019: BART and T5 (large pretrained models)\n",
    "* May 2020: GPT-3 (OpenAI - zero-shot learning)\n",
    "\n",
    "\n",
    "### Key ideas\n",
    "\n",
    "* Pretraining - Input is a very large corpus of text for weeks or months\n",
    "* Fine-tuning - Input is a specific task (e.g. sentiment analysis)\n",
    "* Encoder - Models that are good for understanding the input, like sentence classification or named entity recognition\n",
    "* Decoder - Models that are good for generating output, like text generation or summarization\n",
    "* Attention layers - Model attends to different relationships in different layers [BERT](https://huggingface.co/exbert/?model=bert-base-uncased&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=0&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true)\n",
    "\n",
    "[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Transformers library\n",
    "\n",
    "* [HuggingFace Transformers](https://huggingface.co/transformers/)\n",
    "* [Natural Language Processing Course](https://huggingface.co/course/chapter1/1)\n",
    "\n",
    "<center><img src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" height=\"200\" width=\"200\"></center>\n",
    "\n",
    "### Docs and Tutorials\n",
    "\n",
    "* [Docs](https://huggingface.co/transformers/)\n",
    "* [Tutorials](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "### Installation\n",
    "\n",
    "* `pip install transformers`\n",
    "* `pip install datasets`\n",
    "\n",
    "### Datasets\n",
    "\n",
    "* [Datasets](https://huggingface.co/datasets/)\n",
    "  * Multimodal\n",
    "  * Computer Vision\n",
    "  * NLP\n",
    "  * Audio\n",
    "  * Tabular\n",
    "\n",
    "* NLP Datasets for various tasks\n",
    "  * Text Classification\n",
    "  * Token Classification\n",
    "  * Table Question Answering\n",
    "  * Question Answering\n",
    "  * Zero-Shot Classification\n",
    "  * Translation\n",
    "  * Summarization\n",
    "  * Conversational\n",
    "  * Text Generation\n",
    "  * Text2Text Generation\n",
    "  * Fill Mask\n",
    "  * Sentence similarity\n",
    "  * Table to text\n",
    "  * Multi-choice\n",
    "  * Text retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Datasets https://github.com/huggingface/datasets\n",
    "# !pip install datasets\n",
    "\n",
    "from huggingface_hub import list_datasets\n",
    "# from datasets import list_datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "all_ds = list(list_datasets())\n",
    "print(f'There are {len(all_ds)} datasets available on the HuggingFace Hub')\n",
    "print(f'The first 10 are: {all_ds[:10]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('We are very happy to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## zero-shot classification\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('zero-shot-classification')\n",
    "classifier('We are very happy to show you the ðŸ¤— Transformers library.', candidate_labels=['politics', 'business', 'sports', 'technology'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "generator('Frodo and Sam were walking through the Shire when')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# named entity recognition\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline('ner', grouped_entities=True)\n",
    "ner('Mary graduates this spring from William and Mary. She will continue to study Natural Language Processing at MIT.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question answering\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline('question-answering')\n",
    "\n",
    "question_answerer(\n",
    "    question='Where does Mary study?',\n",
    "    context='Mary graduates this spring from William and Mary. She will continue to study Natural Language Processing at MIT.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarization\n",
    "\n",
    "summarizer = pipeline('summarization', max_length=48, min_length=30, do_sample=False, model='t5-base')\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    The Transformers library provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...)\n",
    "    for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and\n",
    "    deep interoperability between TensorFlow 2.0 and PyTorch.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The importance of training data\n",
    "\n",
    "While the above uses are super easy, the real power of Transformers comes from the fact that they can be fine-tuned on a wide variety of tasks with just a few lines of code. This is made possible by the fact that they are pretrained on a large dataset (usually a few hundred million words) and then fine-tuned on a specific task. This is why Transformers are so powerful and why they are so widely used in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model bias - GPT-2 was trained on novels and other story-like texts, so we will get really poor results in specialized domains\n",
    "generator('SARS-CoV-2, the causative agent of COVID-19, employs its spike glycoprotein', num_return_sequences=5, max_length=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical considerations & Subject Matter Experts\n",
    "\n",
    "The above is good example of how large language models kind 'ramble'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec, GloVe, FastText Embeddings (Shortcomings)\n",
    "\n",
    "The above embeddings are static and do not change based on the context of the sentence. For example, the word 'bank' has different meanings in the following sentences:\n",
    "\n",
    "* I went to the bank to deposit my money.\n",
    "* I sat on the bank of the river and watched the water flow by.\n",
    "* The river bank was full of dead fish.\n",
    "* The bank was robbed yesterday.\n",
    "* I banked on it.\n",
    "* I banked the plane to the left.\n",
    "\n",
    "### Contextualized Embeddings\n",
    "\n",
    "* [ELMo](https://allennlp.org/elmo)\n",
    "* [ULMFiT](https://arxiv.org/abs/1801.06146)\n",
    "* [OpenAI GPT](https://openai.com/blog/language-unsupervised/)\n",
    "* [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "* BERT = Bidirectional Encoder Representations from Transformers\n",
    "* \"BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks ...\" [Delvin et al.](https://arxiv.org/abs/1810.04805)\n",
    "* Trained on a masked language model (MLM) \n",
    "    * [Taylor, 1953](https://gwern.net/doc/psychology/writing/1953-taylor.pdf)\n",
    "    * Randomly mask a percentage of the input and predict the masked words based on the context\n",
    "* Trained on a next sentence prediction (NSP) task\n",
    "    * [Bengio et al., 2003](https://www.aclweb.org/anthology/P03-1003.pdf)\n",
    "    * Given two sentences, predict if the second sentence is the next sentence in a document\n",
    "* [BERT](https://github.dev/google-research/bert) code from Google\n",
    "\n",
    "\n",
    "<center><img src=\"../images/bert.png\" height=\"400\" width=\"1000\"></center>\n",
    "\n",
    "* [Leaderboard](https://gluebenchmark.com/)\n",
    "\n",
    "### Downstream Tasks\n",
    "\n",
    "* [Diachronic linguistic change, Giulianelli et al.](https://aclanthology.org/2020.acl-main.365)\n",
    "* [Linguistic style](https://arxiv.org/abs/1905.05621)\n",
    "* [Vector semantics](https://library.oapen.org/handle/20.500.12657/60191)\n",
    "* Polysemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Contextual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "# from transformers import BloomModel, AutoTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
    "           output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contextual embeddings\n",
    "\n",
    "def bert_text_preparation(text, tokenizer):\n",
    "  \"\"\"\n",
    "  Preprocesses text input in a way that BERT can interpret.\n",
    "  \"\"\"\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "  tokenized_text = tokenizer.tokenize(marked_text)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "  # convert inputs to tensors\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "  return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    \"\"\"\n",
    "    Obtains BERT embeddings for tokens, in context of the given sentence.\n",
    "    \"\"\"\n",
    "    # gradient calculation id disabled\n",
    "    with torch.no_grad():\n",
    "      # obtain hidden states\n",
    "      outputs = model(tokens_tensor, segments_tensor)\n",
    "      # print(outputs[0])\n",
    "      hidden_states = outputs[2]\n",
    "\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # where Y is the number of tokens in the sentence\n",
    "\n",
    "    # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # \"token\" is a [12 x 768] tensor\n",
    "\n",
    "        # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    return token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Advancing mHealth-supported Adoption and Sustainment of an Evidence-based Mental Health Intervention for Youth in a School-based Delivery Setting in Sierra Leone',\n",
    "             'Refining and Pilot Testing a Decision Support Intervention to Facilitate Adoption of Evidence-Based Programs to Improve Parent and Child Mental Health',\n",
    "             'Reusable, transparent, and reconfigurable N95-equivalent Respirator Masks: design, fabrication, and trials for enhanced adoption',\n",
    "             'Understanding the Adoption and Impact of New Risk Assessment Technologies in Prostate Cancer Care',\n",
    "             'Addressing adoption barriers to patient transportation services',\n",
    "             'The College Alcohol Intervention Matrix (College AIM): Adoption and Implementation Across College Campuses',\n",
    "             'Social Networks of Diffusion and Adoption: Investigating the Network Effects on implementation of evidence-based interventions for early intervention providers of children',\n",
    "             'HPV ECHO: Increasing the adoption of evidence-based communication strategies for HPV vaccination in rural primary care practices',\n",
    "             'Understanding disparities in the adoption and use of assistive technology by older Hispanics',\n",
    "             'Adoption and Implementation of an Evidence-based Safe Driving Program for High-Risk Teen Drivers',\n",
    "             'Motion Sequencing for All: pipelining, distribution and training to enable broad adoption of a next-generation platform for behavioral and neurobehavioral analysis',\n",
    "             \"The Implementation, Adoption, and Sustainability of Ho'ouna Pono\",\n",
    "             \"The Challenges and Benefits of Adopting Teens: A Comparative Study\",\n",
    "             \"Navigating the Unique Needs of Adolescent Adoption\",\n",
    "             \"The Impact of Timing on Adoption Outcomes: Examining Infant and Teen Adoption\",\n",
    "             \"Supporting the Transition to Adulthood in Adopted Teens\",\n",
    "             \"Exploring the Long-Term Effects of Adopting Teens versus Infants\",\n",
    "             \"Adopting Teens: A Systematic Review of the Literature\",\n",
    "             \"Addressing the Stereotypes and Realities of Adopting Teens\",\n",
    "             \"Comparing the Parenting Experiences of Adopting Infants and Teens\"\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence, tokenizer)\n",
    "  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "\n",
    "  # make ordered dictionary to keep track of the position of each word\n",
    "  tokens = OrderedDict()\n",
    "\n",
    "  # loop over tokens in sensitive sentence\n",
    "  for token in tokenized_text[1:-1]:\n",
    "    # keep track of position of word and whether it occurs multiple times\n",
    "    if token in tokens:\n",
    "      tokens[token] += 1\n",
    "    else:\n",
    "      tokens[token] = 1\n",
    "\n",
    "    # compute the position of the current token\n",
    "    token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "    current_index = token_indices[tokens[token]-1]\n",
    "\n",
    "    # get the corresponding embedding\n",
    "    token_vec = list_token_embeddings[current_index]\n",
    "    \n",
    "    # save values\n",
    "    context_tokens.append(token)\n",
    "    context_embeddings.append(token_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# embeddings for the word 'record' \n",
    "token = 'adoption'\n",
    "indices = [i for i, t in enumerate(context_tokens) if t == token]\n",
    "\n",
    "token_embeddings = [context_embeddings[i] for i in indices]\n",
    "\n",
    "# # compare 'record' with different contexts\n",
    "list_of_distances = []\n",
    "for sentence_1, embed1 in zip(sentences, token_embeddings):\n",
    "  for sentence_2, embed2 in zip(sentences, token_embeddings):\n",
    "    cos_dist = 1 - cosine(embed1, embed2)\n",
    "    list_of_distances.append([sentence_1, sentence_2, cos_dist])\n",
    "\n",
    "distances_df = pd.DataFrame(list_of_distances, columns=['sentence_1', 'sentence_2', 'distance'])\n",
    "distances_df[distances_df.sentence_1.str.contains('adoption')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# filepath = os.path.join('gdrive/My Drive/projections/')\n",
    "filepath = os.path.join('.')\n",
    "name = 'metadata.tsv'\n",
    "\n",
    "with open(os.path.join(filepath, name), 'w+') as file_metadata:\n",
    "  for i, token in enumerate(context_tokens):\n",
    "    file_metadata.write(token + '\\n')\n",
    "    \n",
    "import csv\n",
    "\n",
    "name = 'embeddings.tsv'\n",
    "\n",
    "with open(os.path.join(filepath, name), 'w+') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "    for embedding in context_embeddings:\n",
    "        writer.writerow(embedding.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose compute architecture\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings with PCA, t-SNE, and/or UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "* [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) is a dimensionality reduction technique\n",
    "* PCA is an orthogonal transformation of data\n",
    "* [PCA Main ideas](https://www.youtube.com/watch?v=HMOI_lkzW08&t=161s)\n",
    "* [PCA in depth](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n",
    "* cf. [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "\n",
    "### T-Distributed Stochastic Neighbor (t-SNE)\n",
    "\n",
    "* [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) is a dimensionality reduction technique to accurately represent high dimensional data in a low dimensional space\n",
    "* Perplexity is a parameter that controls the balance between local and global structure\n",
    "* [How to use t-SNE effectively](https://distill.pub/2016/misread-tsne/)\n",
    "* [t-SNE in depth](https://www.youtube.com/watch?v=NEaUSP4YerM)\n",
    "\n",
    "### UMAP Embedding\n",
    "\n",
    "* Dimensionality reduction technique that can be used to visualize high dimensional data\n",
    "* Topological data analysis [UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html)\n",
    "* [How UMAP Works](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers in Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'Attention is all you need' [Vaswani et al.](https://arxiv.org/abs/1706.03762) 2017\n",
    "* BERT used a Transformer architecture (Significant breakthrough in AI/NLP)\n",
    "* Annotated Transformer [Harvard NLP](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "\n",
    "N.B.: The below transformer code is taken from the Annotated Transformer paper.\n",
    "\n",
    "### Key ideas\n",
    "\n",
    "* Pretraining - Input is a very large corpus of text for weeks or months\n",
    "* Fine-tuning - Input is a specific task (e.g. sentiment analysis)\n",
    "* Encoder - Models that are good for understanding the input, like sentence classification or named entity recognition\n",
    "* Decoder - Models that are good for generating output, like text generation or summarization\n",
    "* Attention layers - Model attends to different relationships in different layers [BERT](https://huggingface.co/exbert/?model=bert-base-uncased&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=0&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.7&tokenInd=null&tokenSide=null&maskInds=..&hideClsSep=true)\n",
    "\n",
    "<center><img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" height=\"600\" width=\"400\"></center>\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter03_transformer-encoder-decoder.png\" width=\"800\" height=\"400\"></center>\n",
    "\n",
    "img src NLP with Transformers, Tunstall et al. 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "* The encoder is a stack of $N$ identical layers (N=6 generally)\n",
    "* Converts the input sequence of word embeddings into a sequence of vectors (the `hidden_state`)\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter03_encoder-zoom.png\" height=\"600\" width=\"800\"></center>\n",
    "\n",
    "#### Input text\n",
    "\n",
    "* Input text is tokenized into a sequence of tokens to create token embeddings\n",
    "* Token embeddings are added to positional embeddings to capture sequence information\n",
    "* Encoding layers can be called `blocks` or `layers` - similar to Convolutional Neural Networks\n",
    "* Encoders output is fed to the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "* Attention is a mechanism that allows the model to focus on specific parts of the input sequence\n",
    "* Attention is a way to compute a weighted average of the input sequence\n",
    "\n",
    "> The main idea behind self-attention is that instead of using a fixed embedding for each token, we can use the whole sequence to compute a weighted average of each embedding. Tunstall el al. 2022:61\n",
    "\n",
    "Let $x'_i$ be the linear combination of the $x_j$'s, where the coefficients $w_{ji}$ are computed as follows:\n",
    "\n",
    "$$x'_i = \\sum_{j=1}^n w_{ji} x_j$$\n",
    "\n",
    "#### Scaled dot product attention\n",
    "\n",
    "1. Project each token embedding into three vectors: $Q$, $K$, and $V$ where $Q$ is the query, $K$ is the key, and $V$ is the value\n",
    "2. Compute the attention scores $A$ by taking the dot product of $Q$ and $K$. Large dot products are indicative to similarity and small dot products are indicative to dissimilarity.\n",
    "3. Compute the attention weights are first multiplied by a scaling factor and then passed through a softmax function to ensure that the weights sum to 1\n",
    "4. Update the token embeddings by computing multiplying the value $V$ to update the representation: $x'_i = \\sum_{j=1}^n w_{ji} v_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query, key, value\n",
    "\n",
    "Adapted from information retrieval systems:\n",
    "\n",
    "* Query - think of recipe and ingredients\n",
    "* Key - think of scanning your cupboard for ingredients\n",
    "* Value - think of the ingredients you find\n",
    "\n",
    "\n",
    "#### Atttention\n",
    "\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "or graphically:\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/48e4a5e5c44b86e1593c0945a49af9675cfd7158//images/chapter03_attention-ops.png\" height=\"200\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer architecture\n",
    "\n",
    "* nn.Linear() is a linear transformation: $y = xA^T + b$\n",
    "* nn.Module() is a base class for all neural network modules\n",
    "* nn.Dropout() applies dropout to the input\n",
    "* nn.LayerNorm() applies layer normalization to the input\n",
    "* nn.Embedding() is a lookup table that stores embeddings of a fixed dictionary and size\n",
    "* nn.GELU() applies the Gaussian error linear unit function\n",
    "* nn.bmm() performs a batch matrix-matrix product of matrices stored in input and mat2\n",
    "* model.forward() is the forward pass of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Encoder\n",
    "\n",
    "The encoder in the Transformer architecture processes the input sequence. It consists of a stack of identical layers, each containing two main subcomponents: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The multi-head attention mechanism allows the model to focus on different positions of the input sequence simultaneously, effectively capturing various aspects of the contextual relationship. The position-wise feed-forward networks apply linear transformations to each position separately and identically.\n",
    "\n",
    "## Decoder\n",
    "\n",
    "The decoder also consists of a stack of identical layers but with an additional sub-layer that performs multi-head attention over the output of the encoder stack. This structure helps the decoder focus on appropriate parts of the input sequence, enhancing its ability to generate coherent and contextually relevant outputs. The multi-head attention mechanism in the decoder ensures that predictions for a given position can depend only on known outputs at positions less than that position, thus maintaining the auto-regressive property.\n",
    "\n",
    "## Attention Mechanism\n",
    "\n",
    "The attention mechanism, particularly the scaled dot-product attention, is a key innovation in the Transformer model. It computes the attention score by taking the dot product of the query with all keys, dividing each by the square root of the dimension of the keys, applying a softmax function to obtain the weights on the values. The multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, enhancing its ability to capture complex dependencies.\n",
    "\n",
    "## Sources\n",
    "\n",
    "1. **Original Paper**: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
