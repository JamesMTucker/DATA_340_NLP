{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/Fall_2023/notebooks/01_Properties_of_Language.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Properties of Langauge\n",
    "\n",
    "In this notebook, we will explore some of the statistical properties of language. To accomplish this goal, let's introduce some import python packages for NLP.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Natural Language Toolkit - NLTK](#NLTK) | [Docs](https://www.nltk.org/)\n",
    "2. [spaCy](#spaCy) | [Docs](https://spacy.io/)\n",
    "3. [TextBlob](#TextBlob) | [Docs](https://textblob.readthedocs.io/en/dev/)\n",
    "4. [Gensim](#Gensim) | [Docs](https://radimrehurek.com/gensim/)\n",
    "5. [Stanza](#Stanza) | [Docs](https://stanfordnlp.github.io/stanza/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "NLTK is a python package for natural language processing. It is a very popular package for NLP and has been around for a long time. It is a very comprehensive package and has a lot of functionality. We will only be using a small portion of the package in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operations with NLTK\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of breaking up a string into tokens. A token is a sequence of characters that represents a unit of meaning. For example, a word can be a token in a sentence. A sentence is a token in a paragraph. A paragraph is a token in a document. A document is a token in a corpus. Thus, token's are furthermore interpreted as elements in a bag of words or elements in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"NLTK is a leading platform for building Python programs. It is an older package but still is very useful.\"\n",
    "word_tokens = word_tokenize(text)\n",
    "sent_tokens = sent_tokenize(text)\n",
    "\n",
    "print(word_tokens)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n",
    "Prior to the advanced computers we have today, NLP practioners would strip out various words from a string. The removed words came to be known as stop words. The idea was that these words did not add any value to the meaning of the string. However, with the advent of deep learning, we have found that stop words can be useful in some cases. Thus, we will not always remove stop words in our work this semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the computational process of reducing words to their root form. For example, the words \"running\", \"runs\", and \"run\" all have the same root form of \"run\". Stemming is a useful technique for reducing the number of unique words in a corpus. This can be useful for reducing the size of a vocabulary and thus the size of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(word) for word in filtered_words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging\n",
    "\n",
    "Parts of speech (POS) tagging is the process of assigning a part of speech to each word in a string. For example, the word \"run\" can be a noun or a verb. POS tagging is useful for understanding the meaning of a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "pos_tags = nltk.pos_tag(word_tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B.: Definitions of the POS are available [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some advanced operations with NLTK\n",
    "\n",
    "#### Named Entity Recognition\n",
    "\n",
    "Named entity recognition (NER) is the process of identifying named entities in a string. Named entities are things like people, places, and organizations. NER is useful for understanding the meaning of a string.\n",
    "\n",
    "#### Dependency Parsing\n",
    "\n",
    "Dependency parsing is the process of identifying the syntactic relationships between words in a string. Dependency parsing is useful for understanding the meaning of a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(pos_tags)\n",
    "\n",
    "# print the matched grammar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the matched grammar in the notebook\n",
    "# need to pip install svgling\n",
    "from nltk import Tree\n",
    "from IPython.display import display\n",
    "\n",
    "tree = Tree.fromstring(str(result))\n",
    "display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "spaCy is an open-source software library for advanced Natural Language Processing (NLP) in Python. It's designed specifically for production use and excels at large-scale information extraction tasks. Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operations with spaCy\n",
    "\n",
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"spaCy is an open-source software library for NLP.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging & Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B.: See the spaCy documentation for the definitions of the POS tags and dependency tags. [POS](https://spacy.io/api/annotation#pos-tagging) || [Dependency](https://spacy.io/api/annotation#dependency-parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some advanced operations with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors and Similarity\n",
    "\n",
    "Word vectors are multi-dimensional representations of words. Word vectors are useful for understanding the meaning of a string. Word vectors are also useful for finding similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token1 = nlp(\"king\")\n",
    "token2 = nlp(\"queen\")\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matcher\n",
    "\n",
    "spaCy Matcher is a rule-based matching tool. It allows you to build a library of token patterns and then match those patterns against a spaCy Doc object that contains a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello, world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob\n",
    "\n",
    "Like NLTK and spaCy, TextBlob is a python package for natural language processing. It is a very popular package for NLP and has been around for a long time. It can be very handy for quick NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operations with TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(\"TextBlob is a Python library for NLP. It's built on top of NLTK.\")\n",
    "print(blob.words)\n",
    "print(blob.sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, pos in blob.tags:\n",
    "    print(word, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for np in blob.noun_phrases:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = blob.sentiment\n",
    "print(\"Polarity:\", sentiment.polarity)\n",
    "print(\"Subjectivity:\", sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(\"I havv goood speling!\")\n",
    "print(blob.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = blob.words[1]\n",
    "print(word.lemmatize(\"v\"))  # \"have\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram in blob.ngrams(2):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "Gensim is an open-source Python library designed to work with textual data using modern text representation techniques, such as Word2Vec, FastText, and Latent Semantic Analysis (LSA). It is particularly useful for topic modeling and document similarity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model\n",
    "\n",
    "We will write our own Word2Vec model soon but by way of an introduction, we can look at Gensim's implementation of Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "print(model.wv['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.similarity('cat', 'dog'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Model\n",
    "\n",
    "FastText is a different interpretation of Word2Vec. It is also implemented in Gensim. Rather than trained on the word or token, it is trained on subword information. This can be useful for languages with a lot of compound words or for texting languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "model_ft = FastText(sentences, min_count=1)\n",
    "print(model_ft.wv['cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced operations with Gensim\n",
    "\n",
    "#### Latent Semantic Analysis\n",
    "\n",
    "LSA is a technique for dimensionality reduction. It is useful for reducing the size of a vocabulary and thus the size of a model. Thus, we can use LSA to discover the latent topics in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(text) for text in sentences]\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "print(lsi.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec Model\n",
    "\n",
    "Doc2Vec is an extension of Word2Vec. It is useful for finding similar documents. There are various ways to implement Doc2Vec. We will look at some in the future lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentences)]\n",
    "model_d2v = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "print(model_d2v.dv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "LDA is a probabilistic model used to identify topics in a collection of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=2)\n",
    "print(lda.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "\n",
    "Collocations are words that occur together often. They are useful for understanding the meaning of a string. They are also useful for advanced tokenization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
    "phrases = Phrases(sentences, min_count=1, threshold=0.1, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "for phrase, score in phrases.find_phrases(sentences).items():\n",
    "    print(phrase, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza\n",
    "\n",
    "Stanza is a Python NLP library developed by the Stanford NLP Group. It provides tools for many languages, leveraging state-of-the-art deep learning models. It's designed to be a successor to the Stanford NLP Java libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza Pipelines\n",
    "\n",
    "Stanza provides a convenient interface to a series of NLP tasks. The interface is called a pipeline. The pipeline is a sequence of components that are applied to a document. The components are applied in order. The output of one component is the input to the next component. The pipeline is a convenient way to apply a series of NLP tasks to a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('en')\n",
    "doc = nlp(\"Stanza is a Python NLP library by Stanford.\")\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word.text, word.lemma, word.pos, word.head, word.deprel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operations with Stanza\n",
    "\n",
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sentences:\n",
    "    for ent in sentence.ents:\n",
    "        print(ent.text, ent.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('ru')\n",
    "nlp_ru = stanza.Pipeline('ru')\n",
    "doc_ru = nlp_ru(\"Стэнза - это библиотека NLP для Python.\")\n",
    "for word in doc_ru.sentences[0].words:\n",
    "    print(word.text, word.feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
