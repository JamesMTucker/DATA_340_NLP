{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/Fall_2023/notebooks/06_Vector_Semantics.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 15: 2023-28-03 Vector Semantics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of lecture\n",
    "\n",
    "- Introduction to lexical semantics\n",
    "- Introduction to Vector Semantics\n",
    "  - Vector semantics: Osgood et al. (1957)\n",
    "  - Vector semantics: Joos (1950), Harris (1954), Firth (1957)\n",
    "- Embeddings\n",
    "    - Word2Vec\n",
    "    - GloVe\n",
    "    - FastText\n",
    "    - ELMo\n",
    "    - BERT\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/Neuron.drawio.png\" width=\"800\" height=\"400\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Caculating Loss (measuring error - training a model adjusts weights and biases to minimize loss)\n",
    "* Optimizing Loss (adjust weights and biases to minimize loss)\n",
    "* Backpropagation (calculate the gradient of the loss function with respect to the weights and biases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Lexical Semantics\n",
    "\n",
    "Taken from Jurafsky and Martin (2023) chapter 23:\n",
    "\n",
    "```\n",
    "\n",
    "Lady Bracknell: Are your parents living?\n",
    "Jack: I have lost both my parents.\n",
    "Lady Bracknell: To lose one parent, Mr. Worthing, may be regarded as a misfortune; to lose both looks like carelessness.\n",
    "\n",
    "```\n",
    "\n",
    "* words are relational units that are prone to messiness and ambiguity\n",
    "* Ambiguity is a fact of life in language (`mouse` as in a rodent or a computer device)\n",
    "* Polysemy: a word or lemma with multiple meanings (`bank` as in a river bank or a financial institution)\n",
    "* `Antonymy`: words (or lemmas) with opposite meanings (`hot` and `cold`)\n",
    "* `Synonym`: words (or lemmas) that are similar in meaning (`couch` and `sofa`)\n",
    "* Taxonimic relations\n",
    "    * `hyponymy` (subordinate): words (or lemmas) that are more specific (`poodle` is a hyponym of `dog`) - subclasses or members\n",
    "    * `hypernym` (superordinate): words (or lemmas) that are more general (`dog` is a hypernym of `poodle`) - classes\n",
    "        * entailment: being A entails being B (`dog` entails `poodle`)\n",
    "        * is-a hierarchy: a hierarchy of classes that is organized by the is-a relation or A IS-A B\n",
    "    * `meronymy`: words (or lemmas) that are part of a larger entity (`leg` is a meronym of `human`) - part-whole relationships\n",
    "    * `metonymy`: words (or lemmas) that are associated with a larger entity (`the crown` is a metonym of `the queen`) - association (prototype categories)\n",
    "    * `holonymy`: words (or lemmas) that are a whole of a smaller entity (`face` is a holonym of `eye`) - whole-part relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_taxonomy(noun):\n",
    "    synsets = wn.synsets(noun)\n",
    "    if synsets:\n",
    "        synset = synsets[0]  # take the first synset\n",
    "        hypernyms = synset.hypernyms()\n",
    "        hyponyms = synset.hyponyms()\n",
    "        meronyms = synset.part_meronyms() + synset.substance_meronyms() + synset.member_holonyms()\n",
    "        holonyms = synset.part_holonyms() + synset.substance_holonyms() + synset.member_meronyms()\n",
    "        return {\n",
    "            \"word\": synset.name(),\n",
    "            \"definition\": synset.definition(),\n",
    "            \"hypernyms\": [h.name() for h in hypernyms],\n",
    "            \"hyponyms\": [h.name() for h in hyponyms],\n",
    "            \"meronyms\": [m.name() for m in meronyms],\n",
    "            \"holonyms\": [h.name() for h in holonyms]\n",
    "        }\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_taxonomy(\"dog\")\n",
    "for k,v in result.items():\n",
    "    print(k, v, sep=\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_verb_relations(verb):\n",
    "    synsets = wn.synsets(verb, pos=wn.VERB)\n",
    "    if synsets:\n",
    "        relations = {\n",
    "            \"antonyms\": set(),\n",
    "            \"entailments\": set(),\n",
    "            \"causes\": set(),\n",
    "            \"also_sees\": set(),\n",
    "            \"verb_groups\": set(),\n",
    "            \"similar_tos\": set()\n",
    "        }\n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                antonyms = lemma.antonyms()\n",
    "                if antonyms:\n",
    "                    relations[\"antonyms\"].add(antonyms[0].name())\n",
    "            for entailment in synset.entailments():\n",
    "                relations[\"entailments\"].add(entailment.name())\n",
    "            for cause in synset.causes():\n",
    "                relations[\"causes\"].add(cause.name())\n",
    "            for also_see in synset.also_sees():\n",
    "                relations[\"also_sees\"].add(also_see.name())\n",
    "            for verb_group in synset.verb_groups():\n",
    "                relations[\"verb_groups\"].add(verb_group.name())\n",
    "            for similar in synset.similar_tos():\n",
    "                relations[\"similar_tos\"].add(similar.name())\n",
    "        return relations\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_verb_relations(\"walk\")\n",
    "for k,v in result.items():\n",
    "    print(k, v, sep=\":\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Semantics\n",
    "\n",
    "* Firth (1957) proposed a model of word meaning based on the idea that words are associated with other words in a network of semantic relations.\n",
    "* Firth (1957), Joos (1950), and Harris (1954) all proposed models of word meaning based on the idea that words are associated with other words in a network of semantic relations. Thus the idea of distributional semantics takes its name from the fact that the meaning of a word is discerned by the words that tend to occur in its company.\n",
    "\n",
    "> You shall know a word by the company it keeps. (Firth, 1957)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity\n",
    "\n",
    "* Word similarity is a measure of the degree of semantic similarity between two words. This measure takes into account the distributional properties of words in a corpus. Whereas words like `coffee` would rarely occur in a dictionary entry for the word `cup`, users of language expect that the words `coffee` and `cup` are similar in meaning. They are similar, in this case, because semantic frames are shared between the two words. The semantic frame of `coffee` is a hot beverage, and the semantic frame of `cup` is a container for a hot beverage. The semantic frames of `coffee` and `cup` overlap, and this overlap is the basis for the similarity between the two words. We can capture these similarities by computing the distributional properties of words in a corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we represent words and their meanings in numerical format?\n",
    "\n",
    "We vectorize it!\n",
    "\n",
    "We can represent words in a vector space or embedding space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec, Mikolov et al., 2013\n",
    "\n",
    "Goal: to create “techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity.” (Mikolov, et al., 2013a, 2013b)\n",
    "\n",
    "Mikolov et al. propose two log-linear solutions\n",
    "\n",
    "* Continuous Bag-of-Words Model\n",
    "* Continuous Skip-gram Model \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/mikolov.png\" width=\"900\" height=\"500\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec embeddings are static embeddings, and therefore they do not capture the cooccurrence of words in a sentence. This is a problem for downstream tasks that require contextualized embeddings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove, Pennington et al., 2014"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“...the shallow window-based methods [e.g., log bi-linear models, CBOW, or Skipgram] suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the entire corpus, which fails to take advantage of the vast amount of repetition in the data.” Pennington, et al., 2014."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/glove.png\" width=\"800\" height=\"400\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText, Bojanowski et al., 2017"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/fasttext.png\" width=\"900\" height=\"500\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elmo, Peters et al., 2018"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/elmo.png\" width=\"900\" height=\"400\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"They [embeddings] should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).\" ([Peters et al., 2018, p. 1](https://arxiv.org/pdf/1802.05365.pdf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create word embeddings\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code out the word2vec CBOW and Skipgram models and compare them. To do this, let's define our configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Number of dimensions\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "# Window size\n",
    "WINDOW_SIZE = 5\n",
    "\n",
    "ITERATIONS = 1000\n",
    "\n",
    "# OUTPUT\n",
    "OUTPUT_PATH = \"outputs\"\n",
    "\n",
    "## Let's plot the loss for the skipgram model\n",
    "SKIPGRAM_LOSS = os.path.join(OUTPUT_PATH, 'loss_skipgram')\n",
    "SKIPGRAM_TSNE = os.path.join(OUTPUT_PATH, 'tsne_skipgram')\n",
    "\n",
    "## let's plot the loss for the cbow model\n",
    "CBOW_LOSS = os.path.join(OUTPUT_PATH, 'loss_cbow')\n",
    "CBOW_TSNE = os.path.join(OUTPUT_PATH, 'tsne_cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to preprocess the textual data\n",
    "\n",
    "# We can use tensorflow to preprocess the data\n",
    "import tensorflow as tf\n",
    "\n",
    "def tokenize_data(data):\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/text_to_word_sequence\n",
    "    tokenized_text = tf.keras.preprocessing.text.text_to_word_sequence(input_text=data)\n",
    "\n",
    "    vocab = sorted(set(tokenized_text))\n",
    "    tokenized_text_size = len(tokenized_text)\n",
    "\n",
    "    return (vocab, tokenized_text_size, tokenized_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our imports \n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use google to load the data from drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "datasets_dir = \"/content/drive/My Drive/DATA_340_3_NLP/Datasets/\"\n",
    "\n",
    "# get the txt files\n",
    "filenames = [os.path.join(datasets_dir, f) for f in os.listdir(datasets_dir) if f.endswith(\".txt\") and 'LOTR' in f]\n",
    "\n",
    "# read the files\n",
    "corpus = []\n",
    "\n",
    "# read \n",
    "for f in filenames:\n",
    "    with open(f, 'r', encoding='ISO-8859-1') as file:\n",
    "        corpus.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's flatten the corpus to one string and remove unnecessary spaces\n",
    "\n",
    "corpus = \" \".join(corpus)\n",
    "corpus = \" \".join(corpus.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import unidecode\n",
    "except ModuleNotFoundError:\n",
    "  !pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "corpus = unidecode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "(vocab, tokenized_text_size, tokenized_text) = tokenize_data(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map our words to indices\n",
    "vocab_to_index = {\n",
    "    uniqueWord:index for (index, uniqueWord) in enumerate(vocab)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of our vocab\n",
    "index_to_vocab = np.array(vocab)\n",
    "index_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text to integers\n",
    "text_as_int = np.array([vocab_to_index[word] for word in tokenized_text])\n",
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of random data for our context vectors\n",
    "context_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenized_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "context_vector_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix of random data for our center vectors\n",
    "center_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenized_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "center_vector_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our optimizer\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "optimizer = tf.optimizers.Adam()\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the vectors for the context and center words\n",
    "for iter in tqdm(range(ITERATIONS)):\n",
    "    loss_per_epoch = 0\n",
    "\n",
    "    for start in range(tokenized_text_size - WINDOW_SIZE):\n",
    "        indices = text_as_int[start:start + WINDOW_SIZE]\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        combined_context = 0\n",
    "\n",
    "        # create our context slider\n",
    "        for count, index in enumerate(indices):\n",
    "            if count != WINDOW_SIZE // 2:\n",
    "                combined_context += context_vector_matrix[index, :]\n",
    "        \n",
    "        combined_context /= (WINDOW_SIZE - 1)\n",
    "\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/linalg/matmul\n",
    "        output = tf.matmul(center_vector_matrix, tf.expand_dims(combined_context, 1))\n",
    "\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/softmax\n",
    "        softout = tf.nn.softmax(output, axis=0)\n",
    "        loss = softout[indices[WINDOW_SIZE // 2]]\n",
    "\n",
    "        logloss = -tf.math.log(loss)\n",
    "\n",
    "        loss_per_epoch += logloss.numpy()\n",
    "        \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "        grad = tape.gradient(\n",
    "            logloss, [context_vector_matrix, center_vector_matrix]\n",
    "        )\n",
    "\n",
    "        optimizer.apply_gradients(\n",
    "            zip(grad, [context_vector_matrix, center_vector_matrix])\n",
    "        )\n",
    "\n",
    "        loss_list.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "print(\"[INFO] Plotting loss ...\")\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(CBOW_LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the embeddings to 2D\n",
    "tsne_embed = (\n",
    "    TSNE(n_components=2)\n",
    "    .fit_transform(center_vector_matrix.numpy())\n",
    ")\n",
    "tsne_decode = (\n",
    "    TSNE(n_components=2)\n",
    "    .fit_transform(context_vector_matrix.numpy())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the embeddings for 100 words\n",
    "index_count = 0\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "print(\"[INFO] Plotting TSNE embeddings ...\")\n",
    "\n",
    "for (word, embedding) in tsne_decode[:100]:\n",
    "    # plot the point in 2d space\n",
    "    plt.scatter(word, embedding)\n",
    "    # annotate the point with the word\n",
    "    plt.annotate(index_to_vocab[index_count], (word, embedding))\n",
    "    index_count += 1\n",
    "plt.savefig(CBOW_TSNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKIPGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## same as above but for skipgram\n",
    "(vocab, tokenize_text_size, tokenized_text) = tokenize_data(corpus)\n",
    "\n",
    "vocab_to_index = {\n",
    "    unique_word:index for (index, unique_word) in enumerate(vocab)\n",
    "}\n",
    "index_to_vocab = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([vocab_to_index[word] for word in tokenized_text])\n",
    "\n",
    "context_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenize_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "\n",
    "center_vector_matrix = tf.Variable(\n",
    "    np.random.rand(tokenize_text_size, EMBEDDING_SIZE)\n",
    ")\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in tqdm(range(ITERATIONS)):\n",
    "    loss_per_epoch = 0\n",
    "\n",
    "    for start in range(tokenize_text_size - WINDOW_SIZE):\n",
    "        indices = text_as_int[start:start + WINDOW_SIZE]\n",
    "        \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        center_vector = center_vector_matrix[indices[WINDOW_SIZE // 2], :]\n",
    "        output = tf.matmul(\n",
    "            context_vector_matrix, tf.expand_dims(center_vector, 1)\n",
    "        )\n",
    "\n",
    "        softmax_output = tf.nn.softmax(output, axis=0)\n",
    "\n",
    "        for (count, index) in enumerate(indices):\n",
    "            if count != WINDOW_SIZE // 2:\n",
    "                loss += softmax_output[index]\n",
    "\n",
    "            logloss = -tf.math.log(loss)\n",
    "\n",
    "        loss_per_epoch += logloss.numpy()\n",
    "        \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "        grad = tape.gradient(\n",
    "            logloss, [context_vector_matrix, center_vector_matrix]\n",
    "        )\n",
    "        optimizer.apply_gradients(\n",
    "            zip(grad, [context_vector_matrix, center_vector_matrix])\n",
    "        )\n",
    "    loss_list.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] plotting loss ...\")\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(SKIPGRAM_LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the embeddings to 2D\n",
    "tsneEmbed = (\n",
    "    TSNE(n_components=2)\n",
    "    .fit_transform(center_vector_matrix.numpy())\n",
    ")\n",
    "tsneDecode = (\n",
    "    TSNE(n_components=2)\n",
    "    .fit_transform(context_vector_matrix.numpy())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexCount = 0 \n",
    "\n",
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "print(\"[INFO] Plotting TSNE Embeddings...\")\n",
    "for (word, embedding) in tsneEmbed[100:200]:\n",
    "    plt.scatter(word, embedding)\n",
    "    plt.annotate(index_to_vocab[indexCount], (word, embedding))\n",
    "    indexCount += 1\n",
    "plt.savefig(SKIPGRAM_TSNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Embeddings Projector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d99a3f87c484a74ba405ca572f7f1b4059e93a8c4d7f8027bf5ae12e7919d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
