{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JamesMTucker/DATA_340_NLP/blob/master/Fall_2023/notebooks/02_Properties_of_Language.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 04: Properties of Language, Statistics, Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Understand the basic properties of language by using some EDA techniques\n",
    "* Using statistical methods to understand language\n",
    "* Understand the basics of information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings\n",
    "\n",
    "* [Jurafsky & Martin, Ch 17](../../course_readings/Jurafsky_Martin_chapter_17_363-388.pdf)\n",
    "  * Context-free grammars\n",
    "  * Treebanks\n",
    "  * CKY parsing\n",
    "  * N.B.: The 'Bibliographical and Historical Notes' section is good context\n",
    "* [Jurafsky & Martin, Ch 18](../../course_readings/Jurafsky_Martin_chapter_18_389-412.pdf)\n",
    "  * Dependency Parsing\n",
    "  * Dependency Relations\n",
    "  * Tradition-based dependency parsing\n",
    "  * Graph-based dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spaCy for Dependency Parsing\n",
    "\n",
    "* [spaCy Dependency Parsing](https://spacy.io/usage/linguistic-features#dependency-parse)\n",
    "* [spaCy Dependency Parsing Demo](https://explosion.ai/demos/displacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency parsing\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{10}} {token.dep_:{10}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency parsing\n",
    "# https://spacy.io/usage/linguistic-features#dependency-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use displacy to visualize the dependency tree\n",
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u'I have flown to LA. Now I am flying to Frisco.')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our shared humanity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049087/\n",
    "\n",
    "| Article | Pubmed Link |\n",
    "| ---- | ---- |\n",
    "| ![17_Languages](../images/figure_1.png) | ![17_Languages](../images/17_languages_qr.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swadesh terms\n",
    "\n",
    "* [Swadesh list](https://en.wikipedia.org/wiki/Swadesh_list)\n",
    "* [Swadesh list](https://en.wiktionary.org/wiki/Appendix:Swadesh_lists)\n",
    "\n",
    "What are Swadesh terms?\n",
    "\n",
    "* A list of words that are thought to be universal across languages\n",
    "\n",
    "\n",
    "How does this affect our analysis of texts?\n",
    "\n",
    "* We can use these words to compare languages\n",
    "* We can use these words to compare texts within a language\n",
    "* We can gain an understanding of how langauge is used within domains of knowledge using statistical methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities of Swadesh term usage in different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/figure_3.png\"  width=\"800\" height=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using simple statistics to shed light on documents and langauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data files\n",
    "data_folder = Path(\"../datasets\")\n",
    "# get all files in the data folder\n",
    "data_files_list = [f for f in os.listdir(data_folder) if os.path.isfile(os.path.join(data_folder, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the list of data files\n",
    "data_files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf's Law\n",
    "\n",
    "[George Kingsley Zipf](https://en.wikipedia.org/wiki/George_Kingsley_Zipf) argued that most words are not used that often. He formally defined his theorem as\n",
    "$$P_n \\sim \\frac{1}{n^a}$$\n",
    "\n",
    "It is a power law distribution. The frequency of any word is inverse in porportion to its rank in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf's Law in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def zipf_analysis(text, book) -> None:\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    # word_freq = Counter(words) # this one line of code does the same as the following for loop\n",
    "    \n",
    "    # vanilla python implementation\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    \n",
    "    \n",
    "    # Sort the words by frequency - highest occuring terms are at the top\n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Plot the word frequency and rank to check for Zipf's law\n",
    "    word_rank = np.arange(1, len(sorted_word_freq)+1) # X variable\n",
    "    word_frequency = [i[1] for i in sorted_word_freq] # Y variable\n",
    "    \n",
    "    # Plot log to visualize the power law distribution\n",
    "    plt.loglog(word_rank, word_frequency, marker='o')\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f\"Zipf's Law for {book}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets iterate over the generator and create a list of lists with a Short Volume name and its text\n",
    "corpus = []\n",
    "\n",
    "# Iterate over the files\n",
    "for f in data_files_list:\n",
    "    # open the file and append the text to the corpus list\n",
    "    with open(os.path.join(data_folder, f), 'r') as file:\n",
    "        corpus.append([f, file.read()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### J.R.R. Tokein's The Lord of The Rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use our zipf_analysis function to plot the word frequency and rank for each book\n",
    "for book in corpus:\n",
    "    zipf_analysis(book[1], book[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friedrich Nietzsche's Beyond Good and Evil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zipf's law in action\n",
    "for book in corpus:\n",
    "    if not 'LOTR' in book[0]:\n",
    "        zipf_analysis(book[1], book[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most used word in the USA:\n",
    "\n",
    "The above demonstration of _The Lord of the Rings_ is generalizable to any English text, and as discussed above to many languages for certain kinds of words.\n",
    "\n",
    "N.B.: Notice the study of Manning and Schutze, _Foundations of Statistical Natural Language Processing_, who demonstrate that a randomly created text follows the power law observation as discussed by Mandelbrot. They conclude their discussion observing that:\n",
    "\n",
    "<img src=\"../images/most_used_01.png\"  width=\"800\" height=\"400\">\n",
    "\n",
    "<br />\n",
    "\n",
    "> what makes frequency-based approaches to language hard is that almost all words are rare. Zipf's law is a good way to encapsulate this insight. (p. 29)\n",
    "\n",
    "Thus ... <br>\n",
    "<img src=\"../images/most_used_02.png\" width=\"800\" height=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about Tokens/Lemmas and Zipf's Law?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's tokenize our text according to their lemmata\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# increase the max length of the text that can be processed\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "# create a list of lists with a Short Volume name and its text\n",
    "corpus = []\n",
    "\n",
    "# Iterate over the files\n",
    "for f in data_files_list:\n",
    "    # open the file and append the text to the corpus list\n",
    "    with open(os.path.join(data_folder, f), 'r') as file:\n",
    "        corpus.append([f, nlp(file.read())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the data again using the lemmata\n",
    "for book in corpus:\n",
    "    zipf_analysis(' '.join([token.lemma_ for token in book[1]]), book[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the distribution of words in a document?\n",
    "\n",
    "### Word Frequency Distribution\n",
    "\n",
    "What is the distribution of words in a document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the distribution of the words\n",
    "\n",
    "# create a list of the lemmata in the corpus\n",
    "words_freq = []\n",
    "\n",
    "# Iterate over the files\n",
    "for f in data_files_list[:1]:\n",
    "    # open the file and append the text to the corpus list\n",
    "    with open(os.path.join(data_folder, f), 'r') as file:\n",
    "        words_freq.extend([token.lemma_ for token in nlp(file.read())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each word\n",
    "word_freq = Counter(words_freq)\n",
    "\n",
    "# count the lemma frequency\n",
    "lemma_freq = Counter([token.lemma_ for token in nlp(' '.join(words_freq))])\n",
    "\n",
    "# 10 most common lemmas\n",
    "most_com = lemma_freq.most_common(20)\n",
    "\n",
    "# 10 least common lemmas\n",
    "least_com = lemma_freq.most_common()[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of the most common lemmas and least common lemmas\n",
    "import pandas as pd\n",
    "\n",
    "df_most = pd.DataFrame(most_com, columns=['lemma', 'frequency'])\n",
    "df_least = pd.DataFrame(least_com, columns=['lemma', 'frequency'])\n",
    "\n",
    "# concatenate the dataframes\n",
    "df = pd.concat([df_most, df_least])\n",
    "df.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Length Distribution\n",
    "\n",
    "What is the distribution of word lengths in a document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word length distribution\n",
    "word_length = [len(word) for word in words_freq]\n",
    "\n",
    "# plot the word length distribution \n",
    "# change the ticks to increment by 5\n",
    "plt.hist(word_length, bins=65)\n",
    "plt.xticks(np.arange(0, 65, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Length Distribution\n",
    "\n",
    "What is the distribution of sentence lengths in a document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sentence length distribution\n",
    "\n",
    "# use spacy to break sentences\n",
    "sentences = [sent for sent in nlp(corpus[0][1]).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as read from the file\n",
    "corpus[0][1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy NLP Sentence Detection\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sentence length distribution\n",
    "sentence_length = [len(sent) for sent in sentences]\n",
    "\n",
    "# plot the sentence length distribution\n",
    "plt.hist(sentence_length, bins=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Numpy to get the mean and standard deviation of the sentence length\n",
    "import numpy as np\n",
    "\n",
    "np.mean(sentence_length), np.std(sentence_length), np.percentile(sentence_length, 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Distribution\n",
    "\n",
    "What is the distribution of parts of speech in a document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spacy to POS tags and plot the distribution of the POS tags\n",
    "\n",
    "pos = [token.pos_ for token in nlp(corpus[0][1])]\n",
    "\n",
    "# plot the POS tag distribution\n",
    "plt.hist(pos, bins=20)\n",
    "# rotate the x-axis\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Distribution\n",
    "\n",
    "What is the distribution of n-grams in a document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the most common n-grams in the corpus?\n",
    "\n",
    "# n-grams using python\n",
    "bi_grams = []\n",
    "\n",
    "# iterate over the tokens in the corpus\n",
    "for i in range(len(words_freq)-1):\n",
    "    bi_grams.append((words_freq[i], words_freq[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the frequency of each bigram\n",
    "bi_gram_freq = Counter(bi_grams)\n",
    "\n",
    "bi_gram_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_com = bi_gram_freq.most_common()[-10:]\n",
    "least_com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocation Distribution\n",
    "\n",
    "What is the distribution of collocations in a document\n",
    "\n",
    "What is a collocation?\n",
    "\n",
    "Linguistic collocations refer to sequences of words that co-occur more often than would be expected by chance. Testing the significance of collocations is a common task in computational linguistics and Natural Language Processing (NLP). There are several statistical measures used to test the significance of collocations. Some of the most common ones include:\n",
    "\n",
    "1. **Mutual Information (MI)**:\n",
    "   $$ MI(w_1, w_2) = \\log_2 \\left( \\frac{P(w_1, w_2)}{P(w_1) \\times P(w_2)} \\right) $$\n",
    "   where \\( P(w_1, w_2) \\) is the probability of the words \\( w_1 \\) and \\( w_2 \\) occurring together, and \\( P(w_1) \\) and \\( P(w_2) \\) are the probabilities of the words \\( w_1 \\) and \\( w_2 \\) occurring independently.\n",
    "\n",
    "2. **Chi-squared Test**:\n",
    "   The chi-squared test compares the observed frequency of a collocation to the expected frequency if the two words were independent. The formula is:\n",
    "   $$ \\chi^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}} $$\n",
    "   where \\( O_{ij} \\) is the observed frequency of the words \\( w_1 \\) and \\( w_2 \\) occurring together, and \\( E_{ij} \\) is the expected frequency if the two words were independent.\n",
    "\n",
    "3. **T-score**:\n",
    "   $$ T(w_1, w_2) = \\frac{f(w_1, w_2) - E(f(w_1, w_2))}{\\sqrt{f(w_1, w_2)}} $$\n",
    "   where \\( f(w_1, w_2) \\) is the observed frequency of the collocation, and \\( E(f(w_1, w_2)) \\) is the expected frequency under the assumption of independence.\n",
    "\n",
    "4. **Log-Likelihood Ratio (LLR)**:\n",
    "   This is a more complex measure that compares the likelihood of the observed data under two different models: one where the words are independent and one where they are dependent.\n",
    "\n",
    "There are other measures as well, but these are among the most commonly used in the field of NLP. The choice of measure often depends on the specific task and the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collocations with Mutual Information in vanilla python\n",
    "\n",
    "# import the Counter module\n",
    "from collections import Counter\n",
    "\n",
    "# create a list of the lemmata in the corpus\n",
    "words_freq = []\n",
    "\n",
    "# Iterate over the files\n",
    "for f in data_files_list[:1]:\n",
    "    # open the file and append the text to the corpus list\n",
    "    with open(os.path.join(data_folder, f), 'r') as file:\n",
    "        words_freq.extend([token.lemma_ for token in nlp(file.read())])\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_freq = Counter(words_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gensim Collocation Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gensim to get the collocations\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# create a list of lists of the lemmata in the corpus\n",
    "words_freq = []\n",
    "\n",
    "# Iterate over the files\n",
    "for f in data_files_list[:1]:\n",
    "    # open the file and append the text to the corpus list\n",
    "    with open(os.path.join(data_folder, f), 'r') as file:\n",
    "        words_freq.append([token.lemma_ for token in nlp(file.read())])\n",
    "        \n",
    "# create the phrases model\n",
    "phrases = Phrases(words_freq, min_count=30, progress_per=10000)\n",
    "\n",
    "# create the collocations\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "# get the collocations\n",
    "bigram.phrasegrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python functions to get the collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    "from math import log\n",
    "from math import sqrt\n",
    "\n",
    "def mutual_information(w1_w2_count, w1_count, w2_count, total_count):\n",
    "    p_w1_w2 = w1_w2_count / total_count\n",
    "    p_w1 = w1_count / total_count\n",
    "    p_w2 = w2_count / total_count\n",
    "    return log2(p_w1_w2 / (p_w1 * p_w2))\n",
    "\n",
    "\n",
    "def chi_squared(w1_w2_count, w1_count, w2_count, total_count):\n",
    "    expected_w1_w2 = (w1_count * w2_count) / total_count\n",
    "    return (w1_w2_count - expected_w1_w2)**2 / expected_w1_w2\n",
    "\n",
    "\n",
    "def t_score(w1_w2_count, w1_count, w2_count, total_count):\n",
    "    expected_w1_w2 = (w1_count * w2_count) / total_count\n",
    "    return (w1_w2_count - expected_w1_w2) / sqrt(w1_w2_count)\n",
    "\n",
    "def log_likelihood_ratio(w1_w2_count, w1_count, w2_count, total_count):\n",
    "    e1 = w1_count * w2_count / total_count\n",
    "    e2 = w1_count * (total_count - w2_count) / total_count\n",
    "    e3 = (total_count - w1_count) * w2_count / total_count\n",
    "    e4 = (total_count - w1_count) * (total_count - w2_count) / total_count\n",
    "\n",
    "    o1 = w1_w2_count\n",
    "    o2 = w1_count - w1_w2_count\n",
    "    o3 = w2_count - w1_w2_count\n",
    "    o4 = total_count - w1_count - w2_count + w1_w2_count\n",
    "\n",
    "    llr = 2 * (o1 * log(o1 / e1) + o2 * log(o2 / e2) + o3 * log(o3 / e3) + o4 * log(o4 / e4))\n",
    "    return llr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Andrew Huberman Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore our data from Andrew Huberman's Podcasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/JamesMTucker/AskHuberman/main/scraper/data/video_metadata.csv'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Merge the Title and Description\n",
    "\n",
    "df['title_description'] = df['video_title'] + ' ' + df['video_description']\n",
    "\n",
    "df = df[['video_id', 'title_description']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF: Term-Frequency Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned from our analysis of Zipf's law that the most frequently occuring terms (bag of words) offer little information about how topics are discussed. We can get a little of an idea of what topics might be discussed by the frequencies of a given term. But we often want and need to do more to analyze a document and its language. So, we created N-grams. We saw that an N-gram methodology provides more information about how language was used in a document. Plotting the various distributions of the linguistic data might lead us to ask important questions. Yet, there are other useful metrics to extract a document's relevant terms: Term-frequency inverse document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informal Definition\n",
    "\n",
    "General intuition of tf-idf is that words isolated to one particular text provide information as to how it relates to other documents in the corpus.\n",
    "\n",
    "It is import to note the following definitions:\n",
    "\n",
    "* `Document` = a news article, journal article, tweet, reddit post, etc.\n",
    "* `Document vocabulary` = frequency of terms in a document\n",
    "* `Corpus` = a collection of documents\n",
    "* `Corpus Vocabulary` = frequency of terms in a corpus of documents\n",
    "\n",
    "Thus,\n",
    "\n",
    "`tf-idf` = `term_frequency` * `inverse_document_frequency`\n",
    "\n",
    "`term_frequency` = count of a words appearence in a document\n",
    "`inverse_document_frequency` = log(total_number_of_documents / number_of_documents_with_term) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Definition\n",
    "\n",
    "$$idf_{t} = log_{10}(\\frac{N}{df_t})$$\n",
    "\n",
    "`term frequency` = $tf_{t,d} = log_{10}(count(t,d) + 1)$ -- The intuition is that a word appearing 100 times does not make that word 100x more likely to be relevant to the meanining of the document. Therefore, we give a heigher weight to words that occur only in a few documents.\n",
    "\n",
    "`total documents in collection` = $idf_{t} = log_{10}(\\frac{N}{df_t})$ -- We let _N_ be the total number of documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemma count per video description\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "df['lemma_count'] = [len([token.lemma_ for token in nlp(text)]) for text in df['title_description']]\n",
    "\n",
    "# plot the distribution of the lemma count\n",
    "plt.hist(df['lemma_count'], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortest video description\n",
    "df[df['lemma_count'] == df['lemma_count'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longest video description\n",
    "df[df['lemma_count'] == df['lemma_count'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Video Descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string punctuation and lemmatize the title_description\n",
    "import string\n",
    "\n",
    "punct = string.punctuation\n",
    "\n",
    "# lemma count per title_description\n",
    "df['tokens'] = df['title_description'].apply(lambda x: [token.lemma_.lower() for token in nlp(x) if token.lemma_ not in punct])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode the lemma_text column\n",
    "\n",
    "token_df = (df\n",
    "      .explode('tokens')\n",
    "      .drop(columns=['title_description', 'lemma_count', 'tokenized'])\n",
    ")\n",
    "\n",
    "token_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a TF count dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word frequency dataframe\n",
    "term_frequency = (token_df\n",
    "                  .groupby(by=['video_id', 'tokens'])\n",
    "                  .agg({'tokens': 'count'})\n",
    "                  .rename(columns={'tokens': 'term_frequency'})\n",
    "                  .reset_index()\n",
    "                  .rename(columns={'tokens': 'term'})\n",
    "                 )\n",
    "\n",
    "term_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we aren't so interested in stop words\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "         'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "         'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "         'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "         'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "         'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a TF Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "term_frequency = term_frequency.drop(term_frequency[term_frequency['term'].isin(stop_words)].index)\n",
    "term_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Document Frequency Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Frequency\n",
    "document_frequency = (term_frequency\n",
    "                      .groupby(['video_id', 'term'])\n",
    "                      .size()\n",
    "                      .unstack()\n",
    "                      .sum()\n",
    "                      .reset_index()\n",
    "                      .rename(columns={0: 'document_frequency'})\n",
    "                     )\n",
    "\n",
    "document_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the TF and DF Dataframes to create TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the document freqs into the term dataframe\n",
    "term_frequency = term_frequency.merge(document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_in_corpus = term_frequency['video_id'].nunique()\n",
    "documents_in_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse document frequency\n",
    "term_frequency['idf'] = np.log((1 + documents_in_corpus) / (1 + term_frequency['document_frequency'])) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the TF-IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency['tfidf'] = term_frequency['term_frequency'] * term_frequency['idf']\n",
    "term_frequency.sort_values(by=['term_frequency'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the values for interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "term_frequency['tfidf_norm'] = preprocessing.normalize(term_frequency[['tfidf']], axis=0, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the TF-IDF values and Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_terms = term_frequency.sort_values(by=['video_id', 'tfidf'], ascending=[True, False]).groupby(['video_id']).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_terms.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidIds = top_n_terms['video_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the links by merging on video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_titles = pd.merge(top_n_terms, df, on='video_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
